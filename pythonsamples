Account.py
from datetime import date,datetime
from pyspark import SparkContext
from pyspark.sql import DataFrame,SQLContext
from pyspark.sql.functions import udf
from pyspark.sql.types import DateType
from pythontestmodule.data.dataexception import AuthException,InputException
from pythontestmodule.function.acewrapper import filterWrapper
from pythontestmodule.function.byteBufferConvert import parseColumnsConvertBinary
from pythontestmodule.function.drop import drop
from pythontestmodule.util.hdfsUtils import existsDir,getClient,getLatestDataPaths
from pythontestmodule.util.login import Session
from pythontestmodule.util.properties import Properties
from typing import Dict,List,Optional,Set

'''
Account Data - checks if the table names are right, columns exist, data is valid format etc.
@todo: Optimize around Parquet.
'''
def accountData(session, table, country, org, product, du, columns = None, rpt_prd = None, version = None):
    # type: (Session, str, str, str, str, str, Optional[List[str]], Optional[str], Optional[str]) -> DataFrame
    if not session.authorize(country, product):
        raise AuthException(session.username, country, product)
    return __accountData(session.sqlContext, table, country, org, product, du, columns, rpt_prd, version)

def accountDataMultiMonth(session, table, country, org, product, du, rpt_prds, columns = None, cache = False):
    # type: (Session, str, str, str, str, str, List[str], Optional[List[str]], Optional[bool]) -> DataFrame
    if not session.authorize(country, product):
        raise AuthException(session.username, country, product)
    return __accountDataMultiMonth(session.sqlContext, table, country, org, product, du, columns, rpt_prds, cache)

def accountDataOptimized(session, country, org, product, du, startPrd, endPrd, columns = None):
    # type: (Session, str, str, str, str, str, str, Optional[List[str]]) -> DataFrame
    #if not session.authorize(country, product):
    #    raise AuthException(session.username, country, product)
    
    startYear = startPrd[:4]
    endYear = endPrd[:4]
    if startYear == endYear:
        df = __getOptimizedData(session.sqlContext, getCachedPath(country, org, product, du, startPrd[:4]), columns)
        if startPrd[-2:] != "01":
            filterString = "(REPORTING_PERIOD >= '" + str(__datetx(startPrd, "%Y%m")) + "')"
            print("Filtering data for selected reporting periods: " + filterString)
            df = filterWrapper(df, filterString)
        if endPrd[-2:] != "12":
            filterString = "(REPORTING_PERIOD <= '" + str(__datetx(endPrd, "%Y%m")) + "')"
            print("Filtering data for selected reporting periods: " + filterString)
            df = filterWrapper(df, filterString)
        return df
    
    res = None
    
    startRange = int(startYear)
    if startPrd[-2:] != "01":
        res = __getOptimizedData(session.sqlContext, getCachedPath(country, org, product, du, startYear), columns)
        filterString = "(REPORTING_PERIOD >= '" + str(__datetx(startPrd, "%Y%m")) + "')"
        print("Filtering data for selected reporting periods: " + filterString)
        res = filterWrapper(res, filterString)
        startRange += 1
        
    years = range(startRange, int(endYear))
    paths = []  #type: List[str]
    for y in years:
        paths.append(getCachedPath(country, org, product, du, str(y)))
    endPath = getCachedPath(country, org, product, du, endYear)    
    if endPrd[-2:] == "12":
        paths.append(endPath)
    if paths:
        df = __getOptimizedDataMulti(session.sqlContext, paths, columns)
        res = df if res is None else res.unionAll(df)
    
    if endPrd[-2:] != "12":
        df = __getOptimizedData(session.sqlContext, getCachedPath(country, org, product, du, endYear), columns)
        filterString = "(REPORTING_PERIOD <= '" + str(__datetx(endPrd, "%Y%m")) + "')"
        print("Filtering data for selected reporting periods: " + filterString)
        df = filterWrapper(df, filterString)
        res = df if res is None else res.unionAll(df)
    
    return res

def getCachedPath(country, org, product, du, year):
    # type: (str, str, str, str, str) -> str
    return Properties.getInstance().get("cache.root") + country + "-" + org + "-" + product + "-" + du + "-" + year

def __checkOptimizedPath(path):
    # type: (str) -> bool
    client = getClient()
    if not existsDir(path, client):
        client.url = client.url.replace('02i1', '01i1')
        return existsDir(path, client)
    else:
        return True

def __getOptimizedData(sqlContext, path, columns):
    # type: (SQLContext, str, List[str]) -> DataFrame
    if not __checkOptimizedPath(path):
        raise InputException(path, "Path does not exist: " + path)
    df = sqlContext.read.parquet(path)
    if columns is not None:
        df = df.select(columns)
    elif 'binningKey' in df.columns:
        df = drop(df, 'binningKey')
    return df

def __getOptimizedDataMulti(sqlContext, paths, columns):
    # type: (SQLContext, List[str], List[str]) -> DataFrame
    for p in paths:
        if not __checkOptimizedPath(p):
            raise InputException(p, "Path does not exist: " + p)
    df = sqlContext.read.parquet(*paths)
    if columns is not None:
        df = df.select(columns)
    return df

def __accountDataMultiMonth(sqlContext, table, country, org, product, du, columns, rpt_prds, cache = False):
    # type: (SQLContext, str, str, str, str, str, List[str], List[str], Optional[bool]) -> DataFrame
    if cache:
        fullYrs = []  # type: List[str]
        prdByYr = {}  # type: Dict[str, Set[str]]
        for prd in rpt_prds:
            if len(prd) == 4:
                fullYrs.append(prd)
            yr = prd[:4]
            if yr in prdByYr:
                prdByYr[yr].add(prd)
            else:
                prdByYr[yr] = {prd}
        df = None
        for y in prdByYr:
            path = getCachedPath(country, org, product, du, y)
            dfRaw = __getOptimizedData(sqlContext, path, columns)
            if len(prdByYr[y]) < 12 and not y in fullYrs:
                filterString = ""
                for p in prdByYr[y]:
                    if filterString:
                        filterString += " OR "
                    filterString += "(REPORTING_PERIOD = '" + str(__datetx(p, "%Y%m")) + "')"
                print("Filtering data for selected reporting periods: " + filterString)
                dfRaw = filterWrapper(dfRaw, filterString)
            if df:
                df = df.unionAll(dfRaw)
            else:
                df = dfRaw
    else:
        paths = getLatestDataPaths(table, country, org, product, du, rpt_prds)
        df = sqlContext.read.format("com.databricks.spark.avro").load(paths)
        df = __fixDF(sqlContext, df, table, columns)
    
    return df

def __accountData(sqlContext, table, country, org, product, du, columns, rpt_prd = None, version = None):
    # type: (SQLContext, str, str, str, str, str, List[str], Optional[str], Optional[str]) -> DataFrame
    pr = Properties.getInstance()
    tables = pr.get("account.tables")
    if (table not in tables.split()):
        raise InputException(table, "Invalid Table name, needs to be in :%s" % tables);
    path = pr.get("account.hdfspath") + table + '/country_code=' + country + '/organization_code=' + org + '/product_code=' + product + '/data_unit=' + du
    if rpt_prd is not None:
        path = path + "/rpt_prd=" + rpt_prd
    if version is not None:
        path = path + "/version=" + version

    df = sqlContext.read.format("com.databricks.spark.avro").load(path)
    if columns is not None:
        if "REPORTING_PERIOD" not in columns:
            columns.insert(0, "REPORTING_PERIOD")
        if "ACCOUNT_NUMBER" not in columns:
            columns.insert(0, "ACCOUNT_NUMBER")
        df = df.select(columns)
    df = __binaryToDecimal(sqlContext, df)
    return df

def __datetx(x, fmt = '%d%b%Y'):
    # type: (str, str) -> date
    if x == 'null' or x == None:
        return date(1900, 1, 1)
    return datetime.strptime(x, fmt).date()

def __fixDF(sqlContext, df, table, columns):
    # type: (SQLContext, DataFrame, str, List[str]) -> DataFrame
    if columns is not None:
        df = df.select(columns)
    
    df = __binaryToDecimal(sqlContext, df)
    
    dateFields = None
    try:
        dateFields = Properties.getInstance().get("account.table.datefields." + table)
    except:
        print ("Table missing: will ignore " + table)
    if dateFields is not None:
        func = udf(__datetx, DateType())
        for colName in df.columns:
            if colName in dateFields.split():
                df = df.withColumn(colName, func(df[colName]))
    
    if 'REPORTING_PERIOD' in df.columns:
        rptFunc = udf(lambda x: datetime.strptime(x, '%Y%m'), DateType())
        df = df.withColumn('REPORTING_PERIOD', rptFunc(df.REPORTING_PERIOD))
    
    return df

def __binaryToDecimal(sqlContext, df):
    # type: (SQLContext, DataFrame) -> DataFrame
    convertedCols = parseColumnsConvertBinary(df, onlybinary = False)
    # Need to force consecutive selects if more than 100 binary columns
    if len(convertedCols) > 1:
        SparkContext.getOrCreate().setCheckpointDir(Properties.getInstance().get("cache.root") + "checkpoint")
        for i in range(len(convertedCols) - 1):
            print("Converting batch " + str(i + 1) + " of 100 binary columns")
            df = df.select(convertedCols[i])
            rdd = df.rdd
            rdd.checkpoint()
            rdd.first()
            df = sqlContext.createDataFrame(rdd, df.schema)
    return df.select(convertedCols[-1])

Dataexception.py
class InputException(Exception):
    def __init__(self,userInput,message):
        self.userInput = userInput
        self.message = message
    
    def __str__(self, *args, **kwargs):
        return "USERERROR:" + str(self.userInput)+ " Message:" + str(self.message)

class BehaviorException(Exception):
    def __init__(self,userInput,message):
        self.userInput = userInput
        self.message = message
    
    def __str__(self, *args, **kwargs):
        return "UnsupportedOperation:" + str(self.userInput)+ " Message:" + str(self.message)

class AuthException(Exception):
    def __init__(self, username, country, product):
        self.username = username
        self.country = country
        self.product = product
        self.message = "User [" + username + "] does not have entitlements for country [" + country + "] and product [" + product + "]"
    
    def __str__(self, *args, **kwargs):
        return "AUTH FAILED: " + str(self.message)

hdfsFrames.py
from datetime import datetime
import os
from pyspark.sql import DataFrame
from pythontestmodule.data.dataexception import InputException
from pythontestmodule.util.hdfsUtils import deletePath,existsDir,listDir,move
from pythontestmodule.util.login import Session
from pythontestmodule.util.properties import Properties
from typing import Dict,List,Optional

def __buildPath(session, logicalPath):
    # type: (Session, str) -> str
    return os.path.join(Properties.getInstance().get("hdfs.acedir"), session.username, str(logicalPath))

def saveCsv(session, dataFrame, logicalPath, overwrite = False):
    # type: (Session,DataFrame, str, bool) -> None
    if not isinstance(dataFrame, DataFrame):
        raise InputException(dataFrame, "The dataFrame parameter must be of type pyspark.sql.DataFrame")
    if not isinstance(session, Session):
        raise InputException(session, "The session parameter must be of type pythontestmodule.util.login.Session")
    
    logicalPath = __buildPath(session, logicalPath)
    tmpDir = logicalPath + ".tmp"
    try:
        dataFrame.repartition(1).write.format("com.databricks.spark.csv").option("header", "true").save(tmpDir)
        tmpTsvFile = os.path.join(tmpDir, "part-00000")
        if existsDir(logicalPath):
            if overwrite:
                print("Deleting existing file at " + logicalPath)
                deletePath(logicalPath)
            else:
                raise InputException(logicalPath, "The provided path '" + logicalPath + "' already exists and the overwrite parameter is False")
        move(tmpTsvFile, logicalPath)
    finally:
        if existsDir(tmpDir):
            deletePath(tmpDir)

def saveDfParquet(session, dataFrame, logicalPath):
    # type: (Session, DataFrame, str) -> Dict[str, str]
    return __saveDf(session, dataFrame, logicalPath, True)
    
def saveDf(session, dataFrame, logicalPath):
    # type: (Session, DataFrame, str) -> Dict[str, str]
    return __saveDf(session, dataFrame, logicalPath, False)

def __saveDf(session, dataFrame, logicalPath, isParquet = False):
    # type: (Session, DataFrame, str, Optional[bool]) -> Dict[str, str]
    if not isinstance(session, Session):
        raise InputException(session, "The session parameter must be of type pythontestmodule.util.login.Session")
    if not isinstance(dataFrame, DataFrame):
        raise InputException(dataFrame, "The dataFrame parameter must be of type pyspark.sql.DataFrame")
    
    writePath = __buildPath(session, logicalPath)
    if isParquet:
        dataFrame.write.parquet(writePath)
    else:
        dataFrame.write.format("com.databricks.spark.avro").mode("overwrite").save(writePath)
    return {"hdfsPath": writePath, "timestamp": str(datetime.now())}

def deleteDf(versionObject):
    # type: (Dict[str, str]) -> None
    if not isinstance(versionObject, dict):
        raise InputException(versionObject, "The versionObject parameter must be a dictionary")
    if not "hdfsPath" in versionObject.keys():
        raise InputException(versionObject, "The versionObject parameter must contain a value with key 'hdfsPath'")
    
    deletePath(versionObject["hdfsPath"])
    
def loadDfParquet(session, versionObject):
    # type: (Session, Dict[str, str]) -> DataFrame
    return loadDf(session, versionObject, True)

def loadDf(session, versionObject, isParquet = False):
    # type: (Session, Dict[str, str], Optional[bool]) -> DataFrame
    if not isinstance(session, Session):
        raise InputException(session, "The session parameter must be of type pythontestmodule.util.login.Session")
    if not isinstance(versionObject, dict):
        raise InputException(versionObject, "The versionObject parameter must be a dictionary")
    if not "hdfsPath" in versionObject.keys():
        raise InputException(versionObject, "The versionObject parameter must contain a value with key 'hdfsPath'")
    
    if isParquet:
        return session.sqlContext.read.parquet(versionObject["hdfsPath"])
    
    return session.sqlContext.read.format("com.databricks.spark.avro").load(versionObject["hdfsPath"])

def getSavedDf(session, logicalPath):
    # type: (Session, str) -> List[Dict[str, str]]
    if not isinstance(session, Session):
        raise InputException(session, "The session parameter must be of type pythontestmodule.util.login.Session")
    
    voList = []
    listPath = __buildPath(session, logicalPath)
    fsList = listDir(listPath, status = True)
    for fs in fsList:
        if fs[1]['type'] == 'DIRECTORY':
            voList.append({"hdfsPath": os.path.join(listPath, str(fs[0])), "timestamp": str(datetime.fromtimestamp(fs[1]['modificationTime']/1000))})
    return voList

Optimize.py
from pythontestmodule.data.account import accountDataMultiMonth,getCachedPath
from pythontestmodule.function.acewrapper import filterWrapper
from pythontestmodule.function.filter.filter import ifelse
from pythontestmodule.function.join import joinTables
from pythontestmodule.function.rename import rename
from pythontestmodule.function.summary import summarize
from pythontestmodule.util.hdfsUtils import deletePath,existsDir
from pyspark.sql.functions import when
from pythontestmodule.data.scenario import getScenarioData
from pythontestmodule.util.login import Session
from typing import Optional
from pythontestmodule.util.properties import Properties


def writeOptimizedData(session, country, org, product, du, year, overwrite = False):
    # type: (Session, str, str, str, str, str, Optional[bool]) -> None
    months = []
    for m in range(1,13):
        months.append(year + "{0:0>2}".format(m))
    
    if country == 'AU' and product == 'MTG':
        actData = accountDataMultiMonth(session, ‘ACT_SAMPLE_TABLE’, country, org, product, du, months)
        acsData = accountDataMultiMonth(session, ‘ACS_SAMPLE_TABLE’, country, org, product, du, months)
        actData = joinTables(actData, acsData, [('ACCOUNT_NUMBER','ACCOUNT_NUMBER'), ('REPORTING_PERIOD','REPORTING_PERIOD')], 'left')
    else:
        actData = accountDataMultiMonth(session, ‘ACT_SAMPLE_TABLE’, country, org, product, du, months,
                                        ['ACCOUNT_NUMBER',…..])
        acsData = accountDataMultiMonth(session, ‘ACS_SAMPLE_TABLE’, country, org, product, du, months,
                                        ['ACCOUNT_NUMBER', 'REPORTING_PERIOD', 'ORIG_ACCT_FICO_SCORE'])
        acsData = ifelse(acsData, 'ORIG_ACCT_FICO_SCORE', "(ORIG_ACCT_FICO_SCORE IS NOT NULL)", 'ORIG_ACCT_FICO_SCORE', 0)
        acsData = filterWrapper(acsData, "(ORIG_ACCT_FICO_SCORE != 0)")
        acsData = rename(summarize(acsData, 'min', 'ORIG_ACCT_FICO_SCORE', ['ACCOUNT_NUMBER', 'REPORTING_PERIOD']), {'min(ORIG_ACCT_FICO_SCORE)': 'ORIG_ACCT_FICO_SCORE'})
        coeData = accountDataMultiMonth(session, 'PROFILED_COE', country, org, product, du, months,
                                        ['ACCOUNT_NUMBER', 'REPORTING_PERIOD', 'CHARGE_OFF_EVENT_PRINCIPAL'])
        coeData = ifelse(coeData, 'CHARGE_OFF_EVENT_PRINCIPAL', "(CHARGE_OFF_EVENT_PRINCIPAL IS NOT NULL)", 'CHARGE_OFF_EVENT_PRINCIPAL', 0)
        coeData = rename(summarize(coeData, 'sum', 'CHARGE_OFF_EVENT_PRINCIPAL', ['ACCOUNT_NUMBER', 'REPORTING_PERIOD']), {'sum(CHARGE_OFF_EVENT_PRINCIPAL)': 'CHARGE_OFF_EVENT_PRINCIPAL'})
        recData = accountDataMultiMonth(session, 'PROFILED_REC', country, org, product, du, months,
                                        ['ACCOUNT_NUMBER', 'REPORTING_PERIOD', 'NET_RECOVERIES'])
        actData = joinTables(actData, acsData, [('ACCOUNT_NUMBER', 'ACCOUNT_NUMBER'), ('REPORTING_PERIOD', 'REPORTING_PERIOD')], 'left_outer')
        actData = joinTables(actData, coeData, [('ACCOUNT_NUMBER', 'ACCOUNT_NUMBER'), ('REPORTING_PERIOD', 'REPORTING_PERIOD')], 'left_outer')
        actData = joinTables(actData, recData, [('ACCOUNT_NUMBER', 'ACCOUNT_NUMBER'), ('REPORTING_PERIOD', 'REPORTING_PERIOD')], 'left_outer')
    
    writePath = getCachedPath(country, org, product, du, year)
    if overwrite and existsDir(writePath):
        print("Deleting existing contents at " + writePath)
        deletePath(writePath)
    if existsDir(writePath):
        print("Skipping write because path already exists: " + writePath)
    else:
        actData = actData.withColumn("binningKey", actData.ACCOUNT_NUMBER.substr(0,2))
        actData = actData.repartition(4, "binningKey")
        print("Writing Parquet data to path " + writePath)
        actData.write.parquet(writePath)

def writeOptimizedScenarioData(session, overwrite = False):
    # type: (Session, Optional[bool]) -> None
    eco = getScenarioData(session)
    eco = filterWrapper(eco, "(REPORTING_PERIOD IS NOT NULL)")
    eco = eco.withColumn('country', when(eco.AttributeType =='Country', eco.AttributeValue).otherwise(None))
    eco = eco.repartition(4, "country")
    eco.write.parquet(Properties.getInstance().get("cache.root") + 'scenario')

Scenario.py
from datetime import date,datetime
from pyspark import SparkContext
from pyspark.sql import DataFrame,SQLContext
from pyspark.sql.functions import udf
from pyspark.sql.types import *
from pythontestmodule.function.acewrapper import filterWrapper
from pythontestmodule.function.join import joinTables
from pythontestmodule.util.login import Session
from pyspark.sql.functions import lit,udf
from typing import Optional

def getScenarioData(session, scenarioName = None):
    # type: (Session, str) -> DataFrame
    sqlContext  = session.sqlContext
    return __getScenarioData(sqlContext,scenarioName)

def getMetadata(session, scenarioName = None):
    sqlContext  = session.sqlContext
    metadataDf  = sqlContext.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").option("delimiter","|").load("/data/gftrrhrn/analytics/metadata.latest.txt")
    return metadataDf

def lastThreeUdf(columnValue , number):
    if columnValue is not None :
      words = columnValue.split(".")
      return words[int(number)][-3:]  
    else:
      return None
  
def firstThreeUdf(columnValue , number):
    if columnValue is not None :
      words = columnValue.split(".") 
      return words[int(number)][:3]
    else:
      return None
    
def datetx(x):
    # type: (str) -> date
    if x == 'null' or x == None:
        return None
    try:
        return datetime.strptime(x, '%b-%Y').date()
    except ValueError:
        return None

def __getScenarioData(sqlContext):
    # type: (SQLContext, Optional[str]) -> DataFrame
    scDf  = sqlContext.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").option("delimiter","|").load("/data/gftrrhrn/analytics/scenarioData.latest.txt")
    #Format Reporting Period
    #rptFunc = udf(datetx, DateType())
    #joinedDf = scDf.withColumn('REPORTING_PERIOD',rptFunc(joinedDf.XAxisConverteValue))
    
    lastThree = udf(lastThreeUdf, StringType())
    firstThree= udf(firstThreeUdf, StringType())
    joinedDf = scDf.withColumn('tenor',lastThree(scDf.RFID , lit(6)))
    joinedDf = joinedDf.withColumn('currency',firstThree(joinedDf.RFID , lit(6)))
    
    return joinedDf

if __name__ == "__main__":
    sc = SparkContext()
    sqlContext = SQLContext(sc)
    df = __getScenarioData(sqlContext)
    df.show(5)
    session = Session('test','test')
    mddf = getMetadata(session)
    mddf.show(5)
    joinedDf = joinTables(df,mddf,[('RFID','RFID' )])
    joinedDf.show(5,False)
    

function.acewrapper.py
from pythontestmodule.data.dataexception import InputException
from pyspark import StorageLevel, SparkContext, Broadcast
from pyspark.sql.dataframe import DataFrame
from pyspark.sql import Row
from pythontestmodule.function.filter.filter import parseCheck, MyFilterVisitor
from typing import List,Union

def filterWrapper(dataFrame, filterString) :
    #type: (DataFrame,str) -> DataFrame
    if not isinstance(dataFrame, DataFrame):
        raise InputException(dataFrame, "Not DataFrame exception!")
    parseCheck(MyFilterVisitor(dataFrame), filterString)
    return dataFrame.filter(filterString)

def selectExprWrapper(dataFrame, filterString):
    #type: (DataFrame,str) -> DataFrame
    if not isinstance(dataFrame, DataFrame):
        raise InputException(dataFrame, "Not DataFrame exception!")
    parseCheck(MyFilterVisitor(dataFrame), filterString)
    return dataFrame.selectExpr(filterString)

def cacheWrapper(table):
    #type: (DataFrame)->None
    if not isinstance(table,DataFrame):
        raise InputException(table, "Table needs to be of type pyspark.sql.DataFrame")
    return table.cache()

def collectWrapper(table) :
    #type: (DataFrame) ->List[Row]
    if not isinstance(table,DataFrame):
        raise InputException(table, "Table needs to be of type pyspark.sql.DataFrame")
    return table.collect()

def countWrapper(table):
    #type: (DataFrame)->int
    if not isinstance(table,DataFrame):
        raise InputException(table, "Table needs to be of type pyspark.sql.DataFrame")
    return table.count()

def persistWrapper(table, level):
    #type: (DataFrame,StorageLevel)->None
    if not isinstance(table,DataFrame):
        raise InputException(table, "Table needs to be of type pyspark.sql.DataFrame")
    if not isinstance(level,StorageLevel):
        raise InputException(level, "Level needs to be of type pyspark.StorageLevel")
    return table.persist(level)

def broadcastWrapper(sc,value) :
    #type: (SparkContext,DataFrame)->Broadcast
    if isinstance(value,DataFrame):
        #FIXME -- add some code around size check
        return sc.broadcast(value)
    else:
        return sc.broadcast(value)

def groupByWrapper(table, cols = None):
    # type: (DataFrame, Union[str, List[str], None]) -> DataFrame
    if not isinstance(table, DataFrame):
        raise InputException(table, "Table needs to be of type pyspark.sql.DataFrame")
    if not isinstance(cols, list): cols = [cols]
    for c in cols:
        if not c in table.columns:
            raise InputException(cols, "The grouping column '" + c + "' is not present in the table")
    return table.groupBy(cols)



function.bin_withQueryImpl.py
from pyspark.sql import DataFrame
import numpy as np
from pythontestmodule.function.acewrapper import selectExprWrapper

def binfn(dataframe, targetCol, sourceCol, rangeMap={}, rangeCol="None"):
    
    if not isinstance(dataframe, DataFrame):
        raise Exception("The inputTable parameter must be of type pyspark.sql.DataFrame")
    
    if not isinstance(targetCol, str):
        raise Exception("The inputTable parameter must be of type str")
    if not isinstance(sourceCol, str):
        raise Exception("The inputTable parameter must be of type str")
        
    if not isinstance(rangeMap, dict):
        raise Exception("The inputTable parameter must be of type dict")
    
    if not isinstance(rangeCol, str):
        raise Exception("The inputTable parameter must be of type str")
    
        
    getRange = True
    if rangeCol in ["NONE","None","none"]:
        getRange = False
    
    #Create binning query           
    query = "case"        
    for pair in rangeMap:   
        query = query + " when " + sourceCol + " >= " + str(pair[0]) + " AND " + sourceCol +" < " + str(pair[1]) +  \
        " then \"" + str(rangeMap[pair])+ "\""
    query = query + " end as " + targetCol
    
        
    cols = dataframe.columns
    queryCols =[]
    for col in cols:
        if str.lower(col) != str.lower(targetCol):
            queryCols.append(col)  
    queryCols.append(query)
    #print("queryCols" + str(queryCols))
    
    if getRange is True:
        rangeStrQuery = " case"
        for pair in rangeMap:
            rangeStrQuery = rangeStrQuery + " when " +  sourceCol + " >= " + str(pair[0]) + " AND " + sourceCol +" < " + str(pair[1]) + \
            " then \"" + str(pair[0]) + "-" + str(pair[1]) + "\"" 
        rangeStrQuery = rangeStrQuery + " end as " + rangeCol
        queryCols.append(rangeStrQuery)
            
    return dataframe.selectExpr(queryCols)

function.bin
from pyspark.sql import DataFrame

from pythontestmodule.data.dataexception import InputException

def binfn(dataframe, targetCol, sourceCol, rangeMap={}, rangeCol = "None") :
    #type:(DataFrame,str,str,Dict[str,str],str)-> DataFrame
    if not isinstance(dataframe, DataFrame):
        raise InputException(dataframe,"The inputTable parameter must be of type pyspark.sql.DataFrame")    
    if not isinstance(targetCol, str):
        raise InputException(targetCol,"The inputTable parameter must be of type str")
    if not isinstance(sourceCol, str):
        raise InputException(sourceCol,"The inputTable parameter must be of type str")        
    if not isinstance(rangeMap, dict):
        raise InputException(rangeMap,"The inputTable parameter must be of type RangeKey")
    
    if not isinstance(rangeCol, str):
        raise Exception(rangeCol,"The inputTable parameter must be of type str")
    
        
    getRange = True
    if rangeCol in ["NONE","None","none"]:
        getRange = False
    
    #Create binning query           
    query = "case"        
    for pair in rangeMap:   
        query = query + " when " + sourceCol + " >= " + str(pair[0]) + " AND " + sourceCol +" < " + str(pair[1]) +  \
        " then \"" + str(rangeMap[pair])+ "\""
    query = query + " end as " + targetCol
    
        
    cols = dataframe.columns
    queryCols =[]
    for col in cols:
        if str.lower(col) != str.lower(targetCol):
            queryCols.append(col)  
    queryCols.append(query)
    #print("queryCols" + str(queryCols))
    
    if getRange is True:
        rangeStrQuery = " case"
        for pair in rangeMap:
            rangeStrQuery = rangeStrQuery + " when " +  sourceCol + " >= " + str(pair[0]) + " AND " + sourceCol +" < " + str(pair[1]) + \
            " then \"" + str(pair[0]) + "-" + str(pair[1]) + "\"" 
        rangeStrQuery = rangeStrQuery + " end as " + rangeCol
        queryCols.append(rangeStrQuery)
            
    return dataframe.selectExpr(queryCols)

function.byteBufferConvert
'''
Created on Nov 7, 2016

@author: an98974
Read a dataframe to identify byteBuffer fields and convert them to big decimal 
'''
from binascii import hexlify
from decimal import Decimal
from pyspark.sql import DataFrame
from pyspark.sql.types import DecimalType
from pyspark.sql.functions import udf
from typing import List,Optional

def __convertBinary(field):
    if field is not None:
        field = Decimal(int(hexlify(field), 16)) / pow(10, 18)
    else:
        field = Decimal(0.)
    return field

def parseColumnsConvertBinary(df, binary_columns = [], nonbinary_columns = [], onlybinary = False):
    # type: (DataFrame, Optional[List[str]], Optional[List[str]], Optional[bool]) -> List[List[str]]
    udfBinaryToDecimal = udf(__convertBinary,DecimalType(38, 18))
    batchCols = []  # type: List[List[str]]
    binaryCount = 0
    
    if not (binary_columns or nonbinary_columns):
        columns = df.columns
        parseCols = columns[:]
        delCols = []  # type: List[int]
        for i in range(len(columns)):
            item = df.dtypes[i]
            if item[1] == 'binary':
                print('Converting binary column ' + item[0])
                if binaryCount == 100:
                    print('Reached 100 binary columns, starting new batch')
                    batchCols.append(parseCols)
                    parseCols = columns[:]
                    binaryCount = 0
                parseCols[i] = udfBinaryToDecimal(item[0]).alias(item[0])
                binaryCount = binaryCount + 1
            elif onlybinary:
                delCols.append(i)
        delCols.reverse()
        for c in delCols:
            print('Removing non-binary column ' + str(parseCols[c]))
            parseCols.pop(c)
        batchCols.append(parseCols)
    else:
        if onlybinary: nonbinary_columns = []
        parseCols = binary_columns + nonbinary_columns
        for i in range(len(binary_columns)):
            column = binary_columns[i]
            print('Converting binary column ' + column)
            if binaryCount == 100:
                print('Reached 100 binary columns, starting new batch')
                batchCols.append(parseCols)
                parseCols = binary_columns.append(nonbinary_columns)
                binaryCount = 0
            parseCols[i] = udfBinaryToDecimal(column).alias(column)
            binaryCount = binaryCount + 1
        batchCols.append(parseCols)
    
    return batchCols

function.concatFunc.py
'''
Created on Nov 7, 2016

@author: an98974

outputTable = citidata.function.concat(inputTable, newFieldName,'delimiter',['field1','field2']);

'''
from pyspark.sql.functions import concat_ws
from pyspark import SparkContext, SparkConf
from pyspark.sql.column import Column
from datetime import datetime , timedelta
from pyspark.sql.functions import udf, lit
from pyspark.sql import DataFrame,SQLContext
from pyspark.sql.types import StringType
from pythontestmodule.data.dataexception import InputException


def __appendFields(sep, *fields):
    #type:(Column, *str) -> Column
    values = []
    for field in fields:
        try:
            values.append(str(field.decode('unicode_escape').encode('ascii','ignore')))
        except:
            values.append(str(field))
    return sep.join(values)

def concatColsData(df, newColumn, delimiter, fields ):
    #type:(DataFrame,str,str,List[str]) -> DataFrame
    if not isinstance(df,DataFrame):
        raise Exception("The table name param must be of type pyspark.sql.DataFrame")
    concatColumns = [ df[field] for field in fields ]
    udfConcat  =  udf(__appendFields,StringType())
    return df.withColumn(newColumn, udfConcat(lit(delimiter), *tuple(concatColumns)))

if __name__=='__main__':
    sc = SparkContext()
    sqlContext = SQLContext(sc)
    print("starting program")
    df = sqlContext.read.format("com.databricks.spark.avro").load("/data/gftrrhrn/managed/hive/PROFILED_ACT/country_code=MX/organization_code=GCB/product_code=CRD/data_unit=BANAMEX/rpt_prd=201511/version=20160622_162800_V6")
    print("Hadoop data read completed")
    df = df.select('ACCOUNT_NUMBER','REPORTING_PERIOD','ACQUISITION_DATE','ACCOUNT_OPEN_DATE','PRODUCT_CCAR_TYPE','CHARGE_OFF_FLAG','ENDING_NET_RECEIVABLES_LOCAL','LOCAL_PRODUCT_ID','LOAN_AMOUNT','LOCAL_CHANNEL_ID','BALANCES_MIX_CASH_CYCLE_END','BALANCES_MIX_OTHER_CYCLE_END','BALANCES_MIX_PENALTY_CYCLE_END','BALANCES_MIX_PROMO_CYCLE_END','ACCOUNT_STATUS_CLOSED_MOED','ORIGINAL_PRINCIPAL_AMOUNT','LOAN_SOURCE_OR_CHANNEL','ORIGIN_CHANNEL','GROSS_OUTSTNG_TOT_BAL_MOED','REPORTING_PERIOD','LOAN_MODIFICATION_FLAG','TOTAL_CREDIT_LINE','ACCOUNT_NUMBER','GROSS_OUTSTNG_PRIN_BAL_MOED')
    print("Hadoop data columns selected")
    #df.select('ENDING_NET_RECEIVABLES_LOCAL').collect()[1]
    # print(" printing the byte array ................................")
    delimiter = ','
    fields = ['ENDING_NET_RECEIVABLES_LOCAL','REPORTING_PERIOD','PRODUCT_CCAR_TYPE','ACCOUNT_OPEN_DATE']
    dataFrame = concatColsData(df, delimiter.join(fields), delimiter, fields)
    dataFrame.select(delimiter.join(fields)).show(4)

function.diffDate.py
'''
Created on Nov 2, 2016

@author: mm79548

outputTable = citidata.function.diffDate(inputTable, calculated_value, fieldDate1, fieldDate2, differenceCriteria)

'''
from pyspark import SparkContext
from datetime import datetime,timedelta, date
from dateutil.relativedelta import relativedelta
from pyspark.sql.functions import lit,udf
from pyspark.sql import DataFrame,SQLContext
from pyspark.sql.types import StringType
from pythontestmodule.data.dataexception import InputException
from typing import Union
def __diffDateConverter(firstDate, secondDate, differenceCriteria):
    #type:(date,date,str) -> Union[float, int] 
    try:
        if firstDate is not None and secondDate is not None:
            #date1 = datetime.strptime(str(fieldDate1).strip(), date1Format)
            #date2 = datetime.strptime(str(fieldDate2).strip(), date2Format)
            if differenceCriteria == "Days" or differenceCriteria == "days":
                if secondDate.day and firstDate.day is not None:
                    #retrun diff in days
                    return abs((secondDate - firstDate).days)
                else:
                    raise InputException([firstDate, secondDate], "One of the Input Date is reporting period (YYYYmm) and difference can not be calculated in days!")
            elif differenceCriteria == "Months" or differenceCriteria == "months":
                #return diff in months
                return abs((secondDate.year - firstDate.year)*12) + abs(secondDate.month - firstDate.month)
            elif differenceCriteria == "Weeks" or differenceCriteria == "weeks":
                if secondDate.day and firstDate.day is not None:
                    start_week = firstDate - timedelta(days=firstDate.weekday())
                    end_week = secondDate - timedelta(days=secondDate.weekday())
                    #return diff in weeks
                    return abs((end_week - start_week).days)/7
                else:
                    raise InputException([firstDate, secondDate], "One of the Input Date is reporting period (YYYYmm) and difference can not be calculated in weeks!")
            elif differenceCriteria == "Years" or differenceCriteria == "years":
                fullYear = secondDate.year - firstDate.year -1
                monthdiff = 12-firstDate.month +secondDate.month
                return fullYear+monthdiff/12+monthdiff%12*.1
            
                #diff = relativedelta(firstDate, secondDate)
                #diffInYrs=diff.years+(diff.months/12)+(diff.days/366)
                #return diffInYrs                                
            else:
                raise InputException(differenceCriteria, "Given diff criteria is not valid")
        else:
            raise InputException([firstDate, secondDate], "Input parameters firstDate or secondDate is null")
    except InputException as err: raise err

def diffDateWithDates(table_name,calculated_value, date1,date2,differenceCriteria):
    #type:(DataFrame,str,date,date, str) -> DataFrame
    if not isinstance(table_name, DataFrame):
        raise InputException(table_name, "The table_name parameter must be of type pyspark.sql.DataFrame")
    if not isinstance(differenceCriteria, str):
        raise InputException(differenceCriteria, "The inputTable parameter 'differenceCriteria' must be of type str")

    udfDifferenceDate = udf(__diffDateConverter,StringType()) 
    dataFrame = table_name.withColumn(calculated_value, udfDifferenceDate(date1 ,date2,lit(differenceCriteria)))
    return dataFrame


def diffDate(table_name, calculated_value, fieldDate1, fieldDate2, differenceCriteria):
    #type:(DataFrame,str,str,str, str) -> DataFrame
    if not isinstance(table_name, DataFrame):
        raise InputException(table_name, "The table_name parameter must be of type pyspark.sql.DataFrame")
    if not isinstance(differenceCriteria, str):
        raise InputException(differenceCriteria, "The inputTable parameter 'differenceCriteria' must be of type str")

    udfDifferenceDate = udf(__diffDateConverter,StringType()) 
    dataFrame = table_name.withColumn(calculated_value, udfDifferenceDate(fieldDate1 ,fieldDate2,lit(differenceCriteria)))
    return dataFrame

if __name__ == '__main__': 
    
    '''
    sc = SparkContext()
    sqlContext = SQLContext(sc)
    print("starting program")
    df = sqlContext.read.format("com.databricks.spark.avro").load("/data/gftrrhrn/managed/hive/PROFILED_ACT/country_code=MX/organization_code=GCB/product_code=CRD/data_unit=BANAMEX/rpt_prd=201511/version=20160622_162800_V6")
    print("Hadoop data read completed")
    df = df.select('ACCOUNT_NUMBER','REPORTING_PERIOD','ACQUISITION_DATE','ACCOUNT_OPEN_DATE','PRODUCT_CCAR_TYPE','CHARGE_OFF_FLAG','ENDING_NET_RECEIVABLES_LOCAL','LOCAL_PRODUCT_ID','LOAN_AMOUNT','LOCAL_CHANNEL_ID','BALANCES_MIX_CASH_CYCLE_END','BALANCES_MIX_OTHER_CYCLE_END','BALANCES_MIX_PENALTY_CYCLE_END','BALANCES_MIX_PROMO_CYCLE_END','ACCOUNT_STATUS_CLOSED_MOED','ORIGINAL_PRINCIPAL_AMOUNT','LOAN_SOURCE_OR_CHANNEL','ORIGIN_CHANNEL','GROSS_OUTSTNG_TOT_BAL_MOED','REPORTING_PERIOD','LOAN_MODIFICATION_FLAG','TOTAL_CREDIT_LINE','ACCOUNT_NUMBER','GROSS_OUTSTNG_PRIN_BAL_MOED')
    print("Hadoop data columns selected")
    #df.show(1)
  
    dataFrame = diffDate(df, "ACCOUNT_AGE_IN_MONTHS","ACCOUNT_OPEN_DATE", "REPORTING_PERIOD", 'months')
    dataFrame.show(10)
    
    dataFrame1 = diffDate(df, "ACCOUNT_AGE_IN_WEEKS","ACCOUNT_OPEN_DATE", "REPORTING_PERIOD", 'weeks')
    dataFrame1.show(10)
    
    dataFrame2 = diffDate(df, "ACCOUNT_AGE_IN_DAYS","ACCOUNT_OPEN_DATE", "REPORTING_PERIOD", 'days')
    dataFrame2.show(10)
    
    dataFrame2 = diffDate(df, "ACCOUNT_AGE_IN_YEARS","ACCOUNT_OPEN_DATE", "REPORTING_PERIOD", 'years')
    dataFrame2.show(10)
    '''
function.drop.py
from pyspark.sql import DataFrame
from pythontestmodule.data.dataexception import InputException
from typing import Union
def drop(inputTable, dropCols):
    #type:(DataFrame, Union[str, List[str]]) -> DataFrame
    if not isinstance(inputTable, DataFrame):
        raise InputException(inputTable, "The inputTable parameter must be of type pyspark.sql.DataFrame")
    
    inputCols = inputTable.columns
    if not isinstance(dropCols, list): dropCols = [dropCols]
    
    for col in dropCols:
        if col in inputCols:
            inputTable = inputTable.drop(col)
        else:
            raise InputException(col, "The provided column name '" + col + "' does not exist in the input table")
    
    return inputTable

function.enrich.py
from pyspark.sql import DataFrame
from pyspark.sql.functions import udf
from pyspark.sql.types import IntegerType,StringType
from pythontestmodule.data.dataexception import InputException

def calcRewriteId(ccarProdCd, baslPortId):
    #type:(str, str) -> int
    if ccarProdCd == "CA_BC" and (baslPortId == "MXUI018" or baslPortId == "MXUI019"): return 1
    else: return 0

def calcRewriteDate(rewriteId, bkgDt):
    
    if rewriteId == 1: return bkgDt
    else: return None

def doRewriteEnrich(dataFrame):
    #type:( DataFrame) -> DataFrame
    if not isinstance(dataFrame, DataFrame):
        raise InputException(dataFrame, "The dataFrame parameter must be of type pyspark.sql.DataFrame")
    
    dfCols = dataFrame.columns
    reqCols = ["PRODUCT_CCAR_TYPE", "BASEL_PORTFOLIO_ID", "ACCOUNT_OPEN_DATE"]
    missing = []
    for i in range(len(reqCols)):
        if not reqCols[i] in dfCols:
            missing.append(reqCols[i])
    if missing:
        raise InputException(dfCols, "Input table is missing required column(s) ['" + "', '".join(missing) + "']")
    
    udfCalcRewriteId = udf(calcRewriteId, IntegerType())
    dataFrame = dataFrame.withColumn("RewriteID", udfCalcRewriteId("PRODUCT_CCAR_TYPE", "BASEL_PORTFOLIO_ID"))
    udfCalcRewriteDate = udf(calcRewriteDate, StringType())
    dataFrame = dataFrame.withColumn("RewriteDate", udfCalcRewriteDate("RewriteID", "ACCOUNT_OPEN_DATE"))
    return dataFrame

function.fillNull.py
from pyspark.sql import DataFrame, SQLContext
from pythontestmodule.data.dataexception import InputException
import sys
from pyspark.sql.window import Window
import pyspark.sql.functions as func

def fillNull(inputTable, inputKey, orderCol, outCol):
    if not isinstance(inputTable, DataFrame):
        raise InputException(inputTable, "The inputTable parameter must be of type pyspark.sql.DataFrame")
    if not isinstance(inputKey, str): 
        raise InputException(retainCol,"The inputTable parameter 'inputKey' must be of type str")
    if not isinstance(outCol, str): 
        raise InputException(outCol,"The inputTable parameter 'outCol' must be of type str")
    
    df_na = inputTable.na.fill(-1)
    lag = df_na.withColumn('outCol_lag', func.lag(outCol, default=-1).over(Window.partitionBy(inputKey).orderBy(orderCol)))
    switch = lag.withColumn('outCol_change', ((lag[outCol] != lag['outCol_lag']) & (lag[outCol] != -1)).cast('integer'))
    switch_sess = switch.withColumn('sub_inputKey', func.sum("outCol_change").over(Window.partitionBy(inputKey).orderBy(orderCol).rowsBetween(-sys.maxsize, 0)))
    switch_fill = switch_sess.withColumn('outCol_rp', func.first(outCol).over(Window.partitionBy(inputKey, 'sub_inputKey').orderBy(orderCol)))
    switch_na = switch_fill.replace(-1, 'null')
    filledDf = switch_na.drop(outCol).drop('outCol_lag').drop('outCol_change').drop('sub_inputKey').withColumnRenamed('outCol_rp', outCol)

    return filledDf

function.first.py
from pythontestmodule.function.drop import drop
from pyspark import StorageLevel
from pyspark.sql import DataFrame
from pyspark.sql.functions import rank
from pyspark.sql.window import Window
from pythontestmodule.data.dataexception import InputException
from pythontestmodule.function.acewrapper import filterWrapper,persistWrapper
from typing import List,Optional,Union

def first(inputTable, groupCols, orderCols, filterCriteria = None):
    # type: (DataFrame, Union[str, List[str]], Union[str, List[str]], Optional[str]) -> DataFrame
    if not isinstance(inputTable, DataFrame):
        raise InputException(inputTable, "The inputTable parameter must be of type pyspark.sql.DataFrame")
    
    if not isinstance(groupCols, list): groupCols = [groupCols]
    if not isinstance(orderCols, list): orderCols = [orderCols]
    cols = inputTable.columns
    for g in groupCols:
        if not g in cols:
            raise InputException(groupCols, "The provided group column name '" + g + "' does not exist in the input table")
    for o in orderCols:
        if not o in cols:
            raise InputException(orderCols, "The provided order column name '" + o + "' does not exist in the input table")
    
    if not filterCriteria is None:
        inputTable = filterWrapper(inputTable, filterCriteria)
    w = Window.partitionBy(groupCols).orderBy(orderCols)
    inputTable = inputTable.withColumn("RANK", rank().over(w))
    inputTable = drop(filterWrapper(inputTable, "(RANK = 1)"), "RANK")
    persistWrapper(inputTable, StorageLevel(True, True, False, True))
    
    return inputTable

function.fmtFunc.py
'''
Created on Nov 28, 2016

@author: an98974
Reads a dataframe and new values to replace the old values of a certain column 
'''
from pyspark import SparkContext
from pyspark.sql import DataFrame,SQLContext
from pythontestmodule.data.dataexception import InputException
from typing import Any
def fmt(dataFrame, column, valueMap):
    #type:(DataFrame,str,Dict[Any, Any]) -> DataFrame
    if not isinstance(dataFrame, DataFrame):
        raise InputException(dataFrame, "The dataFrame parameter must be of type pyspark.sql.DataFrame")
    if not isinstance(valueMap, dict):
        raise InputException(valueMap, "The valueMap parameter must be of type dict of key value pair to replace")
    return dataFrame.replace(list(valueMap.keys()),list(valueMap.values()),column)

if __name__ == "__main__":
    sc = SparkContext()
    sqlContext = SQLContext(sc)
    df = sqlContext.read.format("com.databricks.spark.avro").load("/data/gftrrhrn/managed/hive/PROFILED_ACT/country_code=AU/organization_code=GCB/product_code=MTG/data_unit=ALL/rpt_prd=201511/version=20160812_050440_V6")
    column ='REPORTING_PERIOD'
    df.select('REPORTING_PERIOD').show(5)
    print("after function call")
    txmap={'201511':'201512'}
    dataframe = fmt(df,column, txmap );
    dataframe.select(column).show(5);

function.gradientBoosting.py
import pythontestmodule.function.mlib.gbm as gbm
#from pyspark.sql import DataFrame
from pythontestmodule.data.dataexception import InputException
from datetime import date,datetime
from pythontestmodule.data.hdfsFrames import saveCsv
from pythontestmodule.util.login import Session
from pythontestmodule.function.mlib.gbm import summary
import subprocess
import pandas as pd

def gradientBoosting(df, 
                     model, 
                     formula, 
                     graph, 
                     distribution = "gaussian", 
                     var_monotone = "NA", 
                     ntrees = 1000, 
                     shrinkage = 0.05, 
                     interaction_depth = 3, 
                     bag_fraction = 0.5, 
                     train_fraction = 0.5, 
                     n_minobsinnode = 10,
                     cv_folds = 3):
    
#     if not isinstance(df, DataFrame):
#         raise InputException(df, 'The input table parameter must be of type pyspark.sql.dataFrame')
    if not isinstance(model, str):
        raise InputException(model, 'The input model must be of type string')
    if not isinstance(graph, str):
        raise InputException(graph, 'The input graph must be of type string')
#     if not isinstance(session, Session):
#         raise InputException(session, 'The input graph must be of type string')
    if not isinstance(formula, str):
        raise InputException(formula, 'The input weights must be of type string')
    
    #dfFile = '/user/gftrrasn/ace_poc/AU/tblPlus'+str(datetime.now().strftime('%m_%d_%Y')+'.csv')
    #dfFile = '/data/gftrrhrn/tmp/analytics_results/au/csv/'+str(datetime.now().strftime('%m_%d_%Y')+'.csv')
    #saveCsv(session, df, dfFile, True)
    #subprocess.call(["hadoop", "fs", "-get", dfFile])
    
    #glm.readcsv("aumtgdf", dfFile)
    gbm.readcsv("inputdf", df)
    gbm.gbm("inputdf", 
            model,
            formula, 
            graph, 
            distribution, 
            var_monotone, 
            ntrees,
            shrinkage = 0.05, 
            interaction_depth = 3, 
            bag_fraction = 0.5, 
            train_fraction = 0.5, 
            n_minobsinnode = 10,
            cv_folds)
    #subprocess.call(["hadoop", "fs", "-rm", "-f", dfFile])
    summary(model)
    
    resultsDF = pd.DataFrame.from_dict(dict([("var", glm.var(model)),("relinf", glm.relinf(model))]))
    
    #return dict([("var", gbm.var(model)),("relinf", gbm.relinf(model))])
    return(resultsDF)

function.join.py
from pyspark.sql import DataFrame
from pythontestmodule.data.dataexception import InputException
from typing import List,Optional,Tuple,Union

def joinTables(tableLeft, tableRight, joinCols, joinType = "inner") :
    # type: (DataFrame, DataFrame, Union[Tuple[str, str], List[Tuple[str, str]]], Optional[str]) -> DataFrame
    if not isinstance(tableLeft, DataFrame):
        raise InputException(tableLeft, "The tableLeft parameter must be of type pyspark.sql.DataFrame")
    if not isinstance(tableRight, DataFrame):
        raise InputException(tableRight, "The tableRight parameter must be of type pyspark.sql.DataFrame")
    
    validJoinType = ["inner","left_outer","right_outer"]
    
    tableLeftCols = tableLeft.columns
    tableRightCols = tableRight.columns
    if not isinstance(joinCols, list): joinCols = [joinCols]
    if joinType in ["left","right"]: joinType += "_outer"
    
    if not joinType in validJoinType:
        raise InputException(joinType, "The provided joinType '" + joinType + "' is invalid; must be one of ['" + "', '".join(validJoinType) + "']")
    
    joinCond = []
    dropCols = []
    for i in range(len(joinCols)):
        leftCol = joinCols[i][0]
        if not leftCol in tableLeftCols:
            raise InputException(leftCol, "The provided join column name '" + leftCol + "' does not exist in the left hand table")
        rightCol = joinCols[i][1]
        if not rightCol in tableRightCols:
            raise InputException(rightCol, "The provided join column name '" + rightCol + "' does not exist in the right hand table")
        if leftCol == rightCol:
            if joinType == "right_outer":
                leftCol += "_DROP"
                tableLeft = tableLeft.withColumnRenamed(rightCol, leftCol)
                dropCols.append(leftCol)
            else:
                rightCol += "_DROP"
                tableRight = tableRight.withColumnRenamed(leftCol, rightCol)
                dropCols.append(rightCol)
        joinCond.append(tableLeft[leftCol] == tableRight[rightCol])
    
    dfJoin = tableLeft.join(tableRight, joinCond, joinType)
    
    for i in range(len(dropCols)):
        dfJoin = dfJoin.drop(dropCols[i])
    
    return dfJoin

function.keepfunction.py
from pyspark.sql import DataFrame
from typing import List

def keep(dataframe, fields):
    # type: (DataFrame, List[str]) -> DataFrame
    if not isinstance(dataframe, DataFrame):
        raise Exception("The inputTable parameter must be of type pyspark.sql.DataFrame")
    
    for field in fields:
        assert type(field) == type("")
    
    return dataframe.select(fields)

function.lag.py
from pyspark.sql import DataFrame
import pyspark.sql.functions as fns
from pyspark.sql.window import Window
from pythontestmodule.data.dataexception import InputException
from typing import List,Union

def lag(inputTable, groupCols, orderCols, lagCol, lagCount, outputCol):
    # type: (DataFrame, Union[str, List[str]], Union[str, List[str]], str, int, str) -> DataFrame
    if not isinstance(inputTable, DataFrame):
        raise InputException(inputTable, "The inputTable parameter must be of type pyspark.sql.DataFrame")
    
    cols = inputTable.columns
    if not isinstance(groupCols, list): groupCols = [groupCols]
    if not isinstance(orderCols, list): orderCols = [orderCols]
    if not lagCol in cols:
        raise InputException(lagCol, "The provided column name '" + lagCol + "' does not exist in the input table")
    for g in groupCols:
        if not g in cols:
            raise InputException(g, "The provided group column name '" + g + "' does not exist in the input table")
    for o in orderCols:
        if not o in cols:
            raise InputException(o, "The provided order column name '" + o + "' does not exist in the input table")
    
    w = Window.partitionBy(groupCols).orderBy(orderCols)
    return inputTable.withColumn(outputCol, fns.lag(lagCol, lagCount).over(w))

function.logRegression.py
import pythontestmodule.function.mlib.glm as glm
from pyspark.sql import DataFrame
from pythontestmodule.data.dataexception import InputException
from pythontestmodule.util.login import Session
import subprocess
import pandas as pd
import pythontestmodule.util.hdfsUtils
from pythontestmodule.util import hdfsUtils

def logisticRegression(dfpath, localpath, model, graph, session, formula, weights="reg_weight", maxIt=50, family="binomial", tableNameR="aumtgdf"):
    #type: (str, str, str, str, Session, str, str, int, str) -> pd.DataFrame
    
    if not isinstance(dfpath, str):
        raise InputException(dfpath, 'The dfpath table must be of type string')
    if not isinstance(localpath, str):
        raise InputException(localpath, 'The input localpath must be of type string')
    if not isinstance(model, str):
        raise InputException(model, 'The input model must be of type string')
    if not isinstance(graph, str):
        raise InputException(graph, 'The input graph must be of type string')
    if not isinstance(session, Session):
        raise InputException(session, 'The input session must be of type Session')
    if not isinstance(formula, str):
        raise InputException(formula, 'The input formula must be of type string')
    if not isinstance(weights, str):
        raise InputException(weights, 'The input weights must be of type string')
    if not isinstance(maxIt, int):
        raise InputException(maxIt, 'The input maxIt must be of type string')
    if not isinstance(family, str):
        raise InputException(family, 'The input family must be of type string')
    if not isinstance(tableNameR, str):
        raise InputException(tableNameR, 'The input tableNameR must be of type string')
    
    hdfsUtils.copyToLocal(dfpath, localpath, True)
    #subprocess.call(["hadoop", "fs", "-get", dfpath, localpath])
    glm.readcsv(tableNameR, localpath)
    glm.glm(tableNameR, model, graph, formula, family, weights, maxIt)
    glm.summary(model)
    
    resultsDF = pd.DataFrame.from_dict(dict([("coef", glm.coef(model)),("stderror", glm.stderror(model))]))

    #return dict([("coef", glm.coef(model)),("residuals", glm.residuals(model)),("graph",glm.graph(graph))])
    return resultsDF

function.makeCase.py
from pyspark.sql import DataFrame
from pyspark.sql.functions import lower,upper
from pythontestmodule.data.dataexception import InputException

def makeCase(df, col, case):
    #type:(DataFrame,str,str) -> DataFrame
    if not isinstance(df, DataFrame):
        raise InputException(df, "The df parameter must be of type pyspark.sql.DataFrame")
    if not col in df.columns:
        raise InputException(col, "The provided column name '" + col + "' does not exist in the input table")
    for dtype in df.dtypes:
        if dtype[0] == col and dtype[1] != 'string':
            raise InputException(col, "The 'makeCase' function can only be performed on a column of type string")
    validCase = ["lower","upper"]
    case = case.lower()
    if not case in validCase:
        raise InputException(case, "The provided case '" + case + "' is invalid; must be one of ['" + "', '".join(validCase) + "']")
    
    if case == "lower": return df.withColumn(col, lower(df[col]))
    else: return df.withColumn(col, upper(df[col]))

function.math.py
from pyspark.sql import DataFrame
import pyspark.sql.functions as fns
from pythontestmodule.data.dataexception import InputException

def mathFn(inputDf, inputCol, mathFunc, outputCol):
    #type:(DataFrame,str,str,str) -> DataFrame
    if not isinstance(inputDf, DataFrame):
        raise InputException(inputDf, "The inputDf parameter must be of type pyspark.sql.DataFrame")
    
    if not inputCol in inputDf.columns:
        raise InputException(inputCol, "The provided column name '" + inputCol + "' does not exist in the input table")
    
    try:
        return inputDf.withColumn(outputCol, getattr(fns, mathFunc)(inputDf[inputCol]))
    except AttributeError as ae:
        print("Caught AttributeError: " + str(ae))
        raise InputException(mathFunc, "Provided mathFunc '" + mathFunc + "' is not a known python math function")

function.moveDate.py
from calendar import monthrange
from datetime import datetime,timedelta
from dateutil.relativedelta import relativedelta
from pyspark import SparkContext
from pyspark.sql import DataFrame,SQLContext
from pyspark.sql.functions import lit,udf
from pyspark.sql.types import DateType
from pythontestmodule.data.dataexception import InputException

def __dateConverter(parsedDate, duration, period, endOrStart):
    #type:(datetime,int,str,str)->str
    if parsedDate is not None and duration is not None:
        #=======================================================================
        # parsedDate = None  # type: datetime
        # try:
        #     #ACT_DT
        #     parsedDate = datetime.strptime(date_field, '%d%b%Y')
        # except ValueError:
        #     #RPT_PERIOD
        #     parsedDate = datetime.strptime(date_field, '%Y%m')
        #=======================================================================
        if endOrStart.lower() == 'eom':
            if period.lower() == 'day':
                newDay = parsedDate + timedelta(days = int(duration))
                return datetime(newDay.year, newDay.month, monthrange(newDay.year, newDay.month)[1])
            if period.lower() == 'month':
                newDate = parsedDate + relativedelta(months = int(duration))
                return datetime(newDate.year, newDate.month, monthrange(newDate.year, newDate.month)[1])
            if period.lower() == 'year':
                date_on_next_year = parsedDate.replace(year = parsedDate.year + int(duration))
                return datetime(date_on_next_year.year, date_on_next_year.month, monthrange(date_on_next_year.year, date_on_next_year.month)[1])
            else:
                raise InputException(period, "Criteria should be day, month or year")
        elif endOrStart.lower() == 'start':
            if period.lower() == 'day':
                newDay = parsedDate + relativedelta(days = int(duration))
                return datetime(newDay.year, newDay.month, 1)
            if period.lower() == 'month':
                newDate = parsedDate + relativedelta(months = int(duration))
                return datetime(newDate.year, newDate.month, 1)
            if period.lower() == 'year':
                date_on_next_year = parsedDate.replace(year = parsedDate.year + int(duration))
                return datetime(date_on_next_year.year, date_on_next_year.month, 1)
            else:
                raise InputException(period, "Criteria should be day, month or year")
        else:
            raise InputException(endOrStart, "Criteria should be end or start")
    else:
        return None

def movedate(table_name, newColumnName, date_field, duration, period, eom):
    #type:(DataFrame,str,str,int,str,str) -> DataFrame
    udfMoveDate = udf(__dateConverter,DateType())
    if not duration in table_name.columns:
        dataFrame = table_name.withColumn(newColumnName, udfMoveDate(date_field, lit(duration), lit(period), lit(eom)))
    else:
        dataFrame = table_name.withColumn(newColumnName, udfMoveDate(date_field, duration, lit(period), lit(eom)))
    return dataFrame

if __name__ == '__main__':
    sc = SparkContext()
    sqlContext = SQLContext(sc)
    df = sqlContext.read.format("com.databricks.spark.avro").load("/data/gftrrhrn/managed/hive/PROFILED_ACT/country_code=MX/organization_code=GCB/product_code=CRD/data_unit=BANAMEX/rpt_prd=201511/version=20160622_162800_V6")
    df = df.select('ACCOUNT_NUMBER','REPORTING_PERIOD','ACQUISITION_DATE','ACCOUNT_OPEN_DATE','PRODUCT_CCAR_TYPE','CHARGE_OFF_FLAG','ENDING_NET_RECEIVABLES_LOCAL','LOCAL_PRODUCT_ID','LOAN_AMOUNT','LOCAL_CHANNEL_ID','BALANCES_MIX_CASH_CYCLE_END','BALANCES_MIX_OTHER_CYCLE_END','BALANCES_MIX_PENALTY_CYCLE_END','BALANCES_MIX_PROMO_CYCLE_END','ACCOUNT_STATUS_CLOSED_MOED','ORIGINAL_PRINCIPAL_AMOUNT','LOAN_SOURCE_OR_CHANNEL','ORIGIN_CHANNEL','GROSS_OUTSTNG_TOT_BAL_MOED','REPORTING_PERIOD','LOAN_MODIFICATION_FLAG','TOTAL_CREDIT_LINE','ACCOUNT_NUMBER','GROSS_OUTSTNG_PRIN_BAL_MOED')
    df.show(1)
    dataFrame = movedate(df, 'close_date', 'ACCOUNT_OPEN_DATE',24, 'day', 'start')
    dataFrame.show(1)

function.nonlinearRegression.py
import pythontestmodule.function.mlib.nlsLM as nlsLM
from pyspark.sql import DataFrame
from pythontestmodule.data.dataexception import InputException
from datetime import date,datetime
from pythontestmodule.data.hdfsFrames import saveCsv
from pythontestmodule.util.login import Session
from pythontestmodule.function.mlib.nlsLM import readcsv
from pythontestmodule.function.mlib.nlsLM import summary
import subprocess
import pandas as pd

def nls(dfpath, localpath, session, model, formula, graph, start="parmStartList", weights="reg_weight", maxIter=100):
    
    if not isinstance(dfpath, str):
        raise InputException(dfpath, 'The dfpath table must be of type string')
    if not isinstance(localpath, str):
        raise InputException(model, 'The input localpath must be of type string')
    if not isinstance(model, str):
        raise InputException(model, 'The input model must be of type string')
    if not isinstance(graph, str):
        raise InputException(graph, 'The input graph must be of type string')
    if not isinstance(session, Session):
        raise InputException(session, 'The input graph must be of type string')
    if not isinstance(formula, str):
        raise InputException(formula, 'The input weights must be of type string')
    
    subprocess.call(["hadoop", "fs", "-get", dfpath, localpath])
    nlsLM.readcsv("au_df", localpath)
    nlsLM.nlsLM("au_df", model, graph, formula, start, weights, maxIter)
    summary(model)
    
    resultsDF = pd.DataFrame.from_dict(dict([("coef", nlsLM.coef(model)),("stderror", nlsLM.stderror(model))]))
    
    return (resultsDF)
    #return dict([("coef", nlsLM.coef(model)),("residuals", nlsLM.residuals(model))])


function.pandaFrame_withToPanda_ToLocalIterator_approach.py
from pyspark.sql import DataFrame
from pyspark import StorageLevel
from pythontestmodule.data.dataexception import InputException
import pandas as pd


#To DO 
'''
Do not use the below test method for direct usage rather use the acewrapper function : toPandaFrames(sparkDf,numberOfRecords)


'''

def pandaFrameTestFn(inputTable: DataFrame,numberOfRecords : int) -> pd.DataFrame:
    if not isinstance(inputTable, DataFrame):
        raise InputException(inputTable, "The inputTable parameter must be of type pyspark.sql.DataFrame")
    
    #Option -1
    '''
    pandaFrame=inputTable.toPandas()  -- internally using self.collect() - should be avoided
    '''
   
    #Option -2 
    
    '''
    Return an iterator that contains all of the elements in this RDD.The iterator will consume as much memory as the largest partition in this RDD.
    Note: this results in multiple Spark jobs, and if the input RDD is the result of a wide transformation (e.g. join with different partitioners), 
    to avoid recomputing the input RDD should be cached first.
    
    #recordRdd= inputTable.rdd
    #rowDataList=[rowData for rowData in recordRdd.toLocalIterator()]
    
    '''
    
    #Option -3 
    defaultNumberOfRecords = 10000 
    if numberOfRecords :
        defaultNumberOfRecords=numberOfRecords
        print("Number or records being converted to Panda Frame - " +str(defaultNumberOfRecords))
         
    pdf=pd.DataFrame.from_records(inputTable.take(defaultNumberOfRecords),columns=inputTable.columns)
    
       
    return pdf

function.pandaFrame.py
from datetime import datetime
from functools import partial
from json import JSONDecoder
from os import makedirs,path,sep
from pandas import DataFrame as pandaFrame
from py4j.java_gateway import JavaObject
from pyspark.context import SparkContext
from pyspark.sql import DataFrame
from pythontestmodule.data.dataexception import InputException
from pythontestmodule.util.properties import Properties
from simplejson import dump,load
from typing import Any,Dict,List,Optional,Union
import sys

def convert_rpt_to_string(reporting_period):
    # type: (Any) -> str
    return str(reporting_period)

def pandaFramesSerialize(pandaFramesMap,storeFileName):
    # type: (Dict[str,Dict[str,Any]], str) -> None
    props = Properties.getInstance()
    outputPath = props.get('app.pandaframes.data.path')
    with open(outputPath + sep + storeFileName, 'a') as f:
        dump(pandaFramesMap, f, encoding = 'utf-8', use_decimal = True)

def pandaFramesDeserialize(readFileName):
    # type: (str, str) -> Dict[str,Any]
    with open(readFileName, 'r') as f:
        deserializedPfMap = load(f, encoding = 'utf-8', use_decimal = True)
    return deserializedPfMap

def toPandaFrame(sparkFrame, numberOfRecords = 20000):
    # type: (DataFrame, Optional[int]) -> pandaFrame
    if numberOfRecords > 20000: numberOfRecords = 20000
    pf = pandaFrame.from_records(sparkFrame.take(numberOfRecords), columns = sparkFrame.columns, coerce_float = True)
    for item in sparkFrame.dtypes:
        if item[1] == 'date':
            pf[item[0]] = pf[item[0]].apply(convert_rpt_to_string)
    return pf

def toPandaFrames(sparkFramesMap, storeFileName, numberOfRecords):
    # type: (Dict[str,DataFrame], str, int) -> Dict[str,Dict[str,Any]]
    if not isinstance(sparkFramesMap, dict):
        raise InputException(sparkFramesMap, 'The sparkFramesMap parameter must be of type dict')
    
    pandaFramesMap = {}
    for sparkFrameKey,sparkFrameVal in sparkFramesMap.items():
        pf = toPandaFrame(sparkFrameVal, numberOfRecords)
        pandaFrameToDic = pf.to_dict(orient = 'list')
        pandaFramesMap[sparkFrameKey] = pandaFrameToDic
    pandaFramesSerialize(pandaFramesMap, storeFileName)
    return pandaFramesMap
    
'''
The below method is created to read json format cache data in client side location.
It should be used only in client side local visualization script.
'''
def getCacheFromServer(pandaFrameKey):
    # type: (str) -> pandaFrame
    '''
    Caller of this Python function should always provide the system arg "-i inputFilePath" as path of the cache file to execute the function.
    '''
    progArgs = getopts(sys.argv)
    inputFilePath = progArgs['-i']
    if not path.exists(inputFilePath):
        raise InputException(inputFilePath, 'Cache file is not available for location visualization/processing')
    pandaData = readCacheFromMultipleJsonObj(inputFilePath, pandaFrameKey)
    return pandaData

def readCacheFromMultipleJsonObj(filename,pandaFrameKey):
    with open(filename, 'r') as infh:
        for data in jsonParseByChunk(infh):
            for key,value in data.iteritems():
                if key == pandaFrameKey and pandaFrameKey == "ExecutionTime":
                    # Read non-pandas data
                    return value
                elif key == pandaFrameKey:
                    # Read pandas data
                    pandaFrameData = pandaFrame.from_dict(value)
                    return pandaFrameData

def jsonParseByChunk(fileobj, decoder = JSONDecoder(), buffersize = 2048):
    jsonBuffer = ''
    for chunk in iter(partial(fileobj.read, buffersize), ''):
        jsonBuffer += chunk
        while jsonBuffer:
            try:
                result, index = decoder.raw_decode(jsonBuffer)
                yield result
                jsonBuffer = jsonBuffer[index:]
            except ValueError:
                # Not enough data to decode, read more
                break

def getopts(argv):
    opts = {}
    while argv:
        if argv[0][0] == '-':  # Found a "-name value" pair.
            opts[argv[0]] = argv[1]
        argv = argv[1:]
    return opts

class PandaCache:
    def __init__(self):
        # type: (bytes, str) -> None
        #self.username = username
        #self.basePandaFilePath = Properties.getInstance().get('app.pandaframes.data.path')
        self.basePandaFilePathByUserIdProjectScripts = Properties.getInstance().getOpt('pandacacheloc')        
        self.pandaFolderCreationTm = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
            
    def __str__(self):
        # type: () -> str
        if self.basePandaFilePathByUserIdProjectScripts is not None:
            return self.basePandaFilePathByUserIdProjectScripts + " panda cache location "
        return "No cache location found "
    
    def pandaFramesSerializeToUserDir(self, pandaFramesMap):
        # type: (Dict[str,Dict[str,Any]], str) -> None
        #outputPath = self.basePandaFilePath + sep + self.username
        cacheDir = path.join(self.basePandaFilePathByUserIdProjectScripts, self.pandaFolderCreationTm)
        pandaFilePath = cacheDir + sep + 'accountDataPandaFrames.txt'  #TODO: Get this from property (sync with server side)
        if not path.exists(pandaFilePath):
            makedirs(cacheDir)
        with open(pandaFilePath, 'a') as f:
            dump(pandaFramesMap, f, encoding = 'utf-8', use_decimal = True)
    
    def convertJavaToDict(self, javaDataFrame, numberOfRecords):
        # type: (JavaObject) -> Dict[str, List[str]]
        result = {}
        sc = SparkContext.getOrCreate()
        cols = JSONDecoder().raw_decode(sc._jvm.com.citi.rh.enrichment.util.UtilFunction.toJsonObject(javaDataFrame.columns()))[0]
        for x in javaDataFrame.takeAsList(numberOfRecords):
            # Strip out leading '"[' and trailing ']"'
            y = sc._jvm.com.citi.rh.enrichment.util.UtilFunction.toJsonObject(x.toString())[2:-2].split(',')
            for i in range(len(y)):
                c = str(cols[i])
                if c in result:
                    result[c].append(y[i])
                else:
                    result[c] = [y[i]]
        return result
    
    '''
    The below method is created to send frame data to json format cache in server location.
    It should be used only in server side data preparation script.
    '''        
    def sendToCache(self, dataframeKey, dataFrameVal, numberOfRecords = 20000):
        # type: (str, Union[DataFrame, pandaFrame, JavaObject], Optional[int], Optional[str]) -> None
        # Convert pyspark DataFrame to pandas frame
        if isinstance(dataFrameVal, DataFrame):
            dataFrameVal = toPandaFrame(dataFrameVal, numberOfRecords)
        # Convert pandas frame to Python dictionary for JSON serialization
        if isinstance(dataFrameVal, pandaFrame):
            dataFrameVal = dataFrameVal.to_dict(orient = 'list')
        # Convert Java Spark DataFrame to Python dictionary for JSON serialization
        elif isinstance(dataFrameVal, JavaObject):
            dataFrameVal = self.convertJavaToDict(dataFrameVal, numberOfRecords)
        self.pandaFramesSerializeToUserDir({dataframeKey: dataFrameVal})

function.pyQtGrid.py
import sys
from PyQt5.QtCore import *
from PyQt5.QtGui import *


class PandaGrid(QTableWidget):
    def __init__(self, data, *args):
        QTableWidget.__init__(self, *args)
        self.data = data
        self.setgriddata()
        self.resizeColumnsToContents()
        self.resizeRowsToContents()
 
    def setgriddata(self):
        horHeaders = []
        for n, key in enumerate(sorted(self.data.keys())):
            #if key=='ACCOUNT_NUMBER':                
            #print(n,key)
            horHeaders.append(key)
            for m, item in enumerate(self.data[key]):
                #print(m,item)
                item1 = str(item)
                newitem = QTableWidgetItem(item1)
                self.setItem(m, n, newitem)
            self.setHorizontalHeaderLabels(horHeaders)

function.rank.py
from pyspark import SparkContext
from pyspark.sql import DataFrame,HiveContext,Window
import pyspark.sql.functions as fns

def rank(input_table, targetCol, rankField, *groupbyFields):
    #type:(DataFrame,str,str,*str)->DataFrame
    #create a window spec
    wSpec = Window.partitionBy(*groupbyFields).orderBy(rankField)
    dataFrame = input_table.withColumn(targetCol, fns.rank().over(wSpec))
    return dataFrame

if __name__ == '__main__':
    sc = SparkContext()
    sqlContext = HiveContext(sc)
    df = sqlContext.read.format("com.databricks.spark.avro").load("/data/gftrrhrn/managed/hive/PROFILED_ACT/country_code=MX/organization_code=GCB/product_code=CRD/data_unit=BANAMEX/rpt_prd=201511/version=20160622_162800_V6")
    df = df.select('ACCOUNT_NUMBER','REPORTING_PERIOD','ACQUISITION_DATE','ACCOUNT_OPEN_DATE','PRODUCT_CCAR_TYPE','CHARGE_OFF_FLAG','ENDING_NET_RECEIVABLES_LOCAL','LOCAL_PRODUCT_ID','LOAN_AMOUNT','LOCAL_CHANNEL_ID','BALANCES_MIX_CASH_CYCLE_END','BALANCES_MIX_OTHER_CYCLE_END','BALANCES_MIX_PENALTY_CYCLE_END','BALANCES_MIX_PROMO_CYCLE_END','ACCOUNT_STATUS_CLOSED_MOED','ORIGINAL_PRINCIPAL_AMOUNT','LOAN_SOURCE_OR_CHANNEL','ORIGIN_CHANNEL','GROSS_OUTSTNG_TOT_BAL_MOED','REPORTING_PERIOD','LOAN_MODIFICATION_FLAG','TOTAL_CREDIT_LINE','ACCOUNT_NUMBER','GROSS_OUTSTNG_PRIN_BAL_MOED')
    dataframe = rank(df, "rank", "ACCOUNT_OPEN_DATE", "REPORTING_PERIOD", "CHARGE_OFF_FLAG")
    dataframe.show(10)


function.rename.py
from pyspark.sql import DataFrame
from pythontestmodule.data.dataexception import InputException
from typing import Union,Tuple

def safeRename(inputTable, oldCol, newCol) :
    #type:(DataFrame,str,str)->DataFrame
    if oldCol in inputTable.columns:
        return inputTable.withColumnRenamed(oldCol, newCol)
    else:
        raise InputException(oldCol, "The provided column name '" + oldCol + "' does not exist in the input table")

def rename(inputTable, colNames):
    #type:(DataFrame, Union[Tuple[str, str], List[Tuple[str, str]], Dict[str, str]]) -> DataFrame
    if not isinstance(inputTable, DataFrame):
        raise InputException(inputTable, "The inputTable parameter must be of type pyspark.sql.DataFrame")
    
    if isinstance(colNames, tuple): colNames = [colNames]
    
    if isinstance(colNames, dict):
        for k, v in colNames.items():
            inputTable = safeRename(inputTable, k, v)
    elif isinstance(colNames, list):
        for x in colNames:
            if len(x) == 2:
                inputTable = safeRename(inputTable, x[0], x[1])
            else:
                raise InputException(x, "The provided column name tuple ('" + "', '".join(x) + "') must have exactly 2 values")
    else:
        raise InputException(colNames, "The colNames parameter must be provided as a dict, list or tuple")
    
    return inputTable

function.retain_01_working.py
from datetime import datetime
from drop import drop
from filter.filter import where
from pyspark.sql import DataFrame
from pyspark.sql.functions import rank
from pyspark.sql.window import Window
from pyspark.sql.functions import lit,desc
from pythontestmodule.data.dataexception import InputException


def retainFn(inputTable, retainCol,groupCols, orderCols, outCol ,filterCriteria):
    if not isinstance(inputTable, DataFrame):
        raise InputException(inputTable, "The inputTable parameter must be of type pyspark.sql.DataFrame")
    
    if not isinstance(groupCols, list): groupCols = [groupCols]
    #if not isinstance(orderCols, list): orderCols = [orderCols]
    cols = inputTable.columns
    for g in groupCols:
        if not g in cols:
            raise InputException(groupCols, "The provided group column name '" + g + "' does not exist in the input table")
    '''
    for o in orderCols:
        if not o in cols:
            raise InputException(orderCols, "The provided order column name '" + o + "' does not exist in the input table")
    '''
    if not retainCol in cols:
            raise InputException(retainCol, "The provided retain column name '" + retainCol + "' does not exist in the input table")
    
    #inputTable.show()
    filteredTable = where(inputTable, filterCriteria)
    print("Filtered data by criteria ")
    filteredTable.show()
    #TODO: Replace this with our own rank function?
    w = Window.partitionBy(groupCols).orderBy(desc(orderCols))
    filteredTable = filteredTable.withColumn("RANK", rank().over(w)) 
    print("Filtered data by rank ")
    filteredTable.show()   
    filteredTable = drop(where(filteredTable, "(RANK = 1)"), "RANK")
    print("Filtered data by dropping the rank col and selected by rank = 1 ")
    filteredTable.show()    
    #outColValue=filteredTable.rdd.map(lambda x: x[retainCol]).first()
    #print("Last updated retain col values which will be updated to new coloumn - " + str(outColValue))
    #inputTable=inputTable.withColumn(outCol,lit(outColValue))
    #inputTable.show()
        
    df1=filteredTable.withColumnRenamed(retainCol, outCol).drop(orderCols)
    print("After col renaming")
    df1.show()
    df= inputTable.join(df1,groupCols)
    df.show()    
    #inputTable.cache()
    return df


function.retain_03.py
from pyspark import HiveContext,SparkContext
from pyspark.sql.functions import asc
from pyspark.sql.types import IntegerType,StringType,StructField,StructType

def getRetainData(seq):
    colName=inputTableColName
    orderCol=orderColNames
    retainCol=retainColName
    outCol=outColName
    retain_data = []
    new_crd_line =0
    
    rptDict = {}
    #create python dictionary  for the sorting 
    for row in seq :
        rptDict[row[orderCol]]=row
    for k  in sorted(rptDict.iterkeys()):
            
            row=rptDict[k]
            row_data =[]
            for col in colName:
                row_data.append(row[col])
            if row[retainCol]==0 :
                   if new_crd_line != 0:
                    row_data.append(new_crd_line)                
                   else :
                    row_data.append(None)                
            elif row[retainCol]!= 0 :               
                   new_crd_line=row[retainCol]
                   row_data.append(new_crd_line)
            retain_data.append(row_data)
  
    return retain_data
  


accts = ["1","1","1","1","1","1","1","2","2","2"]
rptprds = [201005,201005,201005,201005,201005,201005,201005,201005,201005,201005]
acsprd = [201011,201607,201608,201609,201610,201611,201612,201608,201609,201610]
crdLine = [12222,0,0,0,0,12000,0,0,20000,0]
prd = ["CRD","CRD","CRD","CRD","CRD","CRD","CRD","PCL","PCL","PCL"]

data = []
for i in range(len(accts)):
    data.append((accts[i], rptprds[i], acsprd[i],crdLine[i],prd[i]))
rdd = sc.parallelize(data)
schema = StructType([StructField("ACCOUNT_NUMBER", StringType(), nullable=False), StructField("REPORTING_PERIOD_ACT", IntegerType(), nullable=False),StructField("REPORTING_PERIOD_ACS", IntegerType(), nullable=False) ,StructField("CRD_LINE", IntegerType(), nullable=False),StructField("PRODUCT_CCAR_TYPE", StringType(), nullable=False)])
orDf = sqlContext.createDataFrame(rdd, schema)
orDf.show()

#orderColNames=orderCols
#retainColName=retainCol
#outColName=outCol
orderColNames="REPORTING_PERIOD_ACS"
retainColName="CRD_LINE"
outColName="NEW_CRD_LINE"
inputTableColName =[colName for (colName,value) in orDf.dtypes]
#print(inputTableColName)
#groupColNames=groupCols
groupColNames="ACCOUNT_NUMBER"
updatedSchema=schema.add(StructField(outColName, IntegerType(), nullable=False))
#print(updatedSchema)

groupedRdd = orDf.rdd.map(lambda r: (r[groupColNames],r)).groupByKey()
retain_data=groupedRdd.flatMapValues(lambda x :getRetainData(x))
#retainedRdd = sc.parallelize(retain_data.values())
retainedDf = sqlContext.createDataFrame(retain_data.values(), updatedSchema)
retainedDf.show()

function.retain.py
from pyspark.sql import DataFrame
from pyspark.sql.window import Window
import pyspark.sql.functions as func
from pythontestmodule.data.dataexception import InputException
import sys

def getRetainData(groupedRecordsList,orderCols,cols,outCol):
    retainData = []
    newRetainColVal = None
    retainDataByOrderCol = {}
    #create python dictionary/map for the sorting based on the orderColumn  as a key to order the required retainCol values 
    for row in groupedRecordsList :
        retainDataByOrderCol[row[orderCols]]=row    
    
    for orderkey  in sorted(retainDataByOrderCol.keys()):            
        #Get each existing ordered row data  
        row=retainDataByOrderCol[orderkey]
        '''
        Create New rowData  to hold the missing values .Fill up new rowData with the existing row using existing table column names/index ,rowData =(row1(0),row1(1).......row1(n) ,row1(missing values))
        '''
        rowData =[]
        for col in cols:
            #Fill up the rowData with missing value at the last index
            if col == outCol:
                if row[outCol]==None :
                    if newRetainColVal != None:                    
                        rowData.append(newRetainColVal)                
                    else :
                        rowData.append(None)                
                elif row[outCol]!= None :               
                    newRetainColVal=row[outCol]
                    rowData.append(newRetainColVal)
            else : rowData.append(row[col])
            
        '''                  
        add the updated rowData to retainData (collection of rows ) holder ,retainData= [(row1(0),row1(1).......row1(n),row1(missing values)),(row2(0),row2(1).......row2(n),row2(missing value))..........]  
        '''            
        retainData.append(rowData)
            
    return retainData

def getMappedDataByKey(record,groupColNames):
 
    keylist = [] 
    for col in groupColNames:
        keylist.append(record[col])
    if len(keylist) != 1:
        key = '-'.join(map(str, keylist))
    else:
        key = record[col]
    #print("Key-----" +str(key))
    return (key,record)

'''
def retainFn(session,inputTable,retainCol,orderCols,outCol,groupCols = []):
    return _retainFn(session.sqlContext,inputTable,retainCol,orderCols,outCol,groupCols)

def _retainFn(sqlContext,inputTable,retainCol,orderCols, outCol,groupCols = []):
    if not isinstance(inputTable, DataFrame):
        raise InputException(inputTable, "The inputTable parameter must be of type pyspark.sql.DataFrame")
    if not isinstance(retainCol, str): 
        raise InputException(retainCol,"The inputTable parameter 'retainCol' must be of type str")
    if not isinstance(outCol, str): 
        raise InputException(outCol,"The inputTable parameter 'outCol' must be of type str")
    
    if not isinstance(groupCols, list):
        groupCols = [groupCols]    
    cols = inputTable.columns
    
    for g in groupCols:
        if not g in cols:
            raise InputException(groupCols, "The provided group column name '" + g + "' does not exist in the input table")
    if not retainCol in cols:
        raise InputException(retainCol, "The provided retain column name '" + retainCol + "' does not exist in the input table")
    
    updatedTable = inputTable.withColumn(outCol, inputTable[retainCol])
    cols=updatedTable.columns
    if groupCols:
        groupedRdd = updatedTable.rdd.map(lambda record: getMappedDataByKey(record,groupCols)).groupByKey()
    else :
        groupedRdd=updatedTable.rdd.groupBy(lambda record: record["ACCOUNT_NUMBER"])  ## self.map(lambda x: (f(x), x)).groupByKey(numPartitions, partitionFunc)
    
    retainDataRdd=groupedRdd.flatMapValues(lambda groupedRecordsList :getRetainData(groupedRecordsList,orderCols,cols,outCol))
    #updatedSchema=inputTable.schema.add(StructField(outCol, retainColDType, nullable=True))
    retainedDf = sqlContext.createDataFrame(retainDataRdd.values(), updatedTable.schema)
    
    return retainedDf
'''
def retainFn(table, targetCol, partitionCol, orderCol, newColName):
    if not isinstance(table, DataFrame):
        raise InputException(table, 'The input table parameter must be of type pyspark.sql.dataFrame')
    if not isinstance(targetCol, str):
        raise InputException(targetCol, 'The input targetCol must be of type string')
    if not isinstance(partitionCol, str):
        raise InputException(targetCol, 'The input patrtionCol must be of type string')
    if not isinstance(orderCol, str):
        raise InputException(targetCol, 'The input orderCol must be of type string')
    if not isinstance(newColName, str):
        raise InputException(targetCol, 'The input newColName must be of type string')
    return _retainFn(table, targetCol, partitionCol, orderCol, newColName)
    
def _retainFn(table, targetCol, partitionCol, orderCol, newColName):
    df_na = table.na.fill(-1)
    lagColName = targetCol + '_lag'
    changeColName = targetCol + '_change'
    subColName = 'sub_' + partitionCol
    rpColName = targetCol + '_rp'
    lag = df_na.withColumn(lagColName, func.lag(targetCol, default=-1).over(Window.partitionBy(partitionCol).orderBy(orderCol)))
    switch = lag.withColumn(changeColName, ((lag[targetCol] != lag[lagColName]) & (lag[targetCol] != -1)).cast('integer'))
    switch_sess = switch.withColumn(subColName, func.sum(changeColName).over(Window.partitionBy(partitionCol).orderBy(orderCol).rowsBetween(-sys.maxsize, 0)))
    switch_fill = switch_sess.withColumn(rpColName, func.first(targetCol).over(Window.partitionBy(targetCol, subColName).orderBy(orderCol)))
    switch_na = switch_fill.replace(-1, 'null')
    return switch_na.drop(lagColName).drop(changeColName).drop(subColName).withColumnRenamed(rpColName, newColName)
    




function.rnull.py

def rnull(input_table,fields):
    dataFrame = input_table.fillna(fields)
    return dataFrame



function.rule.py
'''
Created on Jan 18, 2017

@author: rm44990
'''
from os import path,sep
from py4j.java_gateway import JavaObject
from pyspark import SparkContext
from pyspark.sql import DataFrame
from pythontestmodule.util.properties import Properties
from typing import Optional

def applyRule(dataFrame, ruleName, rulePath = None, refDataPath = None):
    # type: (DataFrame, str, Optional[str], Optional[str]) -> JavaObject
    rule = __getRule(ruleName, rulePath)
    if refDataPath is not None:
        rule.loadReferenceData(refDataPath)    
    return rule.enrichRecordDataFrame(dataFrame._jdf)

def __getRule(ruleName, rulePath = None):
    # type: (str, Optional[str]) -> JavaObject
    sc = SparkContext.getOrCreate()    
    if rulePath is not None:  # Read rule from local disk
        # RH API requires trailing path separator
        if not rulePath.endswith(sep):
            rulePath = rulePath + sep
        rule = sc._jvm.com.citi.rh.enrichment.job.EnrichmentJobImpl(sc._jsc, rulePath, ruleName)
    else:  # Read rule from HDFS
        rule = sc._jvm.com.citi.rh.enrichment.job.EnrichmentJobImpl()
        rulePath = path.join(Properties.getInstance().get('hdfs.rulesdir'), ruleName + '.json')
        ruleDetail = sc._jvm.com.citi.rh.enrichment.util.UtilFunction.loadMetaDataObjectFromHDFS(sc._jsc.hadoopConfiguration(), rulePath, sc._jvm.com.citi.rh.enrichment.rules.metadata.RuleDetail().getClass())
        rule.init(sc._jsc, ruleDetail, ruleName)
    return rule

function.score.py
from decimal import Decimal
from pyspark import RDD,SparkContext
from pyspark.mllib.linalg import Vectors
from pyspark.mllib.linalg.distributed import BlockMatrix,IndexedRow,IndexedRowMatrix
from pythontestmodule.data.dataexception import InputException

from pyspark.sql import SQLContext,DataFrame
from pythontestmodule.util.login import Session

def __makeBlockMatrix(rdd, rowsPerBlock= 1024, colsPerBlock= 1024):
    #type:(RDD,int,int) -> BlockMatrix
    return IndexedRowMatrix(rdd.zipWithIndex().map(lambda x: IndexedRow(x[1], x[0]))).toBlockMatrix(rowsPerBlock, colsPerBlock)

def scoreFn(session, scoreTable, multMatrix):
    #type:(Session,DataFrame,List[List[float]]) -> DataFrame
    for dtype in scoreTable.dtypes:
        typ = dtype[1]
        if not (typ.startswith('decimal') or typ in ['int','long','float','double']):
            raise InputException(scoreTable, "The input table contains a column '" + dtype[0] + "' of non-numeric type '" + typ + "'")
    
    scoreBlock = __makeBlockMatrix(scoreTable.map(lambda x: Vectors.dense([c for c in x])))
    multBlock = __makeBlockMatrix(SparkContext.getOrCreate().parallelize(multMatrix))
    resBlock = scoreBlock.multiply(multBlock)
    # NB: Regardless of numeric input types, output columns will all be DecimalType(38,18)
    return session.sqlContext.createDataFrame(resBlock.toIndexedRowMatrix().rows.sortBy(lambda x: x.index).map(lambda x: [Decimal(y) for y in x.vector.values.tolist()]), scoreTable.columns)

function.setOps.py
from pyspark.sql import DataFrame
from pythontestmodule.data.dataexception import InputException

def checkParameters(df1, df2):
    if not isinstance(df1, DataFrame):
        raise InputException(df1, "The df1 parameter must be of type pyspark.sql.DataFrame")
    if not isinstance(df2, DataFrame):
        raise InputException(df2, "The df2 parameter must be of type pyspark.sql.DataFrame")
    if df1.columns != df2.columns:
        raise InputException([df1.columns, df2.columns], "Input table columns do not match")

def diff(df1, df2):
    checkParameters(df1, df2)
    return df1.subtract(df2)

def intersect(df1, df2):
    checkParameters(df1, df2)
    return df1.intersect(df2)

def union(df1, df2):
    checkParameters(df1, df2)
    return df1.unionAll(df2)


function.sort.py
'''
Created on Nov 4, 2016

@author: mm79548
outputTable=citidata.function.sort(inputTable, ['field1'], ['field2'], noduplicates);

'''
from pyspark.sql import DataFrame
from pythontestmodule.data.dataexception import InputException


def __sortedByColsOrders(table_name,sortCols,sortColorders,noduplicates):
    
    if noduplicates in ["True","true"]:
        df = table_name.orderBy(sortCols,ascending=sortColorders).distinct() 
    else : 
        df =table_name.orderBy(sortCols,ascending=sortColorders)
        
    return df
        
def __sortedByCols(table_name,sortCols,noduplicates):
    
    if noduplicates in ["True","true"]:
        df = table_name.orderBy(sortCols).distinct() 
    else : 
        df =table_name.orderBy(sortCols) 
    
    return df   


def sortFields(table_name,sortCols,sortColorders=[],noduplicates="False"):
    
    if not isinstance(table_name, DataFrame):
        raise InputException(table_name,"The table_name parameter must be of type pyspark.sql.DataFrame")
    if not isinstance(noduplicates, str): 
        raise InputException(noduplicates,"The inputTable parameter 'noduplicates' must be of type str") 
        
    if not isinstance(sortCols, list): sortCols = [sortCols]
    if not isinstance(sortColorders, list): sortColorders = [sortColorders]
    cols = table_name.columns
    
    for sortCol in sortCols:
        if not sortCol in cols:
            raise InputException("The provided sort column name : '" + sortCol + "' does not exist in the input table columns %s"  %cols)
    
    if sortColorders:
        df=__sortedByColsOrders(table_name,sortCols,sortColorders,noduplicates)        
    else:
        df=__sortedByCols(table_name,sortCols,noduplicates)
    return df

function.substr.py
from pyspark.sql import DataFrame
from pythontestmodule.data.dataexception import InputException

def substr(inputTable, inputCol, beginIndex, endIndex, outputCol):
    if not isinstance(inputTable, DataFrame):
        raise InputException(inputTable, "The inputTable parameter must be of type pyspark.sql.DataFrame")
    if not isinstance(beginIndex, int):
        raise InputException(beginIndex, "The beginIndex parameter must be of type int")
    if not isinstance(endIndex, int):
        raise InputException(endIndex, "The endIndex parameter must be of type int")
    
    if not inputCol in inputTable.columns:
        raise InputException(inputCol, "The provided column name '" + inputCol + "' does not exist in the input table")
    if endIndex <= beginIndex:
        raise InputException([beginIndex, endIndex], "The endIndex must be greater than the beginIndex")
    for dtype in inputTable.dtypes:
        if dtype[0] == inputCol and dtype[1] != 'string':
            raise InputException(inputCol, "The 'substr' function can only be performed on a column of type string")
    
    return inputTable.withColumn(outputCol, inputTable[inputCol].substr(beginIndex, endIndex - beginIndex))

function.summary.py
from pyspark.sql import DataFrame
from pythontestmodule.data.dataexception import InputException
from pythontestmodule.function.acewrapper import groupByWrapper
from typing import List,Union

def summarizeDict(inputTable, summDict, groups=[]):
    # type: (DataFrame, Dict[str,str], Union[str, List[str], None]) -> DataFrame
    if not isinstance(inputTable, DataFrame):
        raise InputException(inputTable, "The inputTable parameter must be of type pyspark.sql.DataFrame")
    
    if not isinstance(groups, list): groups = [groups]
    
    inputCols = inputTable.columns
    if groups:
        for i in range(len(groups)):
            col = groups[i]
            if not col in inputCols:
                raise InputException(col, "The provided grouping column name '" + col + "' does not exist in the input table")
        inputTable = groupByWrapper(inputTable, groups)
    
    return inputTable.agg(summDict)


def summarize(inputTable, summFn, fields, groups=[]):
    # type: (DataFrame, str, Union[str, List[str]], Union[str, List[str], None]) -> DataFrame
    if not isinstance(inputTable, DataFrame):
        raise InputException(inputTable, "The inputTable parameter must be of type pyspark.sql.DataFrame")
    
    validSummFn = ["max","mean","min","sum"]
    
    inputCols = inputTable.columns
    summFn = summFn.lower()
    if not isinstance(fields, list): fields = [fields]
    if not isinstance(groups, list): groups = [groups]
    
    if not summFn in validSummFn:
        raise InputException(summFn, "The provided summFn '" + summFn + "' is invalid; must be one of ['" + "', '".join(validSummFn) + "']")
    
    summDict = {}
    for i in range(len(fields)):
        col = fields[i]
        if not col in inputCols:
            raise InputException(col, "The provided aggregation column name '" + col + "' does not exist in the input table")
        summDict[fields[i]] = summFn
    
    if groups:
        for i in range(len(groups)):
            col = groups[i]
            if not col in inputCols:
                raise InputException(col, "The provided grouping column name '" + col + "' does not exist in the input table")
        inputTable = groupByWrapper(inputTable, groups)
    
    return inputTable.agg(summDict)


util.hdfsUtils.py
from hdfs import Config
from hdfs.client import Client
from hdfs.util import HdfsError
import os
from pythontestmodule.data.dataexception import InputException
from pythontestmodule.util.properties import Properties
from shutil import rmtree
from typing import Dict,List,Tuple,Optional,Union

def getClient(env = None):
    # type: (Optional[str]) -> Client
    if env is None: return Config().get_client()
    else: return Config().get_client(env)

def listDir(hdfsDir, client = None, status = False):
    # type: (str, Optional[Client], Optional[bool]) -> List[Union[str, Tuple[str, Dict[str, Union[str, int]]]]]
    if client is None: client = getClient()
    #TODO: Figure out a way to connect to HA nameservice instead (i.e. hdfs://FajitaDevelopment)
    try:
        return client.list(hdfsDir, status)
    except HdfsError as ex:
        if 'Operation category READ is not supported in state standby' in str(ex):
            client.url = client.url.replace('02i1', '01i1')
            return client.list(hdfsDir, status)
        else:
            raise ex

def buildUrl(table, country = None, lob = None, product = None, dataUnit = None, rptPrd = None, version = None, uniqueId = None):
    # type: (str, str, str, str, str, str, str, str) -> str
    props = Properties.getInstance()
    url = props.get("account.hdfspath")
    url += str(table)
    if country is not None:
        url += "/country_code=" + str(country)
        if lob is not None:
            url += "/organization_code=" + str(lob)
            if product is not None:
                url += "/product_code=" + str(product)
                if dataUnit is not None:
                    url += "/data_unit=" + str(dataUnit)
                    if rptPrd is not None:
                        url += "/rpt_prd=" + str(rptPrd)
                        if version is not None:
                            url += "/version=" + str(version)
                            if uniqueId is not None:
                                url += "/unique_id=" + str(uniqueId)
    return url 

def getLatestDataPaths(table, country, lob, product, dataUnit, rptPrds):
    # type: (str, str, str, str, str, List[str]) -> List[str]
    client = getClient()
    retList = []
    for rptPrd in rptPrds:
        try:
            retList.append(getLatestDataPath(table, country, lob, product, dataUnit, rptPrd, client))
        except HdfsError as ex:
            print("Exception getting latest path for reporting period " + rptPrd + ": " + str(ex))
    return retList

def existsDir(path, client = None):
    # type: (str, Optional[Client]) -> bool
    if client is None: client = getClient()
    return (client.status(path, strict = False) != None)
    
def getLatestDataPath(table, country, lob, product, dataUnit, rptPrd, client = None):
    # type: (str, str, str, str, str, str, Optional[Client]) -> str
    if client is None: client = getClient()
    versionUrl = buildUrl(table, country, lob, product, dataUnit, rptPrd)
    versions = listDir(versionUrl, client)
    uniqueIdUrl = os.path.join(versionUrl, str(sorted(versions)[-1]))
    uniqueIds = listDir(uniqueIdUrl, client)
    return os.path.join(uniqueIdUrl, str(sorted(uniqueIds)[-1]))

def deletePath(path, client = None):
    # type: (str, Optional[Client]) -> None
    if client is None: client = getClient()
    path = str(path)
    res = False
    try:
        res = client.delete(path, True)
    except HdfsError as ex:
        if 'Operation category WRITE is not supported in state standby' in str(ex):
            client.url = client.url.replace('02i1', '01i1')
            res = client.delete(path, True)
        else:
            raise ex
    if not res:
        raise InputException(path, "The path '" + path + "' could not be deleted because it does not exist")

def move(srcPath, dstPath, client = None) :
    # type: (str, str, Optional[Client]) -> None
    if client is None: client = getClient()
    print("Attempting to move source '" + srcPath + "' to destination '" + dstPath + "'")
    try:
        client.rename(srcPath, dstPath)
    except HdfsError as ex:
        if 'Operation category WRITE is not supported in state standby' in str(ex):
            client.url = client.url.replace('02i1', '01i1')
            client.rename(srcPath, dstPath)
        else:
            raise ex

def copyToLocal(hdfsPath, localPath, overwrite = False, client = None):
    # type: (str, str, Optional[bool], Optional[Client]) -> None
    if client is None: client = getClient()
    if overwrite:
        tmpDir = localPath + ".tmp"
        client.download(hdfsPath, localPath, overwrite = True, temp_dir = tmpDir)
        rmtree(tmpDir)
    else:
        client.download(hdfsPath, localPath)

util.login.py
from base64 import b64encode
from pyspark import SparkContext
from pyspark.sql import HiveContext
from pythontestmodule.data.dataexception import InputException
from pythontestmodule.util.properties import Properties
from typing import Optional
from urllib2 import urlopen

'''
State holder for token and SQLContext

@author: ac44059
@todo: Add session timeout
'''
class Session:
    def __init__(self, token = None):
        # type: (Optional[bytes]) -> None
        self.token = token
        pr = Properties.getInstance()
        self.username = pr.getOpt('user')
        if self.username is None: self.username = ''
        self.serverUrl = pr.get('server.url')
        sc = SparkContext.getOrCreate()
        self.sqlContext = HiveContext(sc)
            
    def __str__(self):
        # type: () -> str
        if self.token is not None:
            return self.username + ' IS logged in'
        return self.username + ' IS NOT logged in'
    
    def authorize(self, country, product):
        # type: (str, str) -> bool
        if not self.username == Properties.getInstance().getOpt('user'):
            raise InputException(self, 'Session username [' + self.username + '] does not match executor [' + Properties.getInstance().getOpt('user') + ']')
        u = urlopen(self.serverUrl + 'authorize/?user=' + self.username + '&country=' + country + '&product=' + product)
        return u.read().decode().lower() == 'true'

'''
Utility class to authenticate and authorize
@author: ac44059
'''    
class Connect:
    def __init__(self, password):
        # type: (bytes) -> None
        self.pr = Properties.getInstance()
        self.token = None  # type: bytes
        self.login_url = self.pr.get('server.url') + 'login/?user=' + self.pr.getOpt('user') + '&password=' + b64encode(password).decode()
        
    def login(self):
        # type: () -> Session
        try:
            u = urlopen(self.login_url)
            self.token = u.read()
            self.session = Session(self.token)
            return self.session
        except:
            raise InputException(self, 'Login attempt failed for user ' + self.pr.getOpt('user'))

util.mx_data_optimizer.py
from pyspark.context import SparkContext
from pyspark.sql.context import SQLContext

from pythontestmodule.data.account import __accountDataMultiMonth
from pythontestmodule.function.join import joinTables
from pythontestmodule.util import hdfsUtils
from pythontestmodule.function import sort
from pythontestmodule.function.summary import summarize
from pythontestmodule.function.acewrapper import filterWrapper
from pythontestmodule.function.rename import rename
from pythontestmodule.function.filter.filter import ifelse
import hashlib
from pythontestmodule.util.properties import Properties

def __getCachedPath(path):
    return hashlib.sha256(path.encode(encoding='utf_8', errors='strict')).hexdigest()

'''
'200701', '200702', '200703', '200704', '200705', '200706', '200707', '200708', '200709', '200710', '200711', '200712', \
'200801', '200802', '200803', '200804', '200805', '200806', '200807', '200808', '200809', '200810', '200811', '200812', \
                      '200901', '200902', '200903', '200904', '200905', '200906', '200907', '200908', '200909', '200910', '200911', '200912', \
                      '201001', '201002', '201003', '201004', '201005', '201006', '201007', '201008', '201009', '201010', '201011', '201012', \
                      '201101', '201102', '201103', '201104', '201105', '201106', '201107', '201108', '201109', '201110', '201111', '201112', \
                      '201201', '201202', '201203', '201204', '201205', '201206', '201207', '201208', '201209', '201210', '201211', '201212', \
                      '201301', '201302', '201303', '201304', '201305', '201306', '201307', '201308', '201309', '201310', '201311', '201312', \
                      '201401', '201402', '201403', '201404', '201405', '201406', '201407', '201408', '201409', '201410', '201411', '201412', \
                      '201501', '201502', '201503', '201504', '201505', '201506', '201507', '201508', '201509', '201510', '201511', '201512'
'''
if __name__ == "__main__":
    country = 'MX'
    org = 'GCB'
    product='CRD'
    du = 'BANAMEX'
    
    sc = SparkContext()
    sqlContext = SQLContext(sc)
    months = [ '200701','200702','200703' , '200704', '200705', '200706', '200707', '200708', '200709', '200710', '200711', '200712']
    #months = [ '200701','200702']
    actData = __accountDataMultiMonth(sqlContext, ‘ACT_SAMPLE_TABLE’, 'MX', 'GCB', 'CRD', 'BANAMEX',
                     ['ACCOUNT_NUMBER',….],
                     months)
    acsData = __accountDataMultiMonth(sqlContext, ‘ACS_SAMPLE_TABLE’, 'MX', 'GCB', 'CRD', 'BANAMEX',
                     ['ACCOUNT_NUMBER', 'REPORTING_PERIOD', 'ORIG_ACCT_FICO_SCORE'],
                     months)
    acsData = ifelse(acsData, 'ORIG_ACCT_FICO_SCORE', "(ORIG_ACCT_FICO_SCORE IS NOT NULL)", 'ORIG_ACCT_FICO_SCORE', 0)
    acsData = filterWrapper(acsData, "(ORIG_ACCT_FICO_SCORE != 0)")
    byStmt1 = ['ACCOUNT_NUMBER', 'REPORTING_PERIOD']
    varStmt1= ['ORIG_ACCT_FICO_SCORE']
    acsData = summarize(acsData, 'min', varStmt1, byStmt1)
    acsData = rename(acsData, {'min(ORIG_ACCT_FICO_SCORE)':'ORIG_ACCT_FICO_SCORE'}) 
    
    coeData = __accountDataMultiMonth(sqlContext, 'PROFILED_COE', 'MX', 'GCB', 'CRD', 'BANAMEX',
                     ['ACCOUNT_NUMBER', 'REPORTING_PERIOD', 'CHARGE_OFF_EVENT_PRINCIPAL'],
                     months)
    coeData = ifelse(coeData, 'CHARGE_OFF_EVENT_PRINCIPAL', "(CHARGE_OFF_EVENT_PRINCIPAL IS NOT NULL)", 'CHARGE_OFF_EVENT_PRINCIPAL', 0)
    byStmt2 = ['ACCOUNT_NUMBER', 'REPORTING_PERIOD']
    varStmt2= ['CHARGE_OFF_EVENT_PRINCIPAL']
    coeData = summarize(coeData, 'sum', varStmt2, byStmt2)
    coeData = rename(coeData, {'sum(CHARGE_OFF_EVENT_PRINCIPAL)':'CHARGE_OFF_EVENT_PRINCIPAL'})
                     
    recData = __accountDataMultiMonth(sqlContext, 'PROFILED_REC', 'MX', 'GCB', 'CRD', 'BANAMEX',
                     ['ACCOUNT_NUMBER', 'REPORTING_PERIOD', 'NET_RECOVERIES'],
                     months)
    actData = joinTables(actData, acsData, [('ACCOUNT_NUMBER', 'ACCOUNT_NUMBER'), ('REPORTING_PERIOD', 'REPORTING_PERIOD')])
    actData = joinTables(actData, coeData, [('ACCOUNT_NUMBER', 'ACCOUNT_NUMBER'), ('REPORTING_PERIOD', 'REPORTING_PERIOD')], 'left_outer')
    actData = joinTables(actData, recData, [('ACCOUNT_NUMBER', 'ACCOUNT_NUMBER'), ('REPORTING_PERIOD', 'REPORTING_PERIOD')], 'left_outer')
 
    cachedPath = __getCachedPath(country+"-"+org+"-"+product+"-"+du+"-"+str(months))
    pr = Properties.getInstance()
    
    cachedPath = pr.get("cache.root")+cachedPath
    if hdfsUtils.existsDir(cachedPath) == False:
        actData = actData.withColumn("binningKey",actData.ACCOUNT_NUMBER.substr(0,2))
        actData = actData.repartition(4,"binningKey")
        print ("Writing CachedPath:" + cachedPath)
        actData.write.parquet(cachedPath)


util.properties.py
import inspect
from pyspark import Broadcast,SparkContext
from pyspark.sql import DataFrame,Row
from pyspark.storagelevel import StorageLevel
from pythontestmodule.data.dataexception import BehaviorException
from pythontestmodule.util.singleton import singleton
from sys import argv


def joinAce(self, other, on = None, how = None) :
    if str(inspect.stack()[1][3]).find("joinTables") == -1:
        raise BehaviorException("Reserved", "Not allowed native function!")
    return self._join(other, on, how)

DataFrame._join = DataFrame.join
DataFrame.join = joinAce

def cacheAce(self):
    if str(inspect.stack()[1][3]).find("cacheWrapper") == -1:
        raise BehaviorException("Reserved", "Not allowed native function!")
    return self._cache()

DataFrame._cache = DataFrame.cache
DataFrame.cache = cacheAce

def persistAce(self, level):
    
    if str(inspect.stack()[1][3]).find("persistWrapper") == -1:
        raise BehaviorException("Reserved", "Not allowed native function!")
    return self._persist(level)

DataFrame._persist = DataFrame.persist
DataFrame.persist = persistAce

def collectAce(self) :
    if str(inspect.stack()[1][3]).find("collectWrapper") == -1:
        raise BehaviorException("Reserved", "Not allowed native function!")
    return self._collect()

DataFrame._collect = DataFrame.collect
DataFrame.collect = collectAce

def countAce(self):
    if str(inspect.stack()[1][3]).find("countWrapper") == -1:
        raise BehaviorException("Reserved", "Not allowed native function!")
    return self._count()

DataFrame._count = DataFrame.count
DataFrame.count = countAce
 
def selectExprAce(self, exprString):
   
    if str(inspect.stack()[1][3]).find("selectExprWrapper") == -1 and str(inspect.stack()[1][3]).find("ifelse") == -1 and str(inspect.stack()[1][3]).find("binfn") == -1 :
        raise BehaviorException("Reserved", "Not allowed native function!")
    return self._selectExpr(exprString)

DataFrame._selectExpr = DataFrame.selectExpr
DataFrame.selectExpr = selectExprAce

def filterAce(self, exprString):
    
    if str(inspect.stack()[1][3]).find("filterWrapper") == -1:
        raise BehaviorException("Reserved", "Not allowed native function!")
    return self._filter(exprString)

DataFrame._filter = DataFrame.filter
DataFrame.filter = filterAce
DataFrame.where = DataFrame.filter

def groupByAce(self, cols = None) :
    
    if str(inspect.stack()[1][3]).find("groupByWrapper") == -1:
        raise BehaviorException("Reserved", "Not allowed native function!")
    return self._groupBy(cols)
DataFrame._groupBy = DataFrame.groupBy
DataFrame.groupBy = groupByAce

def broadcastAce(self, value):
   
    if str(inspect.stack()[1][3]).find("broadcastWrapper") == -1:
        raise BehaviorException("Reserved", "Not allowed native function!")
    return self._broadcast(value)

SparkContext._broadcast = SparkContext.broadcast
SparkContext.broadcast = broadcastAce

class Properties:
    _instance = None  # type: 'Properties'
    
    def __init__(self, filepath, sep = '=', comment_char = '#'):
        # type: (str, str, str) -> None
        if singleton(self) == False:
            raise ValueError('Should not have attempted double instantiation of Properties')
        self.__props = {}  # type: Dict[str, str]
        with open(filepath, 'rt') as f:
            for line in f:
                l = line.strip()
                if l and not l.startswith(comment_char):
                    key_value = l.split(sep)
                    key = key_value[0].strip()
                    value = sep.join(key_value[1:]).strip().strip('"')
                    self.__props[key] = value
        self.__opts = {}  # type: Dict[str, str]
        for a in argv:
            s = a.split('=')
            if len(s) == 2:
                self.__opts[s[0]] = s[1]
    
    def get(self, key):
        # type: (str) -> str
        if key in self.__props:
            return self.__props[key]
        else:
            return None
    
    def getOpt(self, key):
        # type: (str) -> str
        if key in self.__opts:
            return self.__opts[key]
        else:
            return None
    
    @staticmethod
    def getInstance():
        if Properties._instance is None:
            Properties._instance = Properties(argv[1])
        return Properties._instance

util.rangekey.py


class RangeKey:
    def __init__ (self, mapped):
        #type:( Dict[str, str]) -> None
        self.map = mapped

    def get(self, val):
        #type:( str) -> str
        for k in self.map:
            if val in k:
                return self.map[k]
        return None

util.singleton.py

def singleton(obj) :
    cls = obj.__class__
    if hasattr(cls, '__instantiated'):
        return False
    cls.__instantiated = True
    return True


view.chart.py
'''
Created on Jan 26, 2017

@author: ak64189
'''
from pythontestmodule.data.dataexception import InputException
from pythontestmodule.util.properties import Properties
import urllib
import webbrowser

def launchChart(chart, location):
    #type: (str, str) -> None
    if not isinstance(chart, str):
        raise InputException(chart, "chart param must be of type str")
    if not isinstance(location, str):
        raise InputException(location, "location param must be of type str")
    
    pr = Properties.getInstance()

    url = pr.get('server.url') + "getChart/?userid=" + pr.getOpt('user') + "&chartName=" + chart
    urllib.urlretrieve(url, location)
    webbrowser.open(location)

view.graph.py
'''
Created on Jan 31, 2017

@author: rm44990
'''
from matplotlib import pyplot as plt
from pandas import DataFrame,to_numeric
from typing import Optional

def launchGraph(dataFrame, xAxis, yAxis, graphType, graphTitle = 'Panda Graph Visualization', xLabel = 'X-axis', yLabel = 'Y-axis'):
    # type: (DataFrame, str, str, str, Optional[str], Optional[str], Optional[str]) -> None
    dataFrame[[xAxis, yAxis]] = dataFrame[[xAxis, yAxis]].apply(lambda x: to_numeric(x, errors = 'raise'))
    dataFrame.plot(xAxis, yAxis, graphType, title = graphTitle)
    plt.xlabel(xLabel)
    plt.ylabel(yLabel)
    plt.tight_layout()
    fig = plt.gcf()
    plt.show()
    plt.close(fig)

view.grid.py
'''
Created on Jan 5, 2017

@author: rm44990
'''
from numpy import append,array,savetxt
from openpyxl import Workbook
from pandas import DataFrame as pandaFrame,isnull
from pyspark.sql import DataFrame
from qtpy.QtCore import QAbstractTableModel,QDir,QModelIndex,QObject,Qt
from qtpy.QtGui import QFont
from qtpy.QtWidgets import QApplication,QFileDialog,QPushButton,QVBoxLayout,QWidget,QTableView
from pythontestmodule.function.pandaFrame import toPandaFrame
import sys
from typing import Any,List,Optional,Union

def launchDataGrid(dataFrame):
    # type: (Union[pandaFrame,DataFrame]) -> None
    if isinstance(dataFrame, DataFrame):
        dataFrame = toPandaFrame(dataFrame)
    app = QApplication(sys.argv)
    w = GridWindow(dataFrame)
    w.show()
    ec = app.exec_()
    if ec != 0:
        raise Exception('Data grid display failed with error code ' + str(ec))
    else:
        print('Clean exit from data grid display')

class GridWindow(QWidget):
    __xlExt = '.xlsx'
    
    def __init__(self, pandaFrame, *args):
        # type: (DataFrame, Any) -> None
        QWidget.__init__(self, *args)
        self.setWindowTitle('Analytics Data Grid')
        self.data = pandaFrame
        self.getTableData()
        layout = QVBoxLayout()
        table = self.createTable()
        table.sizeHintForRow = self.rowHeightHint
        table.resizeRowsToContents()
        layout.addWidget(table)
        button = QPushButton('Export Data')
        button.setMaximumWidth(100)
        button.clicked.connect(self.exportData)
        layout.addWidget(button)
        self.setLayout(layout)
    
    def getTableData(self):
        # type: () -> None
        gridData = self.data.as_matrix()
        self.tabledata = gridData
    
    def createTable(self):
        # type: () -> QTableView
        tv = QTableView()
        tm = MyTableModel(self.tabledata, self.data.keys(), self)
        tv.setModel(tm)
        tv.setMinimumSize(900, 500)
        tv.setShowGrid(False)
        font = QFont('Courier New', 8)
        tv.setFont(font)
        vh = tv.verticalHeader()
        vh.setVisible(False)
        hh = tv.horizontalHeader()
        hh.setStretchLastSection(True)
        tv.resizeColumnsToContents()
        tv.setSortingEnabled(True)
        tv.sortByColumn(0, Qt.AscendingOrder)        
        return tv
    
    def rowHeightHint(self, row):
        # type: (int) -> int
        return 18
    
    def exportData(self):
        # type: () -> None
        formats = 'Excel Workbook (*' + self.__xlExt + ');; CSV (Comma delimited) (*.csv)'
        fname = QFileDialog.getSaveFileName(self, 'Save File', QDir.homePath() + QDir.separator() + 'ace_export', formats)
        if isinstance(fname, tuple): fname = fname[0]
        if not fname:
            print('User cancelled save operation')
        elif fname.endswith(self.__xlExt):
            wb = Workbook()
            ws = wb.active
            for r in append(array([self.data.columns.values]), self.tabledata, 0):
                dat = []
                for c in r: dat.append(c)
                ws.append(dat)
            print('Exporting data to Excel file ' + fname)
            wb.save(fname)
        else:
            print('Exporting data to CSV file ' + fname)
            savetxt(fname, self.tabledata, fmt = '%s', delimiter = ',', header = ','.join(self.data.columns), comments = '')

class MyTableModel(QAbstractTableModel):
    def __init__(self, datain, headerdata, parent = None, *args):
        # type: (List[List[Any]], List[str], Optional[QObject], Any) -> None
        QAbstractTableModel.__init__(self, parent, *args)
        self.arraydata = datain
        self.headerdata = headerdata
    
    def rowCount(self, parent):
        # type: (QObject) -> int
        return len(self.arraydata)
    
    def columnCount(self, parent):
        # type: (QObject) -> int
        return len(self.arraydata[0])
    
    def data(self, index, role):
        # type: (QModelIndex, int) -> Union[str,None]
        if not index.isValid():
            return None
        elif role != Qt.DisplayRole:
            return None
        return str(self.arraydata[index.row()][index.column()])
    
    def headerData(self, col, orientation, role):
        # type: (int, Qt.Orientation, int) -> Union[str,None]
        if orientation == Qt.Horizontal and role == Qt.DisplayRole:
            return str(self.headerdata[col])
        return None
    
    def sort(self, Ncol, order):
        # type: (int, Qt.SortOrder) -> None
        self.layoutAboutToBeChanged.emit()
        reverseSort = order == Qt.DescendingOrder
        # The lambda here ensures that NULL/NaN values will always be at the end
        self.arraydata = sorted(self.arraydata, key = lambda x: (isnull(x[Ncol]) if not reverseSort else not isnull(x[Ncol]), x[Ncol]), reverse = reverseSort)
        self.layoutChanged.emit()

function.mlib.gbm.py
# -*- coding: utf-8 -*-
"""
Created on Mon Jan 09 11:10:44 2017

@author: ss71165
"""

from pyper import *

# Path to Revolution R executable (this is the standard location for Big Data EAP)
# r = R(RCMD="/var/app/revor/lib64/Revo-7.0/R-3.0.2/bin/R", use_numpy=True)

# r = R(use_numpy=True)
# Chart Path
chartpath = "/data/1/gftrrasn/ace/chart/"
#chartpath=""

r("library(gbm)")
# for local testing
#export R_LIBS=/home/ss71165/R/x86_64-unknown-linux-gnu-library/3.0

def plotPerf(fitted_model, chartname):
    """
    Create  plot
    """
    rcode = "png('"+ chartpath + chartname + ".png')"
    print(r(rcode))
    #best.iter <- gbm.perf(gbm1,method="cv")
    rplotcode = "best.iter <- gbm.perf(" + fitted_model +" , method='cv')"
    print(r(rplotcode))
    r("dev.off()")
    
def save(model, path):
    """
    Save fitted model to file
    """
    
    rcode = "save(model=" + model + ", file=" + path + ")"
    print(r(rcode))   
    
def load(path):
    """
    Load fitted model from file
    """
    
    rcode = "load(" + path + ")"
    print(r(rcode))   
    
def gbm(data, 
        model, 
        formula, 
        graph, 
        var_monotone, 
        distribution = "gaussian", 
        ntrees = 1000, 
        shrinkage = 0.05, 
        interaction_depth = 3, 
        bag_fraction = 0.5, 
        train_fraction = 0.5, 
        n_minobsinnode = 10, 
        cv_folds = 3):
    """
    R syntax: fitted.model <- gbm(formula=dep_var ~ ind_var1 + ind_var2 + ind_var3 + ind_var4, 
      data=inputdf, 
      var.monotone=c(0,0,0,0),
      distribution="gaussian", 
      n.trees=1000, 
      shrinkage=0.05, 
      interaction.depth=3, 
      bag.fraction = 0.5, 
      train.fraction = 0.5, 
      n.minobsinnode = 10, 
      cv.folds = 3, 
      keep.data=TRUE, 
      verbose=TRUE, 
      n.cores=1)   
      
    gbm(Y~X1+X2+X3+X4+X5+X6,     # formula
    data=data,                   # dataset
    var.monotone=c(0,0,0,0,0,0), # -1: monotone decrease,
                                 # +1: monotone increase,
                                 #  0: no monotone restrictions
    distribution="gaussian",     # see the help for other choices
    n.trees=1000,                # number of trees
    shrinkage=0.05,              # shrinkage or learning rate,
                                 # 0.001 to 0.1 usually work
    interaction.depth=3,         # 1: additive model, 2: two-way interactions, etc.
    bag.fraction = 0.5,          # subsampling fraction, 0.5 is probably best
    train.fraction = 0.5,        # fraction of data for training,
                                 # first train.fraction*N used for training
    n.minobsinnode = 10,         # minimum total weight needed in each node
    cv.folds = 3,                # do 3-fold cross-validation
    keep.data=TRUE,              # keep a copy of the dataset with the object
    verbose=FALSE,               # don't print out progress
    n.cores=1)                   # use only a single core (detecting #cores is
                                 # error-prone, so avoided here)
                                 
    # check performance using an out-of-bag estimator
    # OOB underestimates the optimal number of iterations
    best.iter <- gbm.perf(gbm1,method="OOB")
    # check performance using a 50% heldout test set
    best.iter <- gbm.perf(gbm1,method="test")
    # check performance using 5-fold cross-validation
    best.iter <- gbm.perf(gbm1,method="cv")

    """
    
    hyper_params_string = ", n.trees = " + str(ntrees) + ", cv.folds = " + str(cv_folds)
#     hyper_params_string =  hyper_params_string + 
#     """
#     n_trees=1000, 
#       shrinkage=0.05, 
#       interaction_depth=3, 
#       bag_fraction = 0.5, 
#       train_fraction = 0.5, 
#       n_minobsinnode = 10, 
#       cv.folds = 3, 
#     """
    rcode = model + "<- gbm(formula=" + formula + ", distribution='" + distribution + "', data=" + data + ", var.monotone=" + var_monotone + hyper_params_string + ")"
    print(rcode)
    print(r(rcode))
    plotPerf(model, graph)
    # return(r['fitted_gbm_model'])
    
def predict(fitted_model):
    rcode = "summary(predict(" + fitted_model + "))"
    print(rcode)
    print(r(rcode))

def summary(results):
    rcall = "summary("+results+")"
    print(r(rcall))
    
def relinf(fitted_model):
    #print(pretty.gbm.tree(gbm1,1))
    rcode = "relinf"+ " <- summary(" + fitted_model +")$rel.inf"
    print(r(rcode))
    return(r['relinf'])

def var(fitted_model):
    rcode = "var"+ " <- summary(" + fitted_model +")$var"
    print(r(rcode))
    return(r['var'])
    
def readcsv(tbl, path):
    rcode = tbl + ' <- read.csv("' + path + '", header=TRUE, stringsAsFactors=FALSE)'
    result = r(rcode)
    return(result)

def readtable(tbl, path):
    rcode = tbl + ' <- read.table("' + path + '", header=TRUE, sep=",", quote="")'
    print(rcode)
    result = r(rcode)
    return(result)

def test_gbm():
    readcsv("data", "C:/Users/ss71165/gbmsample1.csv")
    r("library(gbm)")
    #def gbm(data, fitted_gbm_model, formula, graph, distribution="gaussian", var_monotone="NA", ntrees = 1000)
    gbm(data="data", 
        model="gbm_model",
        formula="N_CUST_SCORE_AGR~AGE_JOB+N_RES_YEAR+EMPL_FLAG+self_empl",
        graph="gbm_res.png",
        var_monotone="c(0,0,0,0)",
        distribution="gaussian", 
        ntrees=1000) 
    summary("gbm_model")
    #best.iter <- gbm.perf(gbm1,method="cv")
    
    

"""
data<-read.table("C:/Users/ss71165/gbmsample1.csv", header=TRUE, sep=",", quote="", stringsAsFactors=FALSE)

gbm1 <-
  gbm(N_CUST_SCORE_AGR~AGE_JOB+N_RES_YEAR+EMPL_FLAG+self_empl, 
      data=data, 
      var.monotone=c(0,0,0,0),
      distribution="gaussian", 
      n.trees=1000, 
      shrinkage=0.05, 
      interaction.depth=3, 
      bag.fraction = 0.5, 
      train.fraction = 0.5, 
      n.minobsinnode = 10, 
      cv.folds = 3, 
      keep.data=TRUE, 
      verbose=TRUE, 
      n.cores=1)   
"""

        
if __name__ == "__main__":
    #sc = SparkContext()
   
    r = R(use_numpy=True)
    print(r("Sys.getenv('R_LIBS')"))
    r("library(gbm)")
    #readcsv("data", "C:/Users/ss71165/gbmsample1.csv")
    readcsv("data", "/home/ss71165/gbmsample1.csv")
    gbm(data="data", 
        model="gbm_model",
        formula="N_CUST_SCORE_AGR~AGE_JOB+N_RES_YEAR+EMPL_FLAG+self_empl",
        graph="gbm_perf.png",
        var_monotone="c(0,0,0,0)",
        distribution="gaussian", 
        ntrees=1000) 
    summary("gbm_model")


function.mlib.generalizedLinearModel.py
from pyspark.sql import *
from pyspark.ml.classification import *
from pyspark.mllib.linalg import *
from pyspark.mllib.regression import *
from pyspark.sql.types import *
from numpy import array
import pandas as pd
from pyspark.sql import SQLContext
from pyspark import SparkContext
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf

class  GeneralizedLinearSummary:
    """
    Summary statistics of Logistic regression results
    """
    
    def __init(self, lrm):
        self.coefficients = lrm.weights
    
    def coefficients(self):
        self.coefficients   
    
    def std_error(self):
        self.std_error

class GeneralizedLinearModel:
    """
    Fitted model returned as result of regression
    """
    
    def __init(self):
        self.summary = GeneralizedLinearSummary(self)
        
    def save(self, file):
        pass
    
    def load(self, file):
        pass
  
    # predict
    def transform(self, dataframe): 
        pass
    
    def summary(self):
        self.summary
    

class GeneralizedLinearRegression:
    """
    Logistic Regression
    
    Description: Simple Logistic Regression
        
    Limitations: This does not allow a multinomial response variables or allow specifying a weight column
 
    """
    
    def __init__(self, max_iterations=100, method='BFGS'):
        # self.weight_column = weight_column   # optional
        self.max_iterations = max_iterations
        """
        if (method == 'BFGS'):
            self.logisticRegression = LogisticRegressionWithLBFGS(max_iterations)
        else:
            self.logisticRegression = LogisticRegressionWithLBFGS(max_iterations)
            # self.logisticRegression = LogisticRegression(max_iterations)
        
        """


    def fit(self, dataframe, dependent_vars, independent_vars):
        "this will return a fitted NonLinearRegressionModel"
        self.dataframe = dataframe
        self.dependent_vars = dependent_vars
        self.independent_vars = independent_vars
        
        # construct dataframe from specified independent and dependent variables
        

def testFit():
    """
    Test logistic regression
    """
    import pandas as pd

    sampleDataframe = pd.read_csv('/home/ss71165/sample_data.csv')
    
    # Read the data into a pandas data frame
    # Construct a list of LabeledPoints with just the columns (predictors/response) involved in the regression.  
    # Logistic Regression formula: imp ~ seasoning + ln_seasoning + dU

    # sample_data = pd.read_csv('/home/ss71165/SAMPLE.csv')
    # df = pd.DataFrame(sample_data)

    
    lr = GeneralizedLinearRegression(max_iterations=10)
    det_formula = 'det ~ seasoning + ln_seasoning + ccltv + dU + dGDP_YOY + mortgage_rate'
    ppd_formula = 'ppd ~ seasoning + ln_seasoning + dU + dGDP_YOY + mortgage_rate_change'
    imp_formula = 'imp ~ seasoning + ln_seasoning + dU'
    
    
    print("Fitting Logistic Regression/Generalized Linear model...")
    print("formula: ")
    print(imp_formula)
    

    # lrModelresult = lr.fit(sampleDataframe, dependent_vars, independent_vars)  
    lrModelresult = smf.glm(formula=imp_formula, data=sampleDataframe, family=sm.families.Binomial()).fit()
    
    print("Obtaining Logistic Regression model summary...")  
    print(lrModelresult.summary())
  
    print("Logistic Regression model summary coefficients...") 
  
    
    print("Saving Logistic Regression model...")
    #print(lrModelresult.save(sc,"logisticResults"))
    
    print("Loading Logistic Regression model...")
    #print(lrModelresult.load(sc,"logisticResults"))
    
    print("Logistic Regression model summary standard error...") 
    # summary.std_error()
    
    
def testRegressionSummary():    
     """
    Test logistic regression Summary
    
    """    

def testLoad(): 
   """
    Test logistic regression Load
    """    

    #print("Loading Logistic Regression model...")
    #print(lrModelresult.load(sc,"logisticResults"))


def testSave():    
     """
    Test logistic regression Save
    """
    
if __name__ == "__main__":
    sc = SparkContext()
    testFit()   
    

function.mlib.glm.py
# -*- coding: utf-8 -*-
"""
Created on Mon Jan 09 11:10:44 2017

@author: ss71165
"""

from pyper import *
import inspect

# Path to Revolution R executable (this is the standard location for Big Data EAP)
r = R(RCMD="/var/app/revor/lib64/Revo-7.0/R-3.0.2/bin/R", use_numpy=True)
# Chart Path
chartpath = "/data/1/gftrrasn/ace/chart/"


# for local testing
#export R_LIBS=/home/ss71165/R/x86_64-unknown-linux-gnu-library/3.0

def doc(topic):
    """
    Display documentation for a function/topic
    """
    rcall = "?" + topic
    print(r(rcall))
    
def ls():
    """
    List the R objects
    """
    print(r("ls()"))    
    
def head(df, n=2):
    """
    Show the head n rows
    """
    rcode = "head(" + df + ", " + str(n) + ")"
    print(r(rcode))    
    
def getResGraph(model, chartname, data, regressor, response, xlab, ylab):
    rcode = "rs" + " <- residuals(" + model +")"
    rcode = "png(' "+ chartname + "')"
    print(rcode)
    print(r(rcode))
    rplotcode = "plot(" + data + "$" + regressor + ", " + data + "$" + response + ", pch = 16, xlab = " + xlab + ", ylab = " + ylab + ")"
    r(rplotcode)
    r("dev.off()")
    
def plotRes(fitted_model, chartname):
    """
    Create residuals vs. fitted/predicted plot
    """
    rcode = "png('"+ chartpath + chartname + ".png')"
    print(rcode)
    print(r(rcode))
    rplotcode = "plot(" + fitted_model +" , which=1)"
    print(rplotcode)
    print(r(rplotcode))
    r("dev.off()")
    
def plotSL(fitted_model, chartname):
    """
    Create Scale-Location Plot
    """
    rcode = "png('"+ chartpath + chartname + "2.png')"
    print(rcode)
    print(r(rcode))
    rplotcode = "plot(" + fitted_model +" , which=2)"
    print(rplotcode)
    print(r(rplotcode))
    r("dev.off()")    
    
    
def plotQQ(fitted_model, chartname):
    """
    Create QQ plot
    """
    rcode = "png('"+ chartpath + chartname + "3.png')"
    print(rcode)
    print(r(rcode))
    rplotcode = "plot(" + fitted_model +" , which=3)"
    print(rplotcode)
    print(r(rplotcode))
    r("dev.off()")    
    
def plotRL(fitted_model, chartname):
    """
    Create QQ plot
    """
    rcode = "png('"+ chartpath + chartname + "5.png')"
    print(rcode)
    print(r(rcode))
    rplotcode = "plot(" + fitted_model +" , which=5)"
    print(rplotcode)
    print(r(rplotcode))
    r("dev.off()")       

def plotRes2(model, chartname):
    rcode = "png('"+ chartname + ".png')"
    print(rcode)
    print(r(rcode))
    rplotcode = "plot(pedict" + model +" , residuals" + model + ")"
    print(rplotcode)
    print(r(rplotcode))
    r("dev.off()")
    
def save(model, path):
    """
    Save fitted model to file
    """
    
    rcode = "save(model=" + model + ", file=" + path + ")"
    print(r(rcode))   
    
def load(path):
    """
    Load fitted model from file
    """
    
    rcode = "load(" + path + ")"
    print(r(rcode))     
    
def glm(data, fitted_model, graph, formula, family="binomial", weights="NA", maxiter = 25):
    """
    R syntax: fitted.model <- glm(formula=dep ~ ind, data=dataframe, family=binomial(), control = list(maxit = 50))
    """
    
    control_string = ", control = list(maxit=" + str(maxiter) + ")" 
    rcode = fitted_model + "<- glm(formula=" + formula + ", family=" + family + "(), data=" + data + ", weights=" + weights + control_string + ")"
    print(rcode)
    print(r(rcode))
    plotRes(fitted_model, graph)
    plotSL(fitted_model, graph)
    plotQQ(fitted_model, graph)
    plotRL(fitted_model, graph)
    # return(r['fitted_glm_model'])
    
def predict(fitted_model):
    rcode = "summary(predict(" + fitted_model + "))"
    print(rcode)
    print(r(rcode))

def summary(results):
    rcall = "summary("+results+")"
    print(r(rcall))
    
def residuals(fitted_model):
    rcode = "rs" + " <- residuals(" + fitted_model +")"
    print(rcode)
    print(r(rcode))
    return(r['rs'])
    
def rrange(name):
    rcode = "range(" + name + ")" 
    print(rcode)
    print(r(rcode))
        
def coef(fitted_model):
    rcode = "cf"+ " <- coef(" + fitted_model +")"
    print(rcode)
    print(r(rcode))
    return(r['cf'])

def stderror(fitted_model):
    rcode = "se"+ " <- coef(summary(" + fitted_model +"))[, 2]"
    print(rcode)
    print(r(rcode))
    return(r['se'])
    
def readcsv(tbl, path):
    rcode = tbl + ' <- read.table("' + path + '", header=TRUE, sep=",", quote="", stringsAsFactors=FALSE)'
    print(rcode)
    result = r(rcode)
    return(result)
          

function.mlib.logisticRegression.py
from pyspark.sql import *
from pyspark.ml.classification import *
from pyspark.mllib.linalg import *
from pyspark.mllib.regression import *
from pyspark.sql import HiveContext
from pyspark.sql.types import *
from numpy import array
import pandas as pd
from pyspark.sql import SQLContext
from pythontestmodule.function.acewrapper import collectWrapper
from pyspark import SparkContext
from pyspark.mllib.classification import LogisticRegressionWithLBFGS
from pythontestmodule.data.dataexception import InputException

class LogisticRegressionSummary:
    """
    Summary statistics of Logistic regression results
    """
    
    def __init(self, lrm):
        self.coefficients = lrm.weights
    
    def coefficients(self):
        self.coefficients   
    
    def std_error(self):
        self.std_error

class LogisticRegressionModel:
    """
    Fitted model returned as result of regression
    """
    
    def __init(self):
        self.summary = LogisticRegressionSummary(self)
        
    def save(self, file):
        pass
    
    def load(self, file):
        pass
  
    # predict
    def transform(self, dataframe): 
        pass
    
    def summary(self):
        self.summary
    

class LogisticRegression:
    """
    Logistic Regression
    
    Description: Simple Logistic Regression
        
    Limitations: This does not allow a multinomial response variables or allow specifying a weight column
 
    """
    
    def __init__(self, max_iterations=100, method='BFGS'):
        # self.weight_column = weight_column   # optional
        self.max_iterations = max_iterations
        """
        if (method == 'BFGS'):
            self.logisticRegression = LogisticRegressionWithLBFGS(max_iterations)
        else:
            self.logisticRegression = LogisticRegressionWithLBFGS(max_iterations)
            # self.logisticRegression = LogisticRegression(max_iterations)
        
        """

    def buildDataCollect(self, df, dependent_vars, independent_vars):
        data = []
        max_dep = len(dependent_vars)
        max_ind = len(independent_vars)
        dep_var_name = dependent_vars[0] #  can be only one dependent variable so take first one
        ind_var_locations = []
        
        print("In buildData input df: ") 
        # print(df)   
        
        #for col in range(0, max_ind):
        #    ind_var_locations.append(df.iloc[0][independent_vars[col]])
        
        # for every row in the data frame
        rows = collectWrapper(df)
        # for i in range(0, len(df)):   
        for r in rows: 
            ind_var_locations = []
            
            for col in range(0, max_ind):
                # ind_var_locations.append(df.iloc[i][independent_vars[col]])
                ind_var_locations.append(r[independent_vars[col]])
            
            
            # dependent_var_location = df.iloc[i][dep_var_name]
            dependent_var_location = r[dep_var_name]
           
            data.append(LabeledPoint(dependent_var_location, ind_var_locations))
            
            # data.append(LabeledPoint(imp, [seasoning, ln_seasoning, dU]))
            
        print("buildData LabeledPoint data: ")
        #print(data)    
        return data 
    
    def buildData(self, df, dependent_vars, independent_vars):
        data = []
        max_dep = len(dependent_vars)
        max_ind = len(independent_vars)
        dep_var_name = dependent_vars[0] #  can be only one dependent variable so take first one
        ind_var_locations = []
        
        print("In buildData input df: ") 
        # print(df)   
        
        #for col in range(0, max_ind):
        #    ind_var_locations.append(df.iloc[0][independent_vars[col]])
        
        # for every row in the data frame
        # rows = collectWrapper(df)
        for i in range(0, len(df)):   
        # for r in rows: 
            ind_var_locations = []
            
            for col in range(0, max_ind):
                ind_var_locations.append(df.iloc[i][independent_vars[col]])
                #ind_var_locations.append(r[independent_vars[col]])
            
            
            dependent_var_location = df.iloc[i][dep_var_name]
            # dependent_var_location = r[dep_var_name]
           
            data.append(LabeledPoint(dependent_var_location, ind_var_locations))
            
            # data.append(LabeledPoint(imp, [seasoning, ln_seasoning, dU]))
            
        print("buildData LabeledPoint data: ")
        #print(data)    
        return data 
    
    def buildDataImp(self, df, dependent_vars, independent_vars):
        data = []
        max_dep = len(dependent_vars)
        max_ind = len(independent_vars)
        dep_var_name = dependent_vars[0] #  can be only one dependent variable so take first one
        ind_var_locations = []
        
        print("In buildData input df: ") 
        # print(df)   
        rows = collectWrapper(df)
        # for every row in the data frame
        #for i in range(0, rows):
        for r in rows:
            seasoning = r['seasoning']
            ln_seasoning = r['ln_seasoning']
            dU = r['dU']
            
         #   for col in range(0, max_ind):
          #      ind_var_locations.append(df.iloc[i][independent_vars[col]])
            
            imp = r['imp']
            
        #    dependent_var_location = df.iloc[i][dep_var_name]   
            # data.append(LabeledPoint(dependent_var_location, ind_var_locations))
            data.append(LabeledPoint(imp, [seasoning, ln_seasoning, dU]))
            
        print("buildData LabeledPoint data: ")
        #print(data)    
        return data 
    
    def fit(self, dataframe, dependent_vars, independent_vars):
        "This will return a fitted NonLinearRegressionModel"
        
        if not isinstance(dataframe, DataFrame):
            raise InputException(dataframe, "The dataframe parameter must be of type pyspark.sql.DataFrame")
#         
        self.dataframe = dataframe
        self.dependent_vars = dependent_vars
        self.independent_vars = independent_vars
        
        # construct dataframe from specified independent and dependent variables
        
        # data = self.buildDataImp(dataframe, dependent_vars, independent_vars) # self.buildData(dataframe, dependent_vars, independent_vars)
        # data = self.buildDataDet(dataframe, dependent_vars, independent_vars)
        # data = self.buildDataPpd(dataframe, dependent_vars, independent_vars)
        
        # data = self.buildData(dataframe, dependent_vars, independent_vars) # self.
        data = self.buildDataCollect(dataframe, dependent_vars, independent_vars) # self.
        
        print("Done Building data.\n Number of rows in data:")
        print(len(data))
        
        # self.logisticRegression.fit(df)
        
        # lrm = LogisticRegressionWithLBFGS.train(sc.parallelize(data), numClasses=2, iterations=10)
        
        # self.logisticRegression.train(sc.parallelize(data), numClasses=2, iterations=10)
        print("Creating LogisticRegressionModel")
        self.model = LogisticRegressionModel()
        print("Performing Logistic Regression fit...")
        self.model.summary = LogisticRegressionWithLBFGS.train(sc.parallelize(data), iterations=10)
        print("Model Summary:")
        print(self.model.summary)
        # print(self.model.summary.predict(data))
    
        self.model.summary.coefficients = self.model.summary.weights
      
        #return (LogisticRegressionWithLBFGS.train(sc.parallelize(data), iterations=10))
        return(self.model.summary)
        

def testLogisticRegressionFit():
    """
    Test logistic regression
    """
    
    import pandas as pd
    sc = SparkContext.getOrCreate()
    sqlContext = HiveContext(sc)

    #sampleDataframe = pd.read_csv('/data/1/gftrrasn/ace/python/kb55961/output/tblPlus12_27_2016.csv')
    sampleDataframe = sqlContext.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true").option("delimiter",",").load("/user/gftrrasn/ace_poc/AU/tblPlus12_29_2016.csv")
    
    # Read the data into a pandas data frame
    # Construct a list of LabeledPoints with just the columns (predictors/response) involved in the regression.  
    # Logistic Regression formula: imp ~ seasoning + ln_seasoning + dU

    # sample_data = pd.read_csv('/home/ss71165/SAMPLE.csv')
    # df = pd.DataFrame(sample_data)

    
    lr = LogisticRegression(max_iterations=10)
    
    
    dependent_vars = ['imp']
    independent_vars = ['seasoning', 'dU', 'ln_seasoning']
    print("Fitting imp Logistic Regression model...")
    print("Dependent variables (Response): ")
    print(dependent_vars)
    
    print("Independent variables (Predictors): ")
    print(independent_vars)
    lrModelresult = lr.fit(sampleDataframe, dependent_vars, independent_vars)  
   
    
    print("Obtaining Logistic Regression model summary...")  
    summary = lrModelresult
    print("Logistic Regression model summary coefficients...") 
    # print(summary.coefficients())
    print(lrModelresult.weights)
    print(lrModelresult.intercept)
    
    # indVarsdet = ['seasoning' 'ln_seasoning' 'ccltv' 'dU' 'dGDP_YOY' 'mortgage_rate'];
    # regResultsdet = %logistic(sampleDataset 'detoutput' 'reg_weight' indVarsdet 'det');
    dependent_vars = ['det']
    independent_vars = ['seasoning', 'ln_seasoning', 'ccltv', 'dU', 'dGDP_YOY', 'mortgage_rate']
    print("Fitting det Logistic Regression model...")
    print("Dependent variables (Response): ")
    print(dependent_vars)
    
    print("Independent variables (Predictors): ")
    print(independent_vars)
    lrModelresult = lr.fit(sampleDataframe, dependent_vars, independent_vars)  
   
    print("Obtaining Logistic Regression model summary...")  
    summary = lrModelresult
    print("Logistic Regression model summary coefficients...") 
    # print(summary.coefficients())
    print(lrModelresult.weights)
    print(lrModelresult.intercept)
    
    # indVarsppd = ['seasoning' 'ln_seasoning' 'dGDP_YOY' 'mortgage_rate_change' 'HPA_lag3'];
    # regResultsppd = %logistic(sampleDataset 'ppdoutput' 'reg_weight' indVarsppd 'ppd');
    dependent_vars = ['ppd']
    independent_vars = ['seasoning', 'ln_seasoning', 'dGDP_YOY', 'mortgage_rate_change', 'HPA_lag3']
    print("Fitting ppd Logistic Regression model...")
    print("Dependent variables (Response): ")
    print(dependent_vars)
    
    print("Independent variables (Predictors): ")
    print(independent_vars)
    lrModelresult = lr.fit(sampleDataframe, dependent_vars, independent_vars)  
   
    print("Obtaining Logistic Regression model summary...")  
    summary = lrModelresult
    print("Logistic Regression model summary coefficients...") 
    # print(summary.coefficients())
    print(lrModelresult.weights)
    print(lrModelresult.intercept)
 
    
    print("Saving Logistic Regression model...")
    #print(lrModelresult.save(sc,"logisticResults"))
    
    print("Loading Logistic Regression model...")
    print(lrModelresult.load(sc,"logisticResults"))
    
    print("Logistic Regression model summary standard error...") 
    # summary.std_error()
    
    
def testLogistRegressionSummary():    
     """
    Test logistic regression Summary
    
    """    

def testLogistRegressionLoad(): 
   """
    Test logistic regression Load
    """    

    #print("Loading Logistic Regression model...")
    #print(lrModelresult.load(sc,"logisticResults"))


def testLogistRegressionSave():    
     """
    Test logistic regression Save
    """
    
if __name__ == "__main__":
    sc = SparkContext()
    testLogisticRegressionFit()   

function.mlib.nls.py
"""
Created on Mon Jan 09 11:32:11 2017

@author: ss71165
"""

from pyper import *
import re
import inspect

r = R(RCMD="/var/app/revor/lib64/Revo-7.0/R-3.0.2/bin/R", use_numpy=True)
# RCMD="C:\\Users\\ss71165\\R\\R-3.2.2\\bin\\R"
r("library(nlstools)")

# Chart Path
chartpath = "/data/1/gftrrasn/ace/chart/"

# Make sure the below export is done in the environment so that R can find the minpack.lm library
#export R_LIBS=/home/ss71165/R/x86_64-unknown-linux-gnu-library/3.0

def doc(topic):
    """
    Display documentation for a function/topic
    """
    rcall = "?" + topic
    print(r(rcall))
    
def ls():
    """
    List the R objects
    """
    print(r("ls()"))    
    
def head(df, n=2):
    """
    Show the head n rows
    """
    rcode = "head(" + df + ", " + str(n) + ")"
    print(r(rcode))
    
def plotRes(model, chartname):
    """
    Produce a Residual Chart
    """
    rcode = "png('" + chartpath + chartname + ".png')"
    print(rcode)
    print(r(rcode))
    rplotcode = "plot(" + model +" , which=1)"
    print(rplotcode)
    print(r(rplotcode))
    r("dev.off()")
    
def plotResStd(model, chartname):
    """
    Produce a Stadardized Residual Chart
    """
    rcode = "png('" + chartpath + chartname + "SR.png')"
    print(rcode)
    print(r(rcode))
    r("nlr <- nlsResiduals(" + model +")")
    rplotcode = "plot(nlr , which=2)"
    print(rplotcode)
    print(r(rplotcode))
    r("dev.off()")    
    
def plotQQ(model, chartname):
    """
    Produce a QQ Chart
    """
    rcode = "png('" + chartpath + chartname + "QQ.png')"
    print(rcode)
    print(r(rcode))
    r("nlr <- nlsResiduals(" + model +")")
    rplotcode = "plot(nlr , which=6)"
    print(rplotcode)
    print(r(rplotcode))
    r("dev.off()")       
    
# Support functions required for objective function    

def ifelse(expression, x, y):
    if(expression):
        return x
    else:
        return y

mv_fun_code = "missingValue <- function(x) return(0.000001)"
    

def summary(results):
    rcall = "summary("+results+")"
    print(r(rcall))
    
def residuals(model):
    rcode = "rs" + " <- residuals(" + model +")"
    print(rcode)
    print(r(rcode))
    return(r['rs'])
         
def coef(model):
    rcode = "cf"+ " <- coef(" + model +")"
    print(rcode)
    print(r(rcode))
    return(r['cf'])
    
def stderror(fitted_model):
    rcode = "se"+ " <- coef(summary(" + fitted_model +"))[, 2]"
    print(rcode)
    print(r(rcode))
    return(r['se'])    
    
def readcsv(tbl, path):
    rcode = tbl + ' <- read.table("' + path + '", header=TRUE, sep=",", quote="")'
    print(rcode)
    result = r(rcode)
    return(result)

#def nonlinear_regression_fit(formula, data, initial_values, weights_column, iterations, epsfcn, maxfev):

def createStartValues(name, startValues):
    rcode = name + " <- list("
    n = len(startValues)-1
    print(n)
    i = 0
    for key in startValues:
        namedValue = key + " = " + str(startValues[key])
        if i < n:
            namedValue = namedValue + ", "
        rcode = rcode + namedValue
        i = i + 1
    rcode = rcode + ")"    
    print(rcode)
    r(rcode)

def extractFunction(formula):
    name = formula.split("~")[1].split("(")[0].strip()
    print(name)
    return(name)

def save(model, path):
    """
    Save fitted model to file
    """
    
    rcode = "save(model=" + model + ", file=" + path + ")"
    print(r(rcode))   
    
def load(path):
    """
    Load fitted model from file
    """
    
    rcode = "load(" + path + ")"
    print(r(rcode))     
    
def nls(data, fitted_model, graph, formula, start, weights="NA", maxiter = 25):
#    Invokes R nlsLM()

    control_string = ", control = list(maxiter=" + str(maxiter) + ")"
    r("missingValue <- function(x) return(0.000001)")
    convertFunction(extractFunction(formula))
    rcode = fitted_model + "<- nls(formula=" + formula + ", data=" + data + ', start=' + start + ", weights=" + weights + control_string + ")"
    print(rcode)
    print(r(rcode))
    plotRes(fitted_model, graph)
      
def convertFunction(name):
    py_fun = eval('inspect.getsource(' + name + ')')
    
    # replace "def name:" with: "name -> function(args)" 
    fname = py_fun.split("(")[0].split(" ")[-1]
    fun_args = py_fun.split(")")[0].split("(")[-1]
    fun_args = fun_args.replace("\n", "")
    r_header = name + " <- function(" + fun_args + ") {"
    # add the function body
    
    r_body = py_fun.split(":")[1]
    r_body = r_body.split("\n")[0:]
    # construct the R function
    r_fun = r_header
    #print(r_header)
    file = open(name+".R", "w")
    file.write(r_header)
    for i in range(len(r_body)):
        r_fun = r_fun + r_body[i] + ";"
        file.write(r_body[i])
        file.write("\n")
    r_fun = r_fun + "}"
    file.write("}")
    file.close()
    print(r_fun)
    #r(r_fun)
    r("source('" + name + ".R')")
    return(r_fun)
    
def createObjectiveFunction(name):
    py_fun = eval('inspect.getsource(' + name + ')')
    
    # replace "def name:" with: "name -> function(args)" 
    fname = py_fun.split("(")[0].split(" ")[-1]
    fun_args = py_fun.split(")")[0].split("(")[-1]
    fun_args = fun_args.replace("\n", "")
    r_header = name + " <- function(" + fun_args + ") {"
    # add the function body
    
    r_body = py_fun.split(":")[1]
    r_body = r_body.split("\n")[0:]
    # construct the R function
    r_fun = r_header
    #print(r_header)
    file = open(name+".R", "w")
    file.write(r_header)
    for i in range(len(r_body)):
        r_fun = r_fun + r_body[i] + ";"
        file.write(r_body[i])
        file.write("\n")
    r_fun = r_fun + "}"
    file.write("}")
    file.close()
    print(r_fun)
    #r(r_fun)
    r("source('" + name + ".R')")
    return(r_fun)

    



function.mlib.nlsLM.py
"""
Created on Mon Jan 09 11:32:11 2017

@author: ss71165
"""

from pyper import *
import re
import inspect

# Path to R, below is the standard location on the EAP Big Data Platform
r = R(RCMD="/var/app/revor/lib64/Revo-7.0/R-3.0.2/bin/R", use_numpy=True)
# RCMD="C:\\Users\\ss71165\\R\\R-3.2.2\\bin\\R"

r("library(minpack.lm)")
r("library(nlstools)")

# Chart Path
#chartpath = "/data/1/gftrrasn/ace/chart/"
chartpath = "/data/1/gftrrhrn/ace/chart/"

# Make sure the below export is done in the environment so that R can find the minpack.lm library
#export R_LIBS=/home/ss71165/R/x86_64-unknown-linux-gnu-library/3.0

def doc(topic):
    """
    Display documentation for a function/topic
    """
    rcall = "?" + topic
    print(r(rcall))
    
def ls():
    """
    List the R objects
    """
    print(r("ls()"))    
    
def head(df, n=2):
    """
    Show the head n rows
    """
    rcode = "head(" + df + ", " + str(n) + ")"
    print(r(rcode))
    
def plotContour(model, chartname):
    """
    Produce a Contour Chart
    """
    r("library(nlstools)")
    rcode = "png('" + chartpath + chartname + ".png')"
    print(rcode)
    print(r(rcode))
    r("nlr <- nlsContourRSS(" + model +")")
    rplotcode = "plot(nlr, col=FALSE, nlev=5)"
    print(rplotcode)
    print(r(rplotcode))
    r("dev.off()")  
    
def plotConfRegions(model, chartname):
    """
    Produce a Confidence Interval Chart
    """
    r("library(nlstools)")
    rcode = "png('" + chartpath + chartname + ".png')"
    print(rcode)
    print(r(rcode))
    r("nlr <- nlsConfRegions(" + model +")")
    rplotcode = "plot(nlr, bounds=TRUE)"
    print(rplotcode)
    print(r(rplotcode))
    r("dev.off()")    
    
def plotRes(model, chartname):
    """
    Produce a Residual Chart
    """
    r("library(nlstools)")
    rcode = "png('" + chartpath + chartname + ".png')"
    print(rcode)
    print(r(rcode))
    r("nlr <- nlsResiduals(" + model +")")
    rplotcode = "plot(nlr , which=1)"
    print(rplotcode)
    print(r(rplotcode))
    r("dev.off()")
    
def plotResStd(model, chartname):
    """
    Produce a Standardized Residual Chart
    """
    rcode = "png('" + chartpath + chartname + "2.png')"
    print(rcode)
    print(r(rcode))
    r("nlr <- nlsResiduals(" + model +")")
    rplotcode = "plot(nlr , which=2)"
    print(rplotcode)
    print(r(rplotcode))
    r("dev.off()")    
    
def plotQQ(model, chartname):
    """
    Produce a QQ Char
    """
    rcode = "png('" + chartpath + chartname + "6.png')"
    print(rcode)
    print(r(rcode))
    r("nlr <- nlsResiduals(" + model +")")
    rplotcode = "plot(nlr , which=6)"
    print(rplotcode)
    print(r(rplotcode))
    r("dev.off()")       
    
 
def plotAC(model, chartname):
    """
    Produce Auto-correlation Chart
    """
    rcode = "png('" + chartpath + chartname + "4.png')"
    print(rcode)
    print(r(rcode))
    r("nlr <- nlsResiduals(" + model +")")
    rplotcode = "plot(nlr , which=4)"
    print(rplotcode)
    print(r(rplotcode))
    r("dev.off()")     
    
# Support functions required for objective function    

def ifelse(expression, x, y):
    if(expression):
        return x
    else:
        return y

mv_fun_code = "missingValue <- function(x) return(0.000001)"
    

def summary(results):
    rcall = "summary("+results+")"
    print(r(rcall))
    
def residuals(model):
    rcode = "rs" + " <- residuals(" + model +")"
    print(r(rcode))
    return(r['rs'])
    
def rrange(name):
    rcode = "range(" + name + ")" 
    print(r(rcode))
        
def coef(model):
    rcode = "cf"+ " <- coef(" + model +")"
    print(r(rcode))
    return(r['cf'])
    
def stderror(fitted_model):
    rcode = "se"+ " <- coef(summary(" + fitted_model +"))[, 2]"
    print(r(rcode))
    return(r['se'])    
    
def readcsv(tbl, path):
    rcode = tbl + ' <- read.csv("' + path + '", header=TRUE, stringsAsFactors=FALSE)'
    result = r(rcode)
    return(result)

def readtable(tbl, path):
    rcode = tbl + ' <- read.table("' + path + '", header=TRUE, sep=",", quote="")'
    print(rcode)
    result = r(rcode)
    return(result)

#def nonlinear_regression_fit(formula, data, initial_values, weights_column, iterations, epsfcn, maxfev):

def createStartValues(name, startValues):
    rcode = name + " <- list("
    n = len(startValues)-1
    print(n)
    i = 0
    for key in startValues:
        namedValue = key + " = " + str(startValues[key])
        if i < n:
            namedValue = namedValue + ", "
        rcode = rcode + namedValue
        i = i + 1
    rcode = rcode + ")"    
    print(rcode)
    r(rcode)

def extractFunction(formula):
    name = formula.split("~")[1].split("(")[0].strip()
    print(name)
    return(name)

def save(model, path):
    """
    Save fitted model to file
    """
    
    rcode = "save(model=" + model + ", file=" + path + ")"
    print(r(rcode))   
    
def load(path):
    """
    Load fitted model from file
    """
    
    rcode = "load(" + path + ")"
    print(r(rcode))       

def nlsLM(data, fitted_model, graph, formula, start, weights="NA", maxiter = 25):
    """
    Invokes R nlsLM()
    """
    r("library(minpack.lm)")
    r("library(nlstools)")
    control_string = ", control = list(maxiter=" + str(maxiter) + ")"
    r("missingValue <- function(x) return(0.000001)")
   
    convertFunction(extractFunction(formula))
    rcode = fitted_model + "<- nlsLM(formula=" + formula + ", data=" + data + ', start=' + start + ", weights=" + weights + control_string + ")"
    print(rcode)
    print(r(rcode))
    #r("nlr <- nlsResiduals(" + model +")")
    #plotRes("nlr", graph)
    plotRes(fitted_model, graph)
    plotResStd(fitted_model, graph)
    plotQQ(fitted_model, graph)
    plotAC(fitted_model, graph)
                   
def convertFunction(name):
    py_fun = eval('inspect.getsource(' + name + ')')
    
    # replace "def name:" with: "name -> function(args)" 
    fname = py_fun.split("(")[0].split(" ")[-1]
    fun_args = py_fun.split(")")[0].split("(")[-1]
    fun_args = fun_args.replace("\n", "")
    r_header = name + " <- function(" + fun_args + ") {"
    # add the function body
    
    r_body = py_fun.split(":")[1]
    r_body = r_body.split("\n")[0:]
    # construct the R function
    r_fun = r_header
    #print(r_header)
    file = open(name+".R", "w")
    file.write(r_header)
    for i in range(len(r_body)):
        r_fun = r_fun + r_body[i] + ";"
        file.write(r_body[i])
        file.write("\n")
    r_fun = r_fun + "}"
    file.write("}")
    file.close()
    print(r_fun)
    #r(r_fun)
    r("source('" + name + ".R')")
    return(r_fun)

def createObjectiveFunction(name):
    py_fun = eval('inspect.getsource(' + name + ')')
    
    # replace "def name:" with: "name -> function(args)" 
    fname = py_fun.split("(")[0].split(" ")[-1]
    fun_args = py_fun.split(")")[0].split("(")[-1]
    fun_args = fun_args.replace("\n", "")
    r_header = name + " <- function(" + fun_args + ") {"
    # add the function body
    
    r_body = py_fun.split(":")[1]
    r_body = r_body.split("\n")[0:]
    # construct the R function
    r_fun = r_header
    #print(r_header)
    file = open(name+".R", "w")
    file.write(r_header)
    for i in range(len(r_body)):
        r_fun = r_fun + r_body[i] + ";"
        file.write(r_body[i])
        file.write("\n")
    r_fun = r_fun + "}"
    file.write("}")
    file.close()
    print(r_fun)
    #r(r_fun)
    r("source('" + name + ".R')")
    return(r_fun)



    
def myfunc(ccltv, dU, ln_seasoning, mortgage_rate, mortgage_rate_change, seasoning, transition, u, dGDP_yoy, hpa_lag3,
                    d1, d2, d3,
                    d5, d6, d7, i1, i2,
                    i4,
                    p1, p2,
                    p4, p5, p6, bC30, bC180, bCPPD, bD30, bD180, bDPPD, b30D, b3060, b30180, b30PPD, b60D, b6030, b6090,
                    b60180, b60PPD, b90D, b9030,b9060, b90120, b90180, b90PPD, b120D, b12030, b12060,
                    b12090, b120150, b120180, b120PPD, b150D, b15030, b15060, b15090, b150120, b150180, b180LIQ):
  
  zDet =  d1 * seasoning + d2 * ln_seasoning + d3 * ccltv + d5 * dU + d6 * dGDP_yoy + d7 * mortgage_rate;
   
  zImp =  i1 * seasoning + i2 * ln_seasoning + i4 * u;
  
  zPpd =  p1 * seasoning + p2 * ln_seasoning + p4 * dGDP_yoy + p5 * mortgage_rate_change + p6 * hpa_lag3;
  
  zC30 = bC30 + zDet;
  zC180 = bC180 + zDet;
  zCPPD = bCPPD + zPpd;
  pC30 = exp(zC30) / (1 + exp(zC30) + exp(zC180) + exp(zCPPD));
  pC180 = exp(zC180) / (1 + exp(zC30) + exp(zC180) + exp(zCPPD));
  pCPPD = exp(zCPPD) / (1 + exp(zC30) + exp(zC180) + exp(zCPPD));
  pCC = 1 - pC30 - pC180 - pCPPD;
  
  zD30 = bD30 + zDet;
  zD180 = bD180 + zDet;
  zDPPD = bDPPD + zPpd;
  pD30 = exp(zD30) / (1 + exp(zD30) + exp(zD180) + exp(zDPPD));
  pD180 = exp(zD180) / (1 + exp(zD30) + exp(zD180) + exp(zDPPD));
  pDPPD = exp(zDPPD) / (1 + exp(zD30) + exp(zD180) + exp(zDPPD));
  pDD = 1 - pD30 - pD180 - pDPPD;
  
  z30D = b30D + zImp;
  z3060 = b3060 + zDet;
  z30180 = b30180 + zDet;
  z30PPD = b30PPD + zPpd;
  p30D = exp(z30D) / (1 + exp(z30D) + exp(z3060) + exp(z30180) + exp(z30PPD));
  p3060 = exp(z3060) / (1 + exp(z30D) + exp(z3060) + exp(z30180) + exp(z30PPD));
  p30180 = exp(z30180) / (1 + exp(z30D) + exp(z3060) + exp(z30180) + exp(z30PPD));
  p30PPD = exp(z30PPD) / (1 + exp(z30D) + exp(z3060) + exp(z30180) + exp(z30PPD));
  p3030 = 1 - p30D - p3060 - p30180 - p30PPD;
  
  z60D = b60D + zImp;
  z6030 = b6030 + zImp;
  z6090 = b6090 + zDet;
  z60180 = b60180 + zDet;
  z60PPD = b60PPD + zPpd;
  p60D = exp(z60D) / (1 + exp(z60D) + exp(z6030) + exp(z6090) + exp(z60180) + exp(z60PPD));
  p6030 = exp(z6030) / (1 + exp(z60D) + exp(z6030) + exp(z6090) + exp(z60180) + exp(z60PPD));
  p6090 = exp(z6090) / (1 + exp(z60D) + exp(z6030) + exp(z6090) + exp(z60180) + exp(z60PPD));
  p60180 = exp(z60180) / (1 + exp(z60D) + exp(z6030) + exp(z6090) + exp(z60180) + exp(z60PPD));
  p60PPD = exp(z60PPD) / (1 + exp(z60D) + exp(z6030) + exp(z6090) + exp(z60180) + exp(z60PPD));
  p6060 = 1 - p60D - p6030 - p6090 - p60180 - p60PPD;
  
  z90D = b90D + zImp;
  z9030 = b9030 + zImp;
  z9060 = b9060 + zImp;
  z90120 = b90120 + zDet;
  z90180 = b90180 + zDet;
  z90PPD = b90PPD + zPpd;
  p90D = exp(z90D) / (1 + exp(z90D) + exp(z9030) + exp(z9060) + exp(z90120) + exp(z90180) + exp(z90PPD));
  p9030 = exp(z9030) / (1 + exp(z90D) + exp(z9030) + exp(z9060) + exp(z90120) + exp(z90180) + exp(z90PPD));
  p9060 = exp(z9060) / (1 + exp(z90D) + exp(z9030) + exp(z9060) + exp(z90120) + exp(z90180) + exp(z90PPD));
  p90120 = exp(z90120) / (1 + exp(z90D) + exp(z9030) + exp(z9060) + exp(z90120) + exp(z90180) + exp(z90PPD));
  p90180 = exp(z90180) / (1 + exp(z90D) + exp(z9030) + exp(z9060) + exp(z90120) + exp(z90180) + exp(z90PPD));
  p90PPD = exp(z90PPD) / (1 + exp(z90D) + exp(z9030) + exp(z9060) + exp(z90120) + exp(z90180) + exp(z90PPD));
  p9090 = 1 - p90D - p9030 - p9060 - p90120 - p90180 - p90PPD;
  
  z120D = b120D + zImp;
  z12030 = b12030 + zImp;
  z12060 = b12060 + zImp;
  z12090 = b12090 + zImp;
  z120150 = b120150 + zDet;
  z120180 = b120180 + zDet;
  z120PPD = b120PPD + zPpd;
  p120D = exp(z120D) / (1 + exp(z120D) + exp(z12030) + exp(z12060) + exp(z12090) + exp(z120150) + exp(z120180) + exp(z120PPD));
  p12030 = exp(z12030) / (1 + exp(z120D) + exp(z12030) + exp(z12060) + exp(z12090) + exp(z120150) + exp(z120180) + exp(z120PPD));
  p12060 = exp(z12060) / (1 + exp(z120D) + exp(z12030) + exp(z12060) + exp(z12090) + exp(z120150) + exp(z120180) + exp(z120PPD));
  p12090 = exp(z12090) / (1 + exp(z120D) + exp(z12030) + exp(z12060) + exp(z12090) + exp(z120150) + exp(z120180) + exp(z120PPD));
  p120150 = exp(z120150) / (1 + exp(z120D) + exp(z12030) + exp(z12060) + exp(z12090) + exp(z120150) + exp(z120180) + exp(z120PPD));
  p120180 = exp(z120180) / (1 + exp(z120D) + exp(z12030) + exp(z12060) + exp(z12090) + exp(z120150) + exp(z120180) + exp(z120PPD));
  p120PPD = exp(z120PPD) / (1 + exp(z120D) + exp(z12030) + exp(z12060) + exp(z12090) + exp(z120150) + exp(z120180) + exp(z120PPD));
  p120120 = 1 - p120D - p12030 - p12060 - p12090 - p120150 - p120180 - p120PPD;
  
  z150D = b150D + zImp;
  z15030 = b15030 + zImp;
  z15060 = b15060 + zImp;
  z15090 = b15090 + zImp;
  z150120 = b150120 + zImp;
  z150180 = b150180 + zDet;
  p150D = exp(z150D) / (1 + exp(z150D) + exp(z15030) + exp(z15060) + exp(z15090) + exp(z150120) + exp(z150180));
  p15030 = exp(z15030) / (1 + exp(z150D) + exp(z15030) + exp(z15060) + exp(z15090) + exp(z150120) + exp(z150180));
  p15060 = exp(z15060) / (1 + exp(z150D) + exp(z15030) + exp(z15060) + exp(z15090) + exp(z150120) + exp(z150180));
  p15090 = exp(z15090) / (1 + exp(z150D) + exp(z15030) + exp(z15060) + exp(z15090) + exp(z150120) + exp(z150180));
  p150120 = exp(z150120) / (1 + exp(z150D) + exp(z15030) + exp(z15060) + exp(z15090) + exp(z150120) + exp(z150180));
  p150180 = exp(z150180) / (1 + exp(z150D) + exp(z15030) + exp(z15060) + exp(z15090) + exp(z150120) + exp(z150180));
  p150150 = 1 - p150D - p15030 - p15060 - p15090 - p150120 - p150180;
  
  z180LIQ = b180LIQ;
  p180LIQ = exp(z180LIQ) / (1 + exp(z180LIQ));
  p180180 = 1 - p180LIQ;
  
  p=ifelse(
      transition == 'CC_CC', pCC,
      ifelse(
        transition == 'CC_30', pC30,
        ifelse(
          transition == 'CC_180', pC180,
          ifelse(
            transition == 'CC_PPD', pCPPD,
            ifelse(
              transition == 'CD_CD', pDD,
              ifelse(
                transition == 'CD_30', pD30,
                ifelse(
                  transition == 'CD_180', pD180,
                  ifelse(
                    transition == 'CD_PPD', pDPPD,
                    ifelse(
                      transition == '30_CD', p30D,
                      ifelse(
                        transition == '30_30', p3030,
                        ifelse(
                          transition == '30_60', p3060,
                          ifelse(
                            transition == '30_180', p30180,
                            ifelse(
                              transition == '30_PPD', p30PPD,
                              ifelse(
                                transition == '60_CD', p60D,
                                ifelse(
                                  transition == '60_30', p6030,
                                  ifelse(
                                    transition == '60_60', p6060,
                                    ifelse(
                                      transition == '60_90', p6090,
                                      ifelse(
                                        transition == '60_180', p60180,
                                        ifelse(
                                          transition == '60_PPD', p60PPD,
                                          ifelse(
                                            transition == '90_CD', p90D,
                                            ifelse(
                                              transition == '90_30', p9030,
                                              ifelse(
                                                transition == '90_60', p9060,
                                                ifelse(
                                                  transition == '90_90', p9090,
                                                  ifelse(
                                                    transition == '90_120', p90120,
                                                    ifelse(
                                                      transition == '90_180', p90180,
                                                      ifelse(
                                                        transition == '90_PPD', p90PPD,
                                                        ifelse(
                                                          transition == '120_CD', p120D,
                                                          ifelse(
                                                            transition == '120_30', p12030,
                                                            ifelse(
                                                              transition == '120_60', p12060,
                                                              ifelse(
                                                                transition == '120_90', p12090,
                                                                ifelse(
                                                                  transition == '120_120', p120120,
                                                                  ifelse(
                                                                    transition == '120_150', p120150,
                                                                    ifelse(
                                                                      transition == '120_180', p120180,
                                                                      ifelse(
                                                                        transition == '120_PPD', p120PPD,
                                                                        ifelse(
                                                                          transition == '150_CD', p150D,
                                                                          ifelse(
                                                                            transition == '150_30', p15030,
                                                                            ifelse(
                                                                              transition == '150_60', p15060,
                                                                              ifelse(
                                                                                transition == '150_90', p15090,
                                                                                ifelse(
                                                                                  transition == '150_120', p150120,
                                                                                  ifelse(
                                                                                    transition == '150_150', p150150,
                                                                                    ifelse(
                                                                                      transition == '150_180', p150180,
                                                                                      ifelse(
                                                                                        transition == '180_180', p180180,
                                                                                        ifelse(
                                                                                          transition =='180_LIQ', p180LIQ, 
                                                                                            missingValue(transition))))))))))))))))))))))))))))))))))))))))))));
  p = ifelse(p > 0, p,1e-6);
  retVal = (sqrt(-2 * log(p)));
  return(retVal);
        
if __name__ == "__main__":
    #sc = SparkContext()
    #testNlsLM()
    #testNonlinearRegressionFit()
    r = R(use_numpy=True)
    print(r("Sys.getenv('R_LIBS')"))
    r("library(minpack.lm)")
    
    readcsv("sampleData", "I:/Documents/dev/POC/Aus/DataLocal/Sample_data_Without_CCQ1.csv")
    start_values_code = """parmStartList <- list(
                                                 d1 = 1e-6, d2 = 1e-6, d3 = 1e-6, 
                                                 d5    = 1e-6, d6 = 1e-6,  d7 = 1e-6,  i1 = 1e-6, i2    = 1e-6, 
                                                 i4    = 1e-6, p1    = 1e-6, p2    = 1e-6,
                                                 p4    = 1e-6, p5    = 1e-6, p6    = 1e-6,  bC30    = 1e-6,  bC180 = 1e-6,
                                                 bCPPD      = 1e-6, bD30    = 1e-6,   bD180    = 1e-6,    bDPPD    = 1e-6,   b30D    = 1e-6,   b3060     = 1e-6, b30180    = 1e-6,
                                                 b30PPD  = 1e-6, b60D    = 1e-6,   b6030    = 1e-6,    b6090    = 1e-6,   b60180    = 1e-6, b60PPD = 1e-6, b90D    = 1e-6, 
                                                 b9030      = 1e-6, b9060    = 1e-6,   b90120    = 1e-6,  b90180    = 1e-6,   b90PPD    = 1e-6, b120D     = 1e-6, b12030    = 1e-6,
                                                 b12060  = 1e-6, b12090 = 1e-6,  b120150    = 1e-6,  b120180= 1e-6, b120PPD    = 1e-6, b150D     = 1e-6,
                                                 b15030    = 1e-6, b15060 = 1e-6,  b15090    = 1e-6,  b150120= 1e-6, b150180    = 1e-6, b180LIQ    = 1e-6
                                                 )"""

    
    #r(start_values_code)
    #r(mv_fun_code)
    #r(nl_fun_code)
    convertFunction("myfunc")
    createStartValues("parmStartList",
                      {'d1': 1e-6,
                       'd2': 1e-6,
                       'd3': 1e-6,
                       'd5': 1e-6,
                       'd6': 1e-6,
                       'd7': 1e-6,
                       'i1': 1e-6,
                       'i2': 1e-6,
                       'i4': 1e-6,
                       'p1': 1e-6,
                       'p2': 1e-6,
                       'p4': 1e-6,
                       'p5': 1e-6,
                       'p6': 1e-6,
                       'bC30': 1e-6,
                       'bC180': 1e-6,
                       'bCPPD': 1e-6,
                       'bD30': 1e-6,
                       'bD180': 1e-6,
                       'bDPPD':    1e-6,
                       'b30D':    1e-6,
                       'b3060':    1e-6,
                       'b30180': 1e-6,
                       'b30PPD': 1e-6,
                       'b60D': 1e-6,
                       'b6030':    1e-6,
                       'b6090': 1e-6,
                       'b60180': 1e-6,
                       'b60PPD': 1e-6,
                       'b90D': 1e-6, 
                       'b9030': 1e-6,
                       'b9060': 1e-6,
                       'b90120': 1e-6,
                       'b90180': 1e-6,
                       'b90PPD': 1e-6,
                       'b120D': 1e-6,
                       'b12030': 1e-6,
                       'b12060': 1e-6,
                       'b12090': 1e-6,
                       'b120150': 1e-6,
                       'b120180': 1e-6,
                       'b120PPD': 1e-6,
                       'b150D': 1e-6,
                       'b15030': 1e-6,
                       'b15060': 1e-6,
                       'b15090': 1e-6,
                       'b150120': 1e-6,
                       'b150180': 1e-6,
                       'b180LIQ': 1e-6
                       })
   
    nlsLM(data="sampleData", 
          fitted_model="nlslm_fitted_model", 
          graph="nlslm_res", 
          formula="""loglik ~ myfunc(ccltv, dU, ln_seasoning, mortgage_rate, mortgage_rate_change, seasoning, transition, U, dGDP_YOY, HPA_lag3,
                    d1, d2, d3,d5, d6, d7, i1, i2, i4, p1, p2,
                    p4, p5,p6,bC30, bC180, bCPPD, bD30, bD180, bDPPD, b30D, b3060, b30180, b30PPD, b60D, b6030, b6090, b60180,
                    b60PPD, b90D, b9030, b9060, b90120, b90180, b90PPD, b120D,b12030, b12060, b12090, b120150, b120180, b120PPD,
                    b150D, b15030, b15060, b15090, b150120, b150180, b180LIQ)""", 
          start="parmStartList", 
          weights="reg_weight", maxiter=1)

    



function.mlib.nonlinearRegressionBFGS.py
from com.citi.risk.analytics.ml.nonlinear import NonlinearRegression
from pyspark import SparkContext, SparkConf, SQLContext
import numpy as np
from pyspark.sql.types import Row
import os
      
  
conf = SparkConf().setAppName("Test DataExpression")\
                  .setMaster("local")\
                  .set("spark.sql.warehouse.dir", "tmp")
   
sc = SparkContext(conf=conf)
sqlContext = SQLContext(sc)
   
   
def buildTraingData():
    w0_true = 5.3
    w1_true = 3.8
    x_true = 10 * np.random.sample(10000)
    rawData = []
    for i in range(len(x_true)):
        x = np.float64(x_true[i]).item()
        y = np.float64(w0_true * np.exp(-w1_true * x)).item() + 0.1 * np.random.random_sample()
        rawData.append((x, y))
       
    df = sqlContext.createDataFrame(rawData, ['x', 'y'])
    data = df.map(lambda row : Row(x=np.array([row.x]), y=row.y))
       
    return data
       
   
def func(weights, features):
    return weights[0] * np.exp(-weights[1] * features[0])
   
def gradf(weights, features):
    gper = np.zeros(len(weights))
    gper[0] = np.exp(-weights[1] * features[0])
    gper[1] = -weights[0] * features[0] * np.exp(-weights[1] * features[0])
       
    return gper
   
data = buildTraingData()
   
nlr = NonlinearRegression(initialGuess=np.array([1.0, 1.0]), labelCol="y", featuresCol="x", func=func, gradf=gradf, method="BFGS")
model = nlr.fit(data)
 
print "************************* summary *************************"
print model.summary
 
 
print "************************* statistics *************************"
print model.stat.summary()
 
 
print "************************* predict *************************"
print model.predict(np.array([3.0]))

#path = os.path.join(os.environ["py.basedir"], "target/nonlinear.model")
path  = "/data/1/gftrrasn/ace/python/ss71165/run/nonlinear.model"
model.save(path)

saved_model = model.load(path)
print saved_model.predict(np.array([3.0]))
  
 

function.mlib.nonlinearRegressionLM.py
from com.citi.risk.analytics.ml.lmoptimizer import LevenbergMarquardtOptimizer
import unittest
import os, platform
from pyspark import SparkContext, SparkConf, SQLContext
from math import exp, sqrt, log
import pandas as pd
from pyspark.sql.types import Row
from pyspark.mllib.common import _java2py

class TestLMOptimizer(unittest.TestCase):

    def testLMOptimizer(self):
        if not os.environ.has_key("SPARK_CLASSPATH"):
            if platform.system() == "Windows":
                os.environ["SPARK_CLASSPATH"] = "C:/hc02929/code/analytics/target/analytics-1.0.0-SNAPSHOT.jar;C:/Users/hc02929/Desktop/pyjars/*"
            else:
                return
        
        """ 
        Prepare local spark context
        """    
        conf = SparkConf().setAppName("TestDFSecurity")\
                          .set("spark.sql.warehouse.dir", "tmp")
                          
        sc = SparkContext(conf=conf)
        sqlContext = SQLContext(sc)
        
        zero_params = [1e-6, 1e-6, 1e-6, 1e-6, 1e-6, 1e-6, 1e-6, 1e-6, 1e-6, 1e-6, 1e-6, 1e-6, 1e-6, 1e-6,  1e-6,  1e-6]
        rdd = buildTraingData(sqlContext)

        lmOptimizer = LevenbergMarquardtOptimizer(calcfunc)
        result = lmOptimizer.optimize(sc, rdd, zero_params, "y", "x", "others")
         
	print(rdd)
	print(result)
        print _java2py(sc, result.getPoint())
        print type(result)
        
def testit():
	if not os.environ.has_key("SPARK_CLASSPATH"):
            if platform.system() == "Windows":
                os.environ["SPARK_CLASSPATH"] = "C:/hc02929/code/analytics/target/analytics-1.0.0-SNAPSHOT.jar;C:/Users/hc02929/Desktop/pyjars/*"
            else:
                return

        """
        Prepare local spark context
        """
        conf = SparkConf().setAppName("TestDFSecurity")\
                          .set("spark.sql.warehouse.dir", "tmp")

        sc = SparkContext(conf=conf)
        sqlContext = SQLContext(sc)

        zero_params = [1e-6, 1e-6, 1e-6, 1e-6, 1e-6, 1e-6, 1e-6, 1e-6, 1e-6, 1e-6, 1e-6, 1e-6, 1e-6, 1e-6,  1e-6,  1e-6]
        rdd = buildTraingData(sqlContext)

        lmOptimizer = LevenbergMarquardtOptimizer(calcfunc)
        result = lmOptimizer.optimize(sc, rdd, zero_params, "y", "x", "others")

        print(rdd)
        print(result)
        print _java2py(sc, result.getPoint())
        print type(result)

        
    
def buildTraingData(sqlContext):
    pds_df = pd.read_csv("/home/ss71165/Sample_data_Without_CCQ1.csv")
    df = sqlContext.createDataFrame(pds_df).select("loglik","ccltv","dU","ln_seasoning","mortgage_rate","mortgage_rate_change","seasoning","uk","transition","U","dGDP_YOY","HPA_lag3")
    newdf = df.rdd.filter(lambda row : row.transition=="CC_CC").map(lambda row : Row(y=row.loglik, x=[row.ccltv,row.dU,row.ln_seasoning,row.mortgage_rate,row.mortgage_rate_change,row.seasoning,row.uk,row.U,row.dGDP_YOY,row.HPA_lag3], others=[row.transition]))
    print newdf.count()
    return newdf    
    
def missingValue(x):
    return 0.000001

def ifelse(expression, x, y):
    if(expression):
        return x
    else:
        return y

def calcfunc(w, x, others):
    params=[]
    params.extend(x)
    params.extend(w)
    params.extend(others)
    v = myfunc(*params)
    return v 

def myfunc(ccltv, dU, ln_seasoning, mortgage_rate, mortgage_rate_change, seasoning, uk, u, dGDP_yoy, hpa_lag3,
                    # Following are parameters whose optimal value will be found
                    d1, d2, d3,
                    d4,
                    d5, d6, d7, #i1, i2,
                  # i3,
                  #  i4,
                  # i5,
                    p1, p2,
                    p3,
                    p4, p5, p6, bC30, bC180, bCPPD, #bD30, bD180, bDPPD, b30D, b3060, b30180, b30PPD, b60D, b6030, b6090,
#                     b60180, b60PPD, b90D, b9030,b9060, b90120, b90180, b90PPD, b120D, b12030, b12060,
#                     b12090, b120150, b120180, b120PPD, b150D, b15030, b15060, b15090, b150120, b150180, b180LIQ
                    transition):
    


    zDet =  d1 * seasoning + \
            d2 * ln_seasoning + \
            d3 * ccltv + \
            d5 * dU + \
            d6 * dGDP_yoy + \
            d7 * mortgage_rate
    
    zPpd =  p1 * seasoning + \
          p2 * ln_seasoning + \
          p4 * dGDP_yoy + \
          p5 * mortgage_rate_change + \
          p6 * hpa_lag3
    
    
    zC30 = bC30 + zDet
    zC180 = bC180 + zDet
    zCPPD = bCPPD + zPpd
    
    pCC = 1 / (1 + exp(zC30) + exp(zC180) + exp(zCPPD))
    
    
    p=ifelse(transition == 'CC_CC', pCC, missingValue(transition))
    
  
    retVal = (sqrt(-2 * log(p)))
    return retVal

if __name__ == "__main__":
  if not os.environ.has_key("SPARK_CLASSPATH"):
    if platform.system() == "Windows":
      os.environ["SPARK_CLASSPATH"] = "C:/hc02929/code/analytics/target/analytics-1.0.0-SNAPSHOT.jar;C:/Users/hc02929/Desktop/pyjars/*"
    else:
      print "On Unix: SPARK_CLASSPATH not set!"

  """
  Prepare local spark context
  """
  conf = SparkConf().setAppName("TestDFSecurity")\
 	.set("spark.sql.warehouse.dir", "tmp")

  sc = SparkContext(conf=conf)
  sqlContext = SQLContext(sc)

  zero_params = [1e-6, 1e-6, 1e-6, 1e-6, 1e-6, 1e-6, 1e-6, 1e-6, 1e-6, 1e-6, 1e-6, 1e-6, 1e-6, 1e-6,  1e-6,  1e-6]
  rdd = buildTraingData(sqlContext)

  lmOptimizer = LevenbergMarquardtOptimizer(calcfunc)
  print(lmOptimizer)
  result = lmOptimizer.optimize(sc, rdd, zero_params, "y", "x", "others")

  print(rdd)
  print(result)
  print _java2py(sc, result.getPoint())
  print type(result)

  #unittest.main()


function.mlib.pyper.py
#!/usr/bin/env python
'''
         PypeR (PYthon-piPE-R)

PypeR is free software subjected to the GPL license 3.0. and comes with
ABSOLUTELY NO WARRANT. This package provides a light-weight interface to use R
in Python by pipe.  It can be used on multiple platforms since it is written in
pure python. 

Prerequisites:
    1. Python 2.3 or later is required.

Usage:
    The usage of this packages is very simple. Examples are presented in the
    file "test.py" in the distribution package.

    PypeR provide a class "R" to wrap the R language. An instance of the R
    class is used to manage an R process. Different instances can use different
    R installations. On POSIX systems (including the Cygwin environment on
    Windows), it is even possible to use an R installed on a remote computer.

    Basicly, there are four ways to use an instance of the R class.

    1. Use the methods of the instance
        methods include:
            run:This method is used to pass an R command string to the R process,
                the return value is a string - the standard output from R. Note
                that the return value usually includes the R expression (a
                series of R codes) themselves and the output of the R
                expression.  If the real result value is wanted, use the
                function "get" instead.
            assign: Assign a value to an R variable. No return value.
            get: Get the result of an R expression.
            remove: Remove a R variable.

    2. Call the instance as a function
        The instance is callable. If called as a function, it behaves just
        same as its "run" method.

    3. Use the instance as a Python dictionary
        The instance can mimic some operations on a python dictionary,
        typically, to assign values to R variables, to retrieve values for any
        R expression, or delete an R variable. These two operations do same
        jobs as the methods "assign", "get", and "remove".

    4. Access R variables as if they are the attributes of the instance.
        If the variable name cannot be found in the instance or its class, the
        instance will try to get/set/remove it in R. This way is similar to 3,
        but with more limitations, e.g., the R variable name cannot contain any
        DOT (.)

    Considering that any code block in R is an expression, the "get" method (or
    the form of retrieving values from a dictionary) can be used to run a
    number of R commands with the final result returned.

    Note that PypeR do NOT validate/convert a variable name when pass it to R.
    If a variable name with a leading underscore ("_"), although it legal in
    python, is passed to R, an RError will be raised.

Conversions:
    Python -> R
        None -> NULL, NaN -> NaN, Inf -> Inf
    R -> Python (numpy)
        NULL -> None, NA -> None, NaN -> None (NaN), Inf -> None (Inf)

DEBUG model:
    Since the child process (R) can be easily killed by any ocassional error in
    the codes passed to it, PypeR is set to "DEBUG" model by default. This
    means that any code blocks send to R will be wrapped in the function
    "try()", which will prevent R from crashing. To disable the "DEBUG" model,
    the user can simple set the variable "_DEBUG_MODE" in the R class or in its
    instance to False.

    To model the behavior of the "get" method of a Python dictionary, the
    method "get" allows wild values for variables that does not exists in R.
    Then the R expression will always be wrapped in "try()" to avoid R crashing
    if the method "get" is called.
'''

# the module "subprocess" requires Python 2.4

import os
import sys
import time
import re
import tempfile
from types import *

__version__ = '1.1.2'

if sys.version < '2.3':  # actually python >= 2.3 is required by tempfile.mkstemp used in this module !!!
    set = frozenset = tuple
    basestring = str
elif sys.version < '2.4':
    from sets import Set as set, ImmutableSet as frozenset

if sys.version < '3.0':
    _mystr = _mybytes = lambda s: s
    _in_py3 = False
else:
    from functools import reduce
    long, basestring, unicode = int, str, str
    _mybytes = lambda s: bytes(s, 'utf8')  # 'ascii')
    _mystr = lambda s: str(s, 'utf8')
    _in_py3 = True
try:
    import pandas
    has_pandas = True
except:
    has_pandas = False
try:
    import numpy
    has_numpy = True
except:
    has_numpy = False

_has_subp = False
if sys.platform == 'cli':  # for IronPython
    from System.Diagnostics import Process
    PIPE, _STDOUT = None, None

    def Popen(CMD, *a, **b):
        '''
        CMD is a list - a command and its arguments
        '''
        p = Process()
        p.StartInfo.UseShellExecute = False
        p.StartInfo.RedirectStandardInput = True
        p.StartInfo.RedirectStandardOutput = True
        p.StartInfo.RedirectStandardError = True
        p.StartInfo.FileName = CMD[0]
        p.StartInfo.Arguments = ' '.join(CMD[1:])
        p.Start()
        return(p)

    def sendAll(p, s):
        # remove ending newline since WriteLine will add newline at the end of s!
        if s.endswith('\r\n'):
            s = s[:-2]
        elif s.endswith('\n'):
            s = s[:-1]
        p.StandardInput.WriteLine(_mybytes(s))

    def readLine(p, dump_stdout=False, *a, **b):
        rv = _mystr(p.StandardOutput.ReadLine()) + '\n' # add newline since ReadLine removed it.
        if dump_stdout:
            sys.stdout.write(rv)
            sys.stdout.flush()
        return(rv)

else:

    try:
        import subprocess
        _has_subp = True
        Popen, PIPE, _STDOUT = subprocess.Popen, subprocess.PIPE, subprocess.STDOUT
    except:  # Python 2.3 or older
        PIPE, _STDOUT = None, None
        def Popen(CMD, *a, **b):
            class A:
                None
            p = A()
            p.stdin, p.stdout = os.popen4(' '.join(CMD))
            return(p)

    def sendAll(p, s):
        p.stdin.write(_mybytes(s))
        #os.write(p.stdin.fileno(), s)
        p.stdin.flush()

    def readLine(p, dump_stdout=False, *a, **b):
        rv = _mystr(p.stdout.readline())
        if dump_stdout:
            sys.stdout.write(rv)
            sys.stdout.flush()
        return(rv)


def NoneStr(obj): return('NULL')

def BoolStr(obj):
    return(obj and 'TRUE' or 'FALSE')

def ReprStr(obj):
    return(repr(obj))

if has_numpy:
    def FloatStr(f):
        if f is numpy.NaN or f is numpy.nan:
            return('NaN') # or 'NA'
        if has_pandas and pandas.isnull(f):
            return('NaN')
        if numpy.isposinf(f):
            return('Inf')
        if numpy.isneginf(f):
            return('-Inf')
        return(repr(f))
else:
    FloatStr = repr

def LongStr(obj):  
    rv = repr(obj)
    if rv[-1] == 'L':
        rv = rv[:-1]
    return(rv)

def ComplexStr(obj):
    return(repr(obj).replace('j', 'i'))

def UniStr(obj):
    return(repr(obj.encode('utf8')))

def ByteStr(obj):
    return(repr(obj)[1:])
    #return obj.decode()

def SeqStr(obj, head='c(', tail=')', enclose=True):
    if not enclose: # don't add head and tail
        return(','.join(map(Str4R, obj)))
    if not obj:
        return(head + tail)
    # detect types
    if isinstance(obj, set):
        obj = list(obj)
    obj0 = obj[0]
    tp0 = type(obj0)
    simple_types = [str, bool, int, long, float, complex]
    num_types = [int, long, float, complex]
    is_int = tp0 in (int, long) # token for explicit converstion to integer in R since R treat an integer from stdin as double
    if tp0 not in simple_types:
        head = 'list('
    else:
        tps = isinstance(obj0, basestring) and [StringType] or isinstance(obj0, bool) and [BooleanType] or num_types
        for i in obj[1:]:
            tp = type(i)
            if tp not in tps:
                head = 'list('
                is_int = False
                break
            elif is_int and tp not in (int, long):
                is_int = False
    # convert
    return((is_int and 'as.integer(' or '') + head + ','.join(map(Str4R, obj)) + tail + (is_int and ')' or ''))

def DictStr(obj):
    return('list(' + ','.join(['%s=%s' % (Str4R(a[0]), Str4R(a[1])) for a in obj.items()]) + ')')

# 'b':boolean, 'i':integer, 'u':unsigned int, 'f':float, c complex-float
# 'S'/'a':string, 'U':unicode, 'V':raw data. 'O':string?
_tpdic = {'i':'as.integer(c(%s))', 'u':'as.integer(c(%s))', 'f':'as.double(c(%s))', 'c':'as.complex(c(%s))', 
        'b':'c(%s)', 'S':'c(%s)', 'a':'c(%s)', 'U':'c(%s)', 'V':'list(%s)', 'O':'as.character(c(%s))'}
def getVec(ary): 
    # used for objects from numpy and pandas
    tp = ary.dtype.kind
    if len(ary.shape) > 1:
        ary = ary.reshape(reduce(lambda a,b=1: a*b, ary.shape))
    ary = ary.tolist()
    if tp != 'V':
        return(_tpdic.get(tp, 'c(%s)') % SeqStr(ary, enclose=False))
    # record array
    ary = list(map(SeqStr, ary)) # each record will be mapped to vector or list
    return(_tpdic.get(tp, 'list(%s)') % (', '.join(ary))) # use str here instead of repr since it has already been converted to str by SeqStr

def NumpyNdarrayStr(obj):
    shp = obj.shape
    if len(shp) == 1: # to vector
        tp = obj.dtype
        if tp.kind != 'V':
            return(getVec(obj))
        # One-dimension record array will be converted to data.frame
        def mapField(f):
            ary = obj[f]
            tp = ary.dtype.kind
            return('"%s"=%s' % (f, _tpdic.get(tp, 'list(%s)') % SeqStr(ary.tolist(), enclose=False)))
        return('data.frame(%s)' % (', '.join(map(mapField, tp.names))))
    elif len(shp) == 2: # two-dimenstion array will be converted to matrix
        return('matrix(%s, nrow=%d, byrow=TRUE)' % (getVec(obj), shp[0]))
    else: # to array
        dim = list(shp[-2:]) # row, col
        dim.extend(shp[-3::-1])
        newaxis = list(range(len(shp)))
        newaxis[-2:] = [len(shp)-1, len(shp)-2]
        return('array(%s, dim=c(%s))' % (getVec(obj.transpose(newaxis)), repr(dim)[1:-1]))

def PandasSerieStr(obj):
    return('data.frame(%s=%s, row.names=%s)' % (obj.name, getVec(obj.values), getVec(obj.index)))

def PandasDataFrameStr(obj):
    # DataFrame will be converted to data.frame, have to explicitly name columns
    #return 'data.frame(%s, row.names=%s)' % (', '.join(map(lambda a,b=obj:a+'='+getVec(obj[a]), obj)), getVec(obj.index))
    s = ', '.join(map(lambda a,b=obj: '"%s"=%s' % (str(a), getVec(obj[a])), obj))
    return('data.frame(%srow.names=%s)' % (s and s+', ', getVec(obj.index)))
    s = ''
    for col in obj:
        s = s + col + '=' + getVec(obj[col]) + ', '
    # print 'data.frame(%s row.names=%s)' % (s, getVec(obj.index))
    return('data.frame(%s row.names=%s)' % (s, getVec(obj.index)))

def OtherStr(obj):
    if hasattr(obj, '__iter__'):  # for iterators
        if hasattr(obj, '__len__') and len(obj) <= 10000:
            return(SeqStr(list(obj)))
        else:  # waiting for better solution for huge-size containers
            return(SeqStr(list(obj)))
    return(repr(obj))

str_func = {type(None): NoneStr, bool: BoolStr, long: LongStr, int: repr, float: FloatStr, complex: ComplexStr, 
        unicode: UniStr, str: repr, list: SeqStr, tuple: SeqStr, set: SeqStr, frozenset: SeqStr, dict: DictStr} # str will override uncode in Python 3

base_tps = [type(None), bool, int, long, float, complex, str, unicode, list, tuple, set, frozenset, dict] # use type(None) instead of NoneType since the latter cannot be found in the types module in Python 3
if has_numpy: 
    str_func[numpy.ndarray] = NumpyNdarrayStr
    base_tps.append(numpy.ndarray)
if has_pandas:
    str_func.update({pandas.Series: PandasSerieStr, pandas.DataFrame: PandasDataFrameStr})
    base_tps.extend([pandas.Series, pandas.DataFrame])
base_tps.reverse()

if _in_py3:
    base_tps.append(bytes)
    str_func[bytes] = ByteStr

def Str4R(obj):  
    '''
    convert a Python basic object into an R object in the form of string.
    '''
    #return str_func.get(type(obj), OtherStr)(obj) 
    # for objects known by PypeR
    if type(obj) in str_func:
        return(str_func[type(obj)](obj))
    # for objects derived from basic data types
    for tp in base_tps:
        if isinstance(obj, tp):
            return(str_func[tp](obj))
    # for any other objects
    return(OtherStr(obj))

class RError(Exception):
    def __init__(self, value):
        self.value = value
    def __str__(self):
        return(repr(self.value))

class R(object): # "del r.XXX" fails on FePy-r7 (IronPython 1.1 on .NET 2.0.50727.42) if using old-style class
   _DEBUG_MODE = True

    def __init__(self, RCMD='R', max_len=1000, use_numpy=True, use_pandas=True, use_dict=None,
                 host='localhost', user=None, ssh='ssh', return_err=True, dump_stdout=False):
        '''
        RCMD: The name of a R interpreter, path information should be included
            if it is not in the system search path.
        use_numpy: Used as a boolean value. A False value will disable numpy
            even if it has been imported.
        use_pandas: Used as a boolean value. A False value will disable pandas
            even if it has been imported.
        use_dict: A R named list will be returned as a Python dictionary if
            "use_dict" is True, or a list of tuples (name, value) if "use_dict"
            is False. If "use_dict" is None, the return value will be a
            dictionary if there is no replicated names, or a list if replicated
            names found.
        host: The computer name (or IP) on which the R interpreter is
            installed. The value "localhost" means that R locates on the the
            localhost computer. On POSIX systems (including Cygwin environment
            on Windows), it is possible to use R on a remote computer if the
            command "ssh" works. To do that, the user needs to set this value,
            and perhaps the parameter "user".
        user: The user name on the remote computer. This value needs to be set
            only if the user name on the remote computer is different from the
            local user. In interactive environment, the password can be input
            by the user if prompted. If running in a program, the user needs to
            be able to login without typing password!
        ssh: The program to login to remote computer.
        return_err: redirect stderr to stdout
        dump_stdout:
            prints output from R directly to sys.stdout, useful for long running
            routines which print progress during execution.
        '''
        # use self.__dict__.update to register variables since __setattr__ is
        # used to set variables for R.  tried to define __setattr in the class,
        # and change it to __setattr__ for instances at the end of __init__,
        # but it seems failed.
        # -- maybe this only failed in Python2.5? as warned at
        # http://wiki.python.org/moin/NewClassVsClassicClass:
        # "Warning: In 2.5, magic names (typically those with a double
        # underscore (DunderAlias) at both ends of the name) may look at the
        # class rather than the instance even for old-style classes."
        self.__dict__.update({'prog': None,
            'has_numpy': use_numpy and has_numpy,
            'has_pandas': use_pandas and has_pandas,
            'Rfun': self.__class__.__Rfun,
            'max_len': max_len,
            'use_dict': use_dict,
            'dump_stdout': dump_stdout,
            'localhost': host == 'localhost',
            'newline': sys.platform == 'win32' and '\r\n' or '\n',
            'sendAll' : sendAll # keep a reference to the global function "sendAll" which will be used by __del__
            })

        RCMD = [RCMD]  #shlex.split(RCMD) - shlex do not work properly on Windows! #re.split(r'\s', RCMD)
        if not self.localhost:
            RCMD.insert(0, host)
            if user:
                RCMD.insert(0, '-l%s' % user)
            RCMD.insert(0, ssh)
        # args = ('--vanilla',) # equal to --no-save, --no-restore, --no-site-file, --no-init-file and --no-environ
        args = ('--quiet', '--no-save', '--no-restore') # "--slave" cannot be used on Windows!
        for arg in args:
            if arg not in RCMD:
                RCMD.append(arg)
        if _has_subp and hasattr(subprocess, 'STARTUPINFO'):
            info = subprocess.STARTUPINFO()
            try:
                if hasattr(subprocess, '_subprocess'):
                    info.dwFlags |= subprocess._subprocess.STARTF_USESHOWWINDOW
                    info.wShowWindow = subprocess._subprocess.SW_HIDE
                else:
                    info.dwFlags |= subprocess.STARTF_USESHOWWINDOW
                    info.wShowWindow = subprocess.SW_HIDE
            except:
                info = None
        else:
            info = None
        # create stderr to replace None for py2exe:
        # http://www.py2exe.org/index.cgi/Py2ExeSubprocessInteractions
        if sys.platform != 'win32':
            childstderr = None
        else:
            if hasattr(sys.stderr, 'fileno'):
                childstderr = sys.stderr
            elif hasattr(sys.stderr, '_file') and hasattr(sys.stderr._file, 'fileno'):
                childstderr = sys.stderr._file
            else:  # Give up and point child stderr at nul
                childstderr = file('nul', 'a')

        self.__dict__['prog'] = Popen(RCMD, stdin=PIPE, stdout=PIPE, stderr=return_err and _STDOUT or childstderr, startupinfo=info)
        self.__call__(self.Rfun)

    def __runOnce(self, CMD, use_try=None):
        '''
        CMD: a R command string
        '''
        use_try = use_try or self._DEBUG_MODE
        newline = self.newline
        tail_token = 'R command at time: %s' % repr(time.time())
        # tail_token_r = re.sub(r'[\(\)\.]', r'\\\1', tail_token)
        tail_cmd = 'print("%s")%s' % (tail_token, newline)
        tail_token = tail_token.replace(' ', '\\s').replace('.', '\\.').replace('+', '\\+')
        re_tail = re.compile(r'>\sprint\("%s"\)\r?\n\[1\]\s"%s"\r?\n$' % (tail_token, tail_token))
        if len(CMD) <= self.max_len or not self.localhost:
            fn = None
            CMD = (use_try and 'try({%s})%s%s' or '%s%s%s') % (CMD.replace('\\', '\\\\'), newline, tail_cmd)
        else:
            fh, fn = tempfile.mkstemp()
            os.fdopen(fh, 'wb').write(_mybytes(CMD))
            if sys.platform == 'cli':
                os.close(fh)  # this is necessary on IronPython
            fn = fn.replace('\\', '/')
            CMD = (use_try and 'try({source("%s")})%sfile.remove(%r)%s%s' or '%s%s%s') % (fn, newline, fn, newline, tail_cmd)
        self.sendAll(self.prog, CMD)
        rlt = ''
        while not re_tail.search(rlt):
            try:
                rltonce = readLine(self.prog, dump_stdout=self.dump_stdout)
                if rltonce:
                    rlt = rlt + rltonce
            except:
                break
        else:
            rlt = re_tail.sub('', rlt)
            if rlt.startswith('> '):
                rlt = rlt[2:]
        # if fn is not None: os.unlink(fn)
        return(rlt)

    def __call__(self, CMDS=[], use_try=None):
        '''
        Run a (list of) R command(s), and return the output message from the STDOUT of R.

        CMDS: an R command string or a list of R commands
        '''
        rlt = []
        if isinstance(CMDS, basestring):  # a single command
            rlt.append(self.__runOnce(CMDS, use_try=use_try))
        else: # should be a list of commands
            # for CMD in CMDS:
            #   rlt.append(self.__runOnce(CMD, use_try=use_try))
            rlt.append(self.__runOnce('; '.join(CMDS), use_try=use_try)) # now, ['sink("output.txt")', ..., 'sink()'] is allowed!
        if len(rlt) == 1:
            rlt = rlt[0]
        return(rlt)

    def __getitem__(self, obj, use_try=None, use_dict=None): # to model a dict: "r['XXX']"
        '''
        Get the value of an R variable or expression. The return value is
        converted to the corresponding Python object.

        obj: a string - the name of an R variable, or an R expression
        use_try: use "try" function to wrap the R expression. This can avoid R
            crashing if the obj does not exist in R.
        use_dict: named list will be returned a dict if use_dict is True,
            otherwise it will be a list of tuples (name, value)
        '''
        if obj.startswith('_'):
            raise RError('Leading underscore ("_") is not permitted in R variable names!')
        use_try = use_try or self._DEBUG_MODE
        if use_dict is None:
            use_dict = self.use_dict
        cmd = '.getRvalue4Python__(%s, use_dict=%s, has_numpy=%s, has_pandas=%s)' % (obj, use_dict is None and 'NULL' or use_dict and 'TRUE' or 'FALSE', self.has_numpy and 'TRUE' or 'FALSE', self.has_pandas and 'TRUE' or 'FALSE')
        rlt = self.__call__(cmd, use_try=use_try)
        head = (use_try and 'try({%s})%s[1] ' or '%s%s[1] ') % (cmd, self.newline) 
        # sometimes (e.g. after "library(fastICA)") the R on Windows uses '\n' instead of '\r\n'
        head = rlt.startswith(head) and len(head) or len(head) - 1
        tail = rlt.endswith(self.newline) and len(rlt) - len(self.newline) or len(rlt) - len(self.newline) + 1 # - len('"')
        try:
            rlt = eval(eval(rlt[head:tail])) # The inner eval remove quotes and recover escaped characters.
        except:
            raise RError(rlt)
        return(rlt)

    def __setitem__(self, obj, val):  # to model a dict: "r['XXX'] = YYY"
        '''
        Assign a value (val) to an R variable (obj).

        obj: a string - the name of an R variable
        val: a python object - the value to be passed to an R object
        '''
        if obj.startswith('_'):
            raise RError('Leading underscore ("_") is not permitted in R variable names!')
        self.__call__('%s <- %s' % (obj, Str4R(val)))

    def __delitem__(self, obj):  # to model a dict: "del r['XXX']"
        if obj.startswith('_'):
            raise RError('Leading underscore ("_") is not permitted in R variable names!')
        self.__call__('rm(%s)' % obj)

    def __del__(self):  # to model "del r"
        if self.prog:
            try:
                self.sendAll(self.prog, 'q("no")'+self.newline)
            except:
                pass
            self.prog = None

    def __getattr__(self, obj, use_dict=None):  # to model object attribute: "r.XXX"
        '''
        obj: a string - the name of an R variable
        use_dict: named list will be returned a dict if use_dict is True,
            otherwise it will be a list of tuples (name, value)
        '''
        # Overriding __getattr__ is safer than __getattribute__ since it is
        # only called as a last resort i.e. if there are no attributes in the
        # instance that match the name
        if obj in self.__dict__:
            return(self.__dict__[obj])
        if obj in self.__class__.__dict__:
            return(self.__class__.__dict__[obj])
        try:
            if use_dict is None:
                use_dict = self.use_dict
            rlt = self.__getitem__(obj, use_dict=use_dict)
        except:
            raise  # RError('No this object!')
        return(rlt)

    def __setattr__(self, obj, val):  # to model object attribute: "r.XXX = YYY"
        if obj in self.__dict__ or obj in self.__class__.__dict__: # or obj.startswith('_'):
            self.__dict__[obj] = val # for old-style class
            #object.__setattr__(self, obj, val) # for new-style class
        else:
            self.__setitem__(obj, val)

    def __delattr__(self, obj):  # to model object attribute: "del r.XXX"
        if obj in self.__dict__:
            del self.__dict__[obj]
        else:
            self.__delitem__(obj)

    def get(self, obj, default=None, use_dict=None):  # to model a dict: "r.get('XXX', 'YYY')"
        '''
        obj: a string - the name of an R variable, or an R expression
        default: a python object - the value to be returned if failed to get data from R
        use_dict: named list will be returned a dict if use_dict is True,
            otherwise it will be a list of tuples (name, value). If use_dict is
            None, the value of self.use_dict will be used instead.
        '''
        try:
            rlt = self.__getitem__(obj, use_try=True, use_dict=use_dict)
        except:
            if True:  # val is not None:
                rlt = default
            else:
                raise RError('No this object!')
        return(rlt)

    run, assign, remove = __call__, __setitem__, __delitem__


# for a single-round duty:
def runR(CMDS, Robj='R', max_len=1000, use_numpy=True, use_pandas=True, use_dict=None, host='localhost', user=None, ssh='ssh'):
    '''
    Run a (list of) R command(s), and return the output from the STDOUT.

    CMDS: a R command string or a list of R commands.
    Robj: can be a shell command (like /usr/bin/R), or the R class.
    max_len: define the upper limitation for the length of command string. A
        command string will be passed to R by a temporary file if it is longer
        than this value.
    use_numpy: Used as a boolean value. A False value will disable numpy even
        if it has been imported.
    use_pandas: Used as a boolean value. A False value will disable pandas even
        if it has been imported.
    use_dict: named list will be returned a dict if use_dict is True, otherwise
        it will be a list of tuples (name, value).
    host: The computer name (or IP) on which the R interpreter is
        installed. The value "localhost" means that the R locates on the
        the localhost computer. On POSIX systems (including Cygwin
        environment on Windows), it is possible to use R on a remote
        computer if the command "ssh" works. To do that, the user need set
        this value, and perhaps the parameter "user".
    user: The user name on the remote computer. This value need to be set
        only if the user name is different on the remote computer. In
        interactive environment, the password can be input by the user if
        prompted. If running in a program, the user need to be able to
        login without typing password!
    ssh: The program to login to remote computer.
    '''
    if isinstance(Robj, basestring):
        Robj = R(RCMD=Robj, max_len=max_len, use_numpy=use_numpy, use_pandas=use_pandas, use_dict=use_dict, host=host, user=user, ssh=ssh, dump_stdout=dump_stdout)
    rlt = Robj.run(CMDS=CMDS)
    if len(rlt) == 1:
        rlt = rlt[0]
    return(rlt)
    



function.mlib.util.py
"""
Created on Mon Jan 09 11:32:11 2017

@author: ss71165
"""

def doc(topic):
    """
    Display documentation for a function/topic
    """
    rcall = "?" + topic
    print(r(rcall))
    
def ls():
    """
    List the R objects
    """
    print(r("ls()"))    
    
def head(df, n=2):
    """
    Show the head n rows
    """
    rcode = "head(" + df + ", " + str(n) + ")"
    print(r(rcode))
    
def plotRes(model, chartname):
    """
    Produce a Residual Chart
    """
    rcode = "png('" + chartpath + chartname + ".png')"
    print(rcode)
    print(r(rcode))
    rplotcode = "plot(" + model +" , which=1)"
    print(rplotcode)
    print(r(rplotcode))
    r("dev.off()")
    
# Support functions required for objective function    

def ifelse(expression, x, y):
    if(expression):
        return x
    else:
        return y
    
mv_fun_code = "missingValue <- function(x) return(0.000001)"    

def summary(results):
    rcall = "summary("+results+")"
    print(r(rcall))
    
def residuals(model):
    rcode = "rs" + " <- residuals(" + model +")"
    print(rcode)
    print(r(rcode))
    return(r['rs'])
    
def rrange(name):
    rcode = "range(" + name + ")" 
    print(rcode)
    print(r(rcode))
        
def coef(model):
    rcode = "cf"+ " <- coef(" + model +")"
    print(rcode)
    print(r(rcode))
    return(r['cf'])
    
def stderror(fitted_model):
    rcode = "se"+ " <- coef(summary(" + fitted_model +"))[, 2]"
    print(rcode)
    print(r(rcode))
    return(r['se'])    
    
def readcsv(tbl, path):
    rcode = tbl + ' <- read.csv("' + path + '", header=TRUE, stringsAsFactors=FALSE)'
    print(rcode)
    result = r(rcode)
    return(result)

def readtable(tbl, path):
    rcode = tbl + ' <- read.table("' + path + '", header=TRUE, sep=",", quote="")'
    print(rcode)
    result = r(rcode)
    return(result)

def save(model, path):
    """
    Save fitted model to file
    """
    
    rcode = "save(model=" + model + ", file=" + path + ")"
    print(r(rcode))   
    
def load(path):
    """
    Load fitted model from file
    """
    
    rcode = "load(" + path + ")"
    print(r(rcode))     

